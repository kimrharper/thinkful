{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: https://www.dropbox.com/s/pl5kcrhs2lyj90m/WELLCOME.zip?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol><li>determine the five most common journals and the total articles for each. <li>calculate the mean, median, and standard deviation of the open-access cost per article for each journal <li>identify the open access prices paid by subject area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "endspaces_find = re.compile(r'.*?(\\s{1,2})$')\n",
    "endspaces_remove = re.compile(r'\\s$')\n",
    "symbols_remove = re.compile(r'[^a-zA-Z0-9_\\s]')\n",
    "ampar = re.compile(r'[\\&]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collects all the words loaded in a text object and breaks them into words using Regex\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "def loadchecker():\n",
    "    # 1. populates the data set with real corpora...could be expanded to include user's personal content\n",
    "    # 2. counts the number of times a word occurs in the corpora\n",
    "    dictionarylist = []\n",
    "\n",
    "    # main data sample from existing content\n",
    "    biglist = words(open('../dic/big.txt').read())\n",
    "\n",
    "    # new words added to .txt\n",
    "    newwordslist = words(open('../dic/newwords.txt').read())\n",
    "    \n",
    "    global WORDS\n",
    "    WORDS = Counter(biglist + newwordslist)\n",
    "\n",
    "loadchecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(word, N=sum(WORDS.values())): \n",
    "    # Calculates the frequency count a word occurs in the overall WORDS list (i.e. 'the' has a high percentage)\n",
    "    return WORDS[word] / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word): \n",
    "    check = known([word])\n",
    "    \n",
    "    if len(check) == 0:\n",
    "        ans = input('Save new word? {} \\n y/n'.format(word))\n",
    "        if ans == 'y':\n",
    "            with open(\"../dic/newwords.txt\", \"a\") as addword:\n",
    "                addword.write(1000* (word + ' '))\n",
    "            addword.close()\n",
    "            loadchecker()\n",
    "            return word, 1\n",
    "\n",
    "    # Collects list of possible words only exist in WORDS and checks probability\n",
    "    wordlist = candidates(word)\n",
    "    finalcorrection = max(wordlist, key=P)\n",
    "    \n",
    "    possiblechoices = {}\n",
    "    \n",
    "    for w in wordlist:\n",
    "        possiblechoices[w] = P(w)\n",
    "        \n",
    "    return finalcorrection, possiblechoices\n",
    "\n",
    "def candidates(word):\n",
    "    # (1) return word if in WORDS (i.e. correct), \n",
    "    # (2) return word if edits1 of word results in word in WORDS (i.e. typo)\n",
    "    # (3) return word if edits2 on each word in edits1 results in a word in WORDS (i.e. 2 typos)\n",
    "    # (4) return word if no words in WORDS were found\n",
    "    \n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words): \n",
    "    # checks list of words to see if they exists in the WORDS database\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    # initialize the letters\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    # Create a list of splits for the word so that four processes can be run at each split: (# of splits = length of word)\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    \n",
    "    # 1. Create a list of one letter deletes from R side of split\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    # print('Deletes:', deletes,'\\n')\n",
    "    \n",
    "    # 2. Create a list of words with switched letters using L,R from split for position\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    # print('Transposes:', transposes,'\\n')\n",
    "    \n",
    "    # 3. Create a list of words with replaced letters from R side of split using 26 letters * number splits\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    # print('Replaces:', replaces,'\\n')\n",
    "    \n",
    "    # 4. Create a list of words with letter inserts between L and R of split using 26 letters * number splits\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    # print('Inserts:', inserts,'\\n')\n",
    "    \n",
    "    # Returns the unique set of all real and unreal words created from each of the four processes\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    # If edits1 does not return a known word, edits2 check runs a process on each uknown word made in edits1\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringcorrection(strvalue):\n",
    "    strvalue = str(strvalue).lower()\n",
    "    \n",
    "    if endspaces_find.match(strvalue):\n",
    "        strvalue = endspaces_remove.sub('', strvalue)\n",
    "\n",
    "    wordlist = strvalue.split(\" \")\n",
    "    wordlistfinal = []\n",
    "    \n",
    "    for word in wordlist:\n",
    "        word = re.sub(ampar,'and', word)\n",
    "        word = re.sub(r'[^\\w]', '', word)\n",
    "        \n",
    "        if word != '':\n",
    "            word, prob = correction(word)\n",
    "            wordlistfinal.append(word)\n",
    "\n",
    "    final = \" \".join(wordlistfinal)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "welcomedf = pd.read_csv('../data/WELLCOME_APCspend2013_forThinkful.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "welcomedf['Journalfix1'] = welcomedf['Journal title'].apply(stringcorrection)\n",
    "welcomedf['Journalfix1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_errors = {'jounral':'journal', 'heath':'health','molecular':'mol', 'society':'soc','human':'hum','humanan':'human',\n",
    "                   'biol':'biology', 'chemical':'chem','biolgy':'biology', 'service':'services',\n",
    "                   'organic':'org','americal':'american','bioohysica':'biophysica','journal':'j'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\b(' + '|'.join(spelling_errors.keys()) + r')\\b')\n",
    "\n",
    "def publisher_fix(seriesvalue):\n",
    "       \n",
    "    seriesvalue = seriesvalue.lower()\n",
    "    \n",
    "    if endspaces_find.match(seriesvalue):\n",
    "        seriesvalue = endspaces_remove.sub('', seriesvalue)\n",
    "    if re.match(r'national academy of sciences', seriesvalue):\n",
    "        seriesvalue = 'national academy of sciences'\n",
    "    if re.match(r'acs', seriesvalue):\n",
    "        seriesvalue = 'acs'\n",
    "    if re.match(r'nature', seriesvalue):\n",
    "        seriesvalue = 'nature'\n",
    "    if re.match(r'cell press', seriesvalue):\n",
    "        seriesvalue = 'cell press'\n",
    "    if re.match(r'bmj', seriesvalue):\n",
    "        seriesvalue = 'bmj'\n",
    "    if re.match(r'taylor', seriesvalue):\n",
    "        seriesvalue = 't&f'\n",
    "    if re.match(r'wolters', seriesvalue):\n",
    "        seriesvalue = 'wolters kluwer'\n",
    "    if re.match(r'wiley', seriesvalue):\n",
    "        seriesvalue = 'wiley'\n",
    "        \n",
    "    return seriesvalue\n",
    "\n",
    "\n",
    "def journal_fix(seriesvalue):\n",
    "    seriesvalue = str(seriesvalue)\n",
    "    seriesvalue = seriesvalue.replace(\"&\", \"and\")\n",
    "    \n",
    "    seriesvalue = seriesvalue.lower()\n",
    "    \n",
    "    if endspaces_find.match(seriesvalue):\n",
    "        seriesvalue = endspaces_remove.sub('', seriesvalue)\n",
    "        \n",
    "        \n",
    "    if pattern.match(seriesvalue):\n",
    "#         print(seriesvalue)\n",
    "        seriesvalue = pattern.sub(lambda x: spelling_errors[x.group()], seriesvalue)\n",
    "#         print(seriesvalue)\n",
    "            \n",
    "    return seriesvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "welcomedf['Journalfix2'] = welcomedf['Journal title'].apply(journal_fix)\n",
    "welcomedf['Journalfix2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "welcomedf['Journalfix1'] = welcomedf['Journal title'].apply(correction)\n",
    "welcomedf['Journalfix1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "welcomedf.Publisher = welcomedf.Publisher.apply(publisher_fix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
