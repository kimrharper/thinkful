{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <span>Part 1: Exploration <br><a href=\"https://kimrharper.github.io/port3a.html\"> https://kimrharper.github.io/port3a.html</a> </span><br><br><span>Part 2: Analysis <br><a href=\"https://kimrharper.github.io/port3b.html\"> https://kimrharper.github.io/port3b.html</a> </span><br><br><span>Part 3: Models <br><a href=\"https://kimrharper.github.io/port3c.html\"> https://kimrharper.github.io/port3c.html</a> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# <span style=\"color:darkred\">L1 Prediction from ELL Writing Samples</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Part 1: </span><span style=\"color:darkblue\">Exploration</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Ryan Harper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#ov'>Overview</a><br>\n",
    "<a href='#exp'>Experiment</a><br>\n",
    "<a href='#sec1'>1. Cleaning Data</a><br>\n",
    "<a href='#sec2'>2. Exploring the Data</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ov\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Overview</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Source:__\n",
    "> http://lang-8.com/ [scraped with Beautiful Soup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../data/language/lang8.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary:__\n",
    "> In my previous profession, I have been teaching English to a diverse range of students of all ages, language background, and country origin. During my professional development, I started to observe that different students with different L1s (1st Language) tended to display different patterns of communication that appeared to have some connection to either education in their country of origin or a connection to the linguistic structure of their first language. Different ELL (English Language Learners) needed to focus on different aspects of the English language depending on their background. The purpose of this project is to use a large number of blog posts from a language practicing website and explore whether or not the L1 has any significant impact on the blog writing style of the English learner.<br><br>Part 1: Explore the data to find any noteworthy trends in linguistic structure: <ol><li> vocabulary (word freq, collocations, and cognates) <li>syntax (sentence structure)<li>grammar (i.e. grammar complexity of sentences) <li>errors (types of errors) <li> parts of speech (NLTK Abbreviations: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/)<li>Word Frequency (ANC: http://www.anc.org/data/anc-second-release/frequency-data/)</ol><br>Part 2: Use linguistic trends to determine whether or not a learner's first language can be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variables:__\n",
    ">__id:__ _User ID_<br>\n",
    "__time:__ _Time the blog post was scraped (in order of user posted time)_ <br>\n",
    "__title:__ _Title of the blog post_<br>\n",
    "__content:__ _The blog post_<br>\n",
    "__language:__ _User's self-reported first language_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Experiment</span> <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothesis:__ \n",
    "> L1 (first language) experience and academic environment influences ELLs' (English Language Learners') writing style. The L1 of ELLs can be predicted by looking at English blog posts and identifying patterns unique to their L1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__\n",
    "><li> Chinese learners use more reflexive pronouns than Japanese learners  <li>Japanese and Chinese learners appear to favor different prepositions<li>Japanese and Chinese learners have a different range of subjectivity scores (from Textblob)<li>K Nearest Neighbors does not appear to work for this NLP project<li>Naive Bayes and Random Forest outperformed other models<li>Logistic Regression occasionally has strong predictions (but the order of the first few ranked features do not appear significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Method:__\n",
    "> Using multiple models, the aim of this project is to explore how different models can handle the data (target and features) and to see what information can be gained from using multiple different models. Ultimately, the goal is to determine which models are appropriate for a binary (discrete) target with features that are both qualitative (discrete) and quantitative (ranked/continuous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">1. Cleaning the Data</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import brown\n",
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iPython/Jupyter Notebook\n",
    "import time\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "from IPython.display import Image\n",
    "\n",
    "# Data processing\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import plotly as plo\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "from statsmodels import robust\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, median_test, f_oneway,mood, shapiro\n",
    "\n",
    "# NLP\n",
    "import textblob\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import difflib\n",
    "from string import punctuation\n",
    "\n",
    "# import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# load and close files\n",
    "def get_text(link):\n",
    "    with open(link) as f:\n",
    "        output = f.read()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Jupyter Settings and Imports\n",
    "%pylab\n",
    "%matplotlib inline \n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import data\n",
    "sample = pd.read_csv('../data/language/blogdata-reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17141 entries, 0 to 17140\n",
      "Data columns (total 6 columns):\n",
      "Unnamed: 0    17141 non-null int64\n",
      "id            17141 non-null int64\n",
      "time          17141 non-null object\n",
      "title         17141 non-null object\n",
      "content       17141 non-null object\n",
      "language      17141 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 803.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "blog = pd.read_csv('../data/language/blogdata-reduced.csv')\n",
    "blog.info()\n",
    "\n",
    "# POS Table for reference\n",
    "poscv = pd.read_csv('../data/pos.csv')\n",
    "poscv = poscv.iloc[0:17]\n",
    "poscv.columns = ['Set1','Set 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "blog = pd.read_csv('../data/language/blogdata-reduced.csv')\n",
    "\n",
    "# Clean Data\n",
    "del blog['Unnamed: 0']\n",
    "blog.language = blog.language.mask(blog.language == 'Mandarin', 'Traditional Chinese').replace(['Persian', 'Arabic',\n",
    "        'Bulgarian', 'Swedish', 'Slovenian', 'Slovak', 'Malay', 'Turkish','Romanian', 'Czech', 'Danish', 'Vietnamese',\n",
    "        'Norwegian','Serbian','Other language','Lithuanian', 'Ukrainian', 'Finnish','Estonian','Bengali','Russian', \n",
    "        'Spanish','French', 'German', 'Cantonese','Mongolian', 'Tagalog', 'Polish', 'Dutch','Italian', 'Portuguese(Brazil)', \n",
    "        'Thai', 'Indonesian', 'Cantonese','Urdu', 'Hungarian','Korean','English'], np.nan)\n",
    "blog = blog.dropna().sample(frac=1)\n",
    "\n",
    "del blog['title']\n",
    "del blog['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14262 entries, 7046 to 11936\n",
      "Columns: 3 entries, id to language\n",
      "dtypes: int64(1), object(2)"
     ]
    }
   ],
   "source": [
    "blog.info(verbose=False, memory_usage=False,null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirmation that there are no more null values\n",
    "blog.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lettercheck(val):\n",
    "    reLetters = re.compile('[^a-zA-Z]')\n",
    "    onlyletters = reLetters.sub('', val)\n",
    "    return len(onlyletters)/len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Blogs with less than 70% letter percentage: 1227\n"
     ]
    }
   ],
   "source": [
    "blog['letters_per'] = blog.content.apply(lettercheck)\n",
    "print('Removing Blogs with less than 70% letter percentage: {}'.format(blog.loc[blog['letters_per'] <= .7].content.count()))\n",
    "blog = blog.loc[blog['letters_per'] > .7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13035 entries, 7046 to 11936\n",
      "Data columns (total 4 columns):\n",
      "id             13035 non-null int64\n",
      "content        13035 non-null object\n",
      "language       13035 non-null object\n",
      "letters_per    13035 non-null float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 509.2+ KB\n"
     ]
    }
   ],
   "source": [
    "blog.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">2. Exploring the Data</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vals = list(blog.language.value_counts().values)\n",
    "languages = list(blog.language.value_counts().index)\n",
    "plt.figure(figsize(6,4))\n",
    "plt.bar(languages,vals,edgecolor='black')\n",
    "plt.title('Blogs by L1 Count')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts by 'Native' English Speakers: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Posts by 'Native' English Speakers: {}\".format(blog.id.loc[blog.language == 'English'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nlp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">NLP: Spell Check, Tokenization, Collocations, Parts of Speech, and Syntax</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word Level Ranking__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCI_WORDS = pd.read_csv('../data/language/ANC-written-count.txt', \n",
    "                         sep='\\t', \n",
    "                         encoding='latin-1', \n",
    "                         names=['word','stem','pos','freq'],header=None)\n",
    "word_freq = list(zip(ANCI_WORDS['word'].values,ANCI_WORDS['freq'].values))\n",
    "\n",
    "full_words_dict = {}\n",
    "words_dict = {}\n",
    "\n",
    "# full_freq\n",
    "i = 0\n",
    "for w in word_freq:\n",
    "    i = i + 1\n",
    "    if w[0] not in full_words_dict:\n",
    "        full_words_dict[w[0]] = w[1]\n",
    "        \n",
    "# basic_freq\n",
    "i = 0\n",
    "for w in word_freq:\n",
    "    i = i + 1\n",
    "    if w[0] not in words_dict:\n",
    "        if i < 500:\n",
    "            words_dict[w[0]] = 1\n",
    "        elif (i >= 500) & (i < 5000):\n",
    "            words_dict[w[0]] = 2\n",
    "        elif (i >= 5000) & (i < 10000):\n",
    "            words_dict[w[0]] = 3\n",
    "        elif (i >= 5000) & (i < 20000):\n",
    "            words_dict[w[0]] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_freq_rating(l):\n",
    "    score = 0\n",
    "    c=0\n",
    "    for w in l:\n",
    "        if w in full_words_dict:\n",
    "            c = c + 1\n",
    "            score = score + full_words_dict[w]\n",
    "    if c == 0:\n",
    "        c=1\n",
    "    return score / c\n",
    "\n",
    "def freq_rating(l):\n",
    "    score = 0\n",
    "    c=0\n",
    "    for w in l:\n",
    "        if w in words_dict:\n",
    "            c = c + 1\n",
    "            score = score + words_dict[w]\n",
    "    if c == 0:\n",
    "        c=1\n",
    "    return score / c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TextBlob__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 246 ms, sys: 16.1 ms, total: 262 ms\n",
      "Wall time: 264 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "blob = blog.content.apply(lambda val: textblob.TextBlob(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posbigram(val):\n",
    "    bigramlist,l = [],[]\n",
    "    \n",
    "    for s in val.sentences:\n",
    "        ns = textblob.TextBlob(str(s)).tags\n",
    "        l = [v[1] for v in ns]\n",
    "        bigrm = list(nltk.bigrams(l))\n",
    "        \n",
    "        for bigram in bigrm:\n",
    "            bigramlist.append('-'.join(bigram))\n",
    "        \n",
    "    return bigramlist\n",
    "\n",
    "def postrigram(val):\n",
    "    trigramlist, l = [],[]\n",
    "    \n",
    "    for s in val.sentences:\n",
    "        ns = textblob.TextBlob(str(s)).tags\n",
    "        l = [v[1] for v in ns]\n",
    "        trigrm = list(nltk.trigrams(l))\n",
    "        \n",
    "        for trigram in trigrm:\n",
    "            trigramlist.append('-'.join(trigram))\n",
    "        \n",
    "    return trigramlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_check(string_value, total):\n",
    "    percentage = len(string_value)\n",
    "    if percentage != 0:\n",
    "        percentage = float(total / percentage) * 100\n",
    "    else:\n",
    "        percentage = 0\n",
    "    return percentage\n",
    "\n",
    "def punc_count(string_value):\n",
    "    count = 0\n",
    "    for c in string_value:\n",
    "        if c in punctuation:\n",
    "            count+= 1\n",
    "    return per_check(string_value, count)\n",
    "\n",
    "def caplet_count(string_value):\n",
    "    count = 0\n",
    "    for c in string_value:\n",
    "        if c.isupper():\n",
    "            count+= 1\n",
    "    return per_check(string_value, count)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__General Text Analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43 s, sys: 694 ms, total: 43.7 s\n",
      "Wall time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "blog['wc'] = blob.apply(lambda val: len(val.words))\n",
    "blog['sc'] = blob.apply(lambda val: len(val.sentences))\n",
    "blog['tokens'] = blob.apply(lambda val: [w.lower() for w in val.words])\n",
    "blog['sent_pol'] = blob.apply(lambda val: val.sentiment[0])\n",
    "blog['sent_subj'] = blob.apply(lambda val: val.sentiment[1])\n",
    "blog['cap_let'] = blob.apply(caplet_count)\n",
    "blog['punc_count'] = blob.apply(punc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog['freq_score'] = blob.apply(lambda val: freq_rating(val.words))\n",
    "blog['full_freq_score'] = blob.apply(lambda val: full_freq_rating(val.words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Cleaning Round 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog = blog[blog['wc'] >= 4]\n",
    "blog = blog[blog['full_freq_score'] > 1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parts of Speech Tokens__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/util.py:491: DeprecationWarning:\n",
      "\n",
      "generator 'ngrams' raised StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min, sys: 7.53 s, total: 5min 8s\n",
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "blog['pos'] = blob.apply(lambda val: [v[1] for v in val.tags])\n",
    "blog['pos2'] = blob.apply(posbigram)\n",
    "blog['pos3'] = blob.apply(postrigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Backup File__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "blog.to_csv('processed_blog_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frequent Words Per Language__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 17.1 s, total: 1min 21s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "js = Counter(reduce((lambda x, y: x + y), blog.content[blog.language == 'Japanese'].str.lower().apply(nltk.word_tokenize))).most_common(1000)\n",
    "cs = Counter(reduce((lambda x, y: x + y), blog.content[blog.language == 'Traditional Chinese'].str.lower().apply(nltk.word_tokenize))).most_common(1000)\n",
    "cl,jl = [l[0] for l in cs],[l[0] for l in js]\n",
    "cuw,juw = [item for item in cl if item not in jl],[item for item in jl if item not in cl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">2. Feature Processing:</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_binary_df(df,col,name,keep=[],):\n",
    "    colset = set(df[col].sum())\n",
    "    finalsetlist = []\n",
    "    if len(keep) > 0:\n",
    "        colset = [k for k in keep if k in colset]\n",
    "    \n",
    "    for c in colset:\n",
    "        colname = name+'_'+str(c)\n",
    "        df[colname] = df[col].apply(lambda val: val.count(c))\n",
    "        \n",
    "        if df[colname].sum() < 1:\n",
    "            del df[colname]\n",
    "        else:\n",
    "            finalsetlist.append(colname)\n",
    "        \n",
    "    print('Created dummy counter for {} features'.format(name))\n",
    "        \n",
    "    return finalsetlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_count_df(df,col,colset,name,keep=[]): \n",
    "    finalsetlist = []\n",
    "    if len(keep) > 0:\n",
    "        colset = [k for k in keep if k in colset]\n",
    "    \n",
    "    for c in colset:\n",
    "        colname = name+'_'+str(c)\n",
    "        df[colname] = df[col].apply(lambda val: val.count(c))\n",
    "        \n",
    "        if df[colname].sum() < 1:\n",
    "            del df[colname]\n",
    "        else:\n",
    "            finalsetlist.append(colname)\n",
    "        \n",
    "    print('Created dummy counter for {} features'.format(name))\n",
    "        \n",
    "    return finalsetlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 25.8 s, total: 1min 56s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "colset = set(blog['tokens'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy counter for prp features\n",
      "Created dummy counter for cc features\n",
      "Created dummy counter for prep features\n",
      "Created dummy counter for adv features\n",
      "Created dummy counter for punct features\n",
      "Created dummy counter for cuw features\n",
      "Created dummy counter for juw features\n",
      "CPU times: user 12.6 s, sys: 191 ms, total: 12.8 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prplist = list(ANCI_WORDS['word'][ANCI_WORDS.pos == 'PRP'])\n",
    "pronouns = create_dummy_count_df(blog,'tokens',colset,'prp',prplist)\n",
    "\n",
    "cclist = list(ANCI_WORDS['word'][ANCI_WORDS.pos == 'CC'])\n",
    "coordinators = create_dummy_count_df(blog,'tokens',colset,'cc',cclist)\n",
    "\n",
    "inlist = list(ANCI_WORDS['word'][ANCI_WORDS.pos == 'IN'])\n",
    "preposition = create_dummy_count_df(blog,'tokens',colset,'prep',inlist)\n",
    "\n",
    "adverblist = list(ANCI_WORDS['word'][ANCI_WORDS.pos == 'RB'])[0:50]\n",
    "adverb = create_dummy_count_df(blog,'tokens',colset,'adv',adverblist)\n",
    "\n",
    "punct = create_dummy_count_df(blog,'tokens',colset,'punct',list(punctuation))\n",
    "\n",
    "cuw = create_dummy_count_df(blog,'tokens',colset,'cuw',cuw)\n",
    "juw = create_dummy_count_df(blog,'tokens',colset,'juw',juw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 1min, total: 4min 33s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos1set = set(blog['pos'].sum())\n",
    "pos2set = set(blog['pos2'].sum())\n",
    "pos3set = set(blog['pos3'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy counter for pos2 features\n",
      "Created dummy counter for pos1 features\n",
      "CPU times: user 30.5 s, sys: 1.39 s, total: 31.9 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos2 = create_dummy_count_df(blog,'pos2',pos2set,'pos2')\n",
    "pos1 = create_dummy_count_df(blog,'pos',pos1set,'pos1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy counter for pos3 features\n",
      "CPU times: user 7min 54s, sys: 1min 46s, total: 9min 41s\n",
      "Wall time: 9min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos3 = create_dummy_count_df(blog,'pos3',pos3set,'pos3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters1 = []\n",
    "for let in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    name = 'let1_'+let\n",
    "    blog[name] = blog.tokens.apply(lambda val: ''.join(val).count(let))\n",
    "    letters1.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.9 s, sys: 10.4 s, total: 54.2 s\n",
      "Wall time: 51.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "alphabet = list('abcdefghijklmnopqrstuvwxyz')\n",
    "letters2 = []\n",
    "for let in alphabet:\n",
    "    for let2 in alphabet:\n",
    "        letters2.append(let+let2)\n",
    "\n",
    "letters2name = []        \n",
    "for let in letters2:\n",
    "    name = 'let2_'+let\n",
    "    blog[name] = blog.tokens.apply(lambda val: ' '.join(val).count(let))\n",
    "    if blog[name].sum() < 10:\n",
    "        del blog[name]\n",
    "    else:\n",
    "        letters2name.append(name)        \n",
    "letters2 = letters2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 39s, sys: 9.12 s, total: 3min 48s\n",
      "Wall time: 1h 14min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# For backup\n",
    "blog.to_csv('blogfeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'blog' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# For second notebook\n",
    "%store blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
