{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><strong>Capstone #3:</strong> <span style=\"color:darkred\">Supervised Learning</span> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:darkred\">__Part 1: Data Exploration__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-exploration.ipynb </span><br><br><span style=\"color:darkred\">__Part 2: Models__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-models.ipynb </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Part 2: </span><span style=\"color:darkblue\">L1 Prediction from ELL Writing Samples</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Ryan Harper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#ov'>Overview</a><br>\n",
    "<a href='#exp'>Experiment</a><br>\n",
    "<a href='#sec1'>1. Models:</a><br>\n",
    "><a href='#seca'>A. LR - Ordinary Least Squares</a><br>\n",
    "<a href='#secb'>B. LR - Logistic Regression</a> <a href='#secb1'> (Lasso)</a> <a href='#secb2'> (Ridge)</a><br>\n",
    "<a href='#secc'>C. NN - K Nearest Neighbors</a><br>\n",
    "<a href='#secd'>D. NN - Naive Bayes</a><br>\n",
    "<a href='#sece'>E. NN - Decision Tree</a><br>\n",
    "<a href='#secf'>F. Ensemble - Random Forest</a><br>\n",
    "\n",
    "<a href='#sec2'>2. Model Comparison</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ov\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Overview</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Source:__\n",
    "> http://lang-8.com/ [scraped with Beautiful Soup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../data/language/lang8.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary:__\n",
    "> In my previous profession, I have been teaching English to a diverse range of students of all ages, language background, and country origin. During my professional development, I started to observe that different students with different L1s (1st Language) tended to display different patterns of communication that appeared to have some connection to either education in their country of origin or a connection to the linguistic structure of their first language. Different ELL (English Language Learners) needed to focus on different aspects of the English language depending on their background. The purpose of this project is to use a large number of blog posts from a language practicing website and explore whether or not the L1 has any significant impact on the blog writing style of the English learner.<br><br>Part 1: Explore the data to find any noteworthy trends in linguistic structure: <ol><li> vocabulary (word freq, collocations, and cognates) <li>syntax (sentence structure)<li>grammar (i.e. grammar complexity of sentences) <li>errors (types of errors) <li> parts of speech (NLTK Abbreviations: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/)</ol><br>Part 2: Use linguistic trends to determine whether or not a learner's first language can be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variables:__\n",
    ">__id:__ _User ID_<br>\n",
    "__time:__ _Time the blog post was scraped (in order of user posted time)_ <br>\n",
    "__title:__ _Title of the blog post_<br>\n",
    "__content:__ _The blog post_<br>\n",
    "__language:__ _User's self-reported first language_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Experiment</span> <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothesis:__ \n",
    "> L1 (first language) experience and academic environment influences ELLs' (English Language Learners') writing style. The L1 of ELLs can be predicted by looking at English blog posts and identifying patterns unique to their L1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__\n",
    "><li> --<li>--<li>--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Method:__\n",
    "> Using multiple different models. The aim of this project is to explore how different models can handle the data (target and features) and to see what information can be gained from using multiple different models. Ultimately, the goal is to determine which models are appropriate for a binary (discrete) target with features that are both qualitative (discrete) and quantitative (ranked/continuous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">1. Models:</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iPython/Jupyter Notebook\n",
    "import time\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "from IPython.display import Image\n",
    "\n",
    "import time\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import plotly as plo\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import difflib\n",
    "\n",
    "# Stats\n",
    "from sklearn.metrics import classification_report, roc_curve,roc_auc_score,accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Preparing Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "# Models\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB,GaussianNB\n",
    "\n",
    "# Ensemble\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Visualization\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "# import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import Features + Target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('blogfeatures.csv').sample(frac=1.0)\n",
    "del features['Unnamed: 0']\n",
    "del features['id']\n",
    "lang = list(features.language.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Declare X,y Variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14148,)\n",
      "(14148, 14665)\n"
     ]
    }
   ],
   "source": [
    "y = features['language'].values.reshape(-1, 1).ravel()\n",
    "X = features[features.columns[~features.columns.str.contains('language')]]\n",
    "X.head()\n",
    "\n",
    "print(np.shape(y))\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SVD Truncate: To find most important features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=7,\n",
       "       random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\n",
    "svd.fit(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = [X.columns[i] for i in svd.components_[0].argsort()[::-1]][0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reduce Features (Unused): 14,665 to n_components __"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(svd.components_[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())  \n",
    "print(svd.singular_values_)\n",
    "svd.get_params"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newX = svd.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split Data to Train/Test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[best_features], y, test_size=0.20, random_state=32)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Function for Comparing Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['name','time','total','precision','recall','f1']\n",
    "\n",
    "model_set = pd.DataFrame(columns=cols)\n",
    "models_stored = []\n",
    "pattern = \"%.2f\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cpu_time(s):\n",
    "    return (((s.stdout.splitlines())[0].split(','))[0].split('user '))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,name):\n",
    "    global model_set\n",
    "    m = model\n",
    "    m.fit(X_train, y_train)\n",
    "    start = time.time()\n",
    "\n",
    "    total_score = m.score(X_test,y_test)\n",
    "    pscore = [pattern % i for i in list(metrics.precision_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    rscore = [pattern % i for i in list(metrics.recall_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    fscore = [pattern % i for i in list(metrics.f1_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    end = time.time()\n",
    "    t= pattern % (end - start)\n",
    "\n",
    "    r = dict(zip(cols,[name,t,total_score,pscore,rscore,fscore]))\n",
    "    print('Total Score is: {}\\n'.format(total_score))\n",
    "    print(classification_report(y_test, m.predict(X_test)))\n",
    "    \n",
    "    model_set = model_set.append(r,ignore_index=True)\n",
    "    return r,m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:gray\">A. LR - Ordinary Least Squares _(not used)_</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Target is discrete so this model may not be appropriate <br>Many features are binary so model may not be appropriate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%capture timeA --no-stderr\n",
    "%%time\n",
    "\n",
    "# Instantiate our model.\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Fit our model to our data.\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secb\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">B. LR - Logistic Regression</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Target is binary so logistic regression will operate on probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score is: 0.7219081272084805\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            English       0.00      0.00      0.00        15\n",
      "           Japanese       0.73      0.95      0.83      1906\n",
      "             Korean       0.00      0.00      0.00       241\n",
      "Traditional Chinese       0.64      0.36      0.46       668\n",
      "\n",
      "        avg / total       0.65      0.72      0.67      2830\n",
      "\n",
      "CPU times: user 972 ms, sys: 31.8 ms, total: 1 s\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lreg_data,lreg = run_model(linear_model.LogisticRegression(),'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>English</th>\n",
       "      <th>Japanese</th>\n",
       "      <th>Korean</th>\n",
       "      <th>Traditional Chinese</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>0</td>\n",
       "      <td>1805</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Traditional Chinese</th>\n",
       "      <td>1</td>\n",
       "      <td>429</td>\n",
       "      <td>0</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                English  Japanese  Korean  Traditional Chinese\n",
       "row_0                                                              \n",
       "English                    0        10       0                    5\n",
       "Japanese                   0      1805       1                  100\n",
       "Korean                     0       214       0                   27\n",
       "Traditional Chinese        1       429       0                  238"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test,lreg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.std(X)*lreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secb1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:gray\">C. Lasso _(not used)_</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "lamvalues = [.1,.25,.5,.75,1,3,5,10]\n",
    "\n",
    "for lam in lamvalues:\n",
    "    # Instantiate our model.\n",
    "    lasso = linear_model.Lasso(alpha=lam,fit_intercept=False)\n",
    "    \n",
    "    # Fit our model to our data.\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    print('\\u03bb={} \\tLasso Score: {}'.format(lam, lasso.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secb2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:gray\">D. Ridge _(not used)_</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lamvalues = [.1,.5,.75,1,3,5,10]\n",
    "\n",
    "for lam in lamvalues:\n",
    "    # Instantiate our model.\n",
    "    ridg = linear_model.Ridge(alpha=lam,fit_intercept=False)\n",
    "    \n",
    "    # Fit our model to our data.\n",
    "    ridg.fit(X_train, y_train)\n",
    "    \n",
    "    print('\\u03bb={} \\tRidge Score: {}'.format(lam, ridg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lasso and Ridge are not good predictors so should I just be using them for parameter manipulation?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sece\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">E. K Nearest Neighbors</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can handle discrete values for target <br>Quantitative values are limited (not continuous) and might be problematic for nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n",
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score is: 0.684452296819788\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            English       0.00      0.00      0.00        15\n",
      "           Japanese       0.72      0.92      0.81      1906\n",
      "             Korean       0.13      0.02      0.04       241\n",
      "Traditional Chinese       0.52      0.28      0.36       668\n",
      "\n",
      "        avg / total       0.62      0.68      0.63      2830\n",
      "\n",
      "CPU times: user 1.77 s, sys: 17.7 ms, total: 1.79 s\n",
      "Wall time: 1.83 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "neighbors_data,neighbors = run_model(KNeighborsClassifier(n_neighbors=5),'K Nearest Neighbor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">F. Naive Bayes - Bernoulli</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Multinomial (Default prior distribution)\n",
    "Bernoulli (Binomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Should be best for boolean classification but not for multiple discrete values (multinomial should be better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score is: 0.6123674911660777\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            English       0.00      0.00      0.00        15\n",
      "           Japanese       0.71      0.78      0.75      1906\n",
      "             Korean       0.00      0.00      0.00       241\n",
      "Traditional Chinese       0.33      0.36      0.34       668\n",
      "\n",
      "        avg / total       0.56      0.61      0.58      2830\n",
      "\n",
      "CPU times: user 110 ms, sys: 17.8 ms, total: 128 ms\n",
      "Wall time: 134 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n",
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n",
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bnb_data,bnb = run_model(BernoulliNB(),'Naive Bayes - Bernoulli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">G. Decision Tree</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visualizes most important features by hierarchy <br>Longer processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score is: 0.7060070671378091\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            English       0.50      0.07      0.12        15\n",
      "           Japanese       0.72      0.94      0.82      1906\n",
      "             Korean       0.00      0.00      0.00       241\n",
      "Traditional Chinese       0.59      0.30      0.40       668\n",
      "\n",
      "        avg / total       0.63      0.71      0.65      2830\n",
      "\n",
      "CPU times: user 141 ms, sys: 9.9 ms, total: 151 ms\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dt_data,dt = run_model(tree.DecisionTreeClassifier(criterion='entropy',max_depth=7),'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Render tree.\n",
    "dot_data = tree.export_graphviz(\n",
    "    dt, out_file=None,\n",
    "    feature_names=best_features,\n",
    "    class_names=lang,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n",
    "\n",
    "graph.write_png('decision_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sc', 'pos2_DT-JJ', 'wc', 'pos2_PRP-VBD', 'pos2_NN-NN', 'pos2_PRP-VBP', 'pos3_IN-DT-NN', 'pos3_DT-NN-IN', 'pos2_IN-NN', 'pos2_IN-PRP', 'pos2_DT-NN', 'pos2_NN-CC', 'pos2_NNS-IN', 'pos2_JJ-NN', 'pos2_TO-VB', 'pos2_NN-IN', 'pos2_IN-DT']\n"
     ]
    }
   ],
   "source": [
    "dimportance = list(zip(best_features[0:20],dt.feature_importances_))\n",
    "dimportance = dict(dimportance)\n",
    "a1_sorted_keys = sorted(dimportance, key=dimportance.get, reverse=True)\n",
    "p = []\n",
    "for r in a1_sorted_keys:\n",
    "    if dimportance[r] != 0:\n",
    "        p.append(r)\n",
    "#         print(r, dimportance[r])\n",
    "        \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Good visualization of important features and presentation of entropy weighting_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sech\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">H. Random Forest</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Runs decision tree multiple times for best output <br>Longest processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score is: 0.7173144876325088\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            English       0.00      0.00      0.00        15\n",
      "           Japanese       0.73      0.95      0.82      1906\n",
      "             Korean       0.00      0.00      0.00       241\n",
      "Traditional Chinese       0.64      0.33      0.44       668\n",
      "\n",
      "        avg / total       0.64      0.72      0.66      2830\n",
      "\n",
      "CPU times: user 528 ms, sys: 24.5 ms, total: 553 ms\n",
      "Wall time: 562 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n",
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n",
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_data,rf = run_model(ensemble.RandomForestClassifier(n_estimators=20),'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71126761 0.73144876 0.69964664 0.70619469 0.70088496]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(rf, X_test, y_test, cv=5)\n",
    "print(cvs)\n",
    "scoreH = cvs.sum()/len(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7098885316023853\n"
     ]
    }
   ],
   "source": [
    "rf.score(X_train,y_train)\n",
    "print(scoreH)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rf.feature_importances_\n",
    "importance = list(zip(X.columns,rf.feature_importances_))\n",
    "\n",
    "dimportance = dict(importance)\n",
    "\n",
    "a1_sorted_keys = sorted(dimportance, key=dimportance.get, reverse=True)\n",
    "# for r in a1_sorted_keys:\n",
    "#     if dimportance[r] >0:\n",
    "#         print(r, dimportance[r])\n",
    "\n",
    "print(a1_sorted_keys[0:36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">2. Model Comparison</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>total</th>\n",
       "      <th>prec: | JA | CH | KO | EN |</th>\n",
       "      <th>rec: | JA | CH | KO | EN |</th>\n",
       "      <th>f1: | JA | CH | KO | EN |</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.721908</td>\n",
       "      <td>[0.73, 0.64, 0.00, 0.00]</td>\n",
       "      <td>[0.95, 0.36, 0.00, 0.00]</td>\n",
       "      <td>[0.83, 0.46, 0.00, 0.00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K Nearest Neighbor</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.684452</td>\n",
       "      <td>[0.72, 0.52, 0.13, 0.00]</td>\n",
       "      <td>[0.92, 0.28, 0.02, 0.00]</td>\n",
       "      <td>[0.81, 0.36, 0.04, 0.00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes - Bernoulli</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.612367</td>\n",
       "      <td>[0.71, 0.33, 0.00, 0.00]</td>\n",
       "      <td>[0.78, 0.36, 0.00, 0.00]</td>\n",
       "      <td>[0.75, 0.34, 0.00, 0.00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.706007</td>\n",
       "      <td>[0.72, 0.59, 0.00, 0.50]</td>\n",
       "      <td>[0.94, 0.30, 0.00, 0.07]</td>\n",
       "      <td>[0.82, 0.40, 0.00, 0.12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.717314</td>\n",
       "      <td>[0.73, 0.64, 0.00, 0.00]</td>\n",
       "      <td>[0.95, 0.33, 0.00, 0.00]</td>\n",
       "      <td>[0.82, 0.44, 0.00, 0.00]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name  time     total prec: | JA | CH | KO | EN |  \\\n",
       "0      Logistic Regression  0.05  0.721908    [0.73, 0.64, 0.00, 0.00]   \n",
       "1       K Nearest Neighbor  1.43  0.684452    [0.72, 0.52, 0.13, 0.00]   \n",
       "2  Naive Bayes - Bernoulli  0.05  0.612367    [0.71, 0.33, 0.00, 0.00]   \n",
       "3            Decision Tree  0.04  0.706007    [0.72, 0.59, 0.00, 0.50]   \n",
       "4            Random Forest  0.10  0.717314    [0.73, 0.64, 0.00, 0.00]   \n",
       "\n",
       "  rec: | JA | CH | KO | EN | f1: | JA | CH | KO | EN |  \n",
       "0   [0.95, 0.36, 0.00, 0.00]  [0.83, 0.46, 0.00, 0.00]  \n",
       "1   [0.92, 0.28, 0.02, 0.00]  [0.81, 0.36, 0.04, 0.00]  \n",
       "2   [0.78, 0.36, 0.00, 0.00]  [0.75, 0.34, 0.00, 0.00]  \n",
       "3   [0.94, 0.30, 0.00, 0.07]  [0.82, 0.40, 0.00, 0.12]  \n",
       "4   [0.95, 0.33, 0.00, 0.00]  [0.82, 0.44, 0.00, 0.00]  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_set.columns = ['name','time','total','prec: | JA | CH | KO | EN |','rec: | JA | CH | KO | EN |','f1: | JA | CH | KO | EN |']\n",
    "model_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cpu_time(s):\n",
    "    return (((s.stdout.splitlines())[0].split(','))[0].split('user '))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
