{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><strong>Capstone #3:</strong> <span style=\"color:darkred\">Supervised Learning</span> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:darkred\">__Part 1: Data Exploration__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-exploration.ipynb </span><br><br><span style=\"color:darkred\">__Part 2: Models__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-models.ipynb </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Part 2: </span><span style=\"color:darkblue\">L1 Prediction from ELL Writing Samples</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Ryan Harper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#ov'>Overview</a><br>\n",
    "<a href='#exp'>Experiment</a><br>\n",
    "<a href='#sec1'>1. Models:</a><br>\n",
    "><a href='#seca'>A. LR - Ordinary Least Squares</a><br>\n",
    "<a href='#secb'>B. LR - Logistic Regression</a> <a href='#secb1'> (Lasso)</a> <a href='#secb2'> (Ridge)</a><br>\n",
    "<a href='#secc'>C. NN - K Nearest Neighbors</a><br>\n",
    "<a href='#secd'>D. NN - Naive Bayes</a><br>\n",
    "<a href='#sece'>E. NN - Decision Tree</a><br>\n",
    "<a href='#secf'>F. Ensemble - Random Forest</a><br>\n",
    "\n",
    "<a href='#sec2'>2. Model Comparison</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ov\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">1. Models:</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iPython/Jupyter Notebook\n",
    "import time\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "from IPython.display import Image\n",
    "\n",
    "import time\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import plotly as plo\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import difflib\n",
    "\n",
    "# Stats\n",
    "from sklearn.metrics import classification_report, roc_curve,roc_auc_score,accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Preparing Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "# Models\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB,GaussianNB\n",
    "\n",
    "# Ensemble\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Visualization\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "# import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import Features + Target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('blogfeatures.csv').sample(frac=1.0)\n",
    "del features['Unnamed: 0']\n",
    "del features['id']\n",
    "del features['content']\n",
    "del features['pos']\n",
    "del features['pos2']\n",
    "del features['pos3']\n",
    "del features['tokens']\n",
    "del features['nns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Japanese               9536\n",
       "Traditional Chinese    3407\n",
       "Korean                 1122\n",
       "English                  65\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.language.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Declare X,y Variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14130,)\n",
      "(14130, 15450)\n"
     ]
    }
   ],
   "source": [
    "y = features['language'].values.reshape(-1, 1).ravel()\n",
    "X = features[features.columns[~features.columns.str.contains('language')]]\n",
    "X.head()\n",
    "\n",
    "print(np.shape(y))\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:darkblue\">C. Statistical Significance <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import median_test,mannwhitneyu,f_oneway\n",
    "def mw_test(a,b):\n",
    "    stat,p = mannwhitneyu(a,b, use_continuity=True, alternative=None)\n",
    "    return stat,p\n",
    "\n",
    "def moods_median_test(a,b,c,d):\n",
    "    stat, p, med, tbl = median_test(a,b,c,d)\n",
    "    return stat,p\n",
    "\n",
    "def f1way_test(a,b,c,d):\n",
    "    f,b = f_oneway(a,b,c,d)\n",
    "    return f,b\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B. Moodâ€™s Median test (2+ Non-Normally Distributed Independent Samples)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = ['id','content','language','tokens','pos','pos2','pos3','nns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = list(features.language.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "moodslist = {}\n",
    "for c in features.columns: \n",
    "    if c not in exclude: \n",
    "        g = [(features[c][features.language == l]) for l in lang]\n",
    "        stat,p = moods_median_test(g[0],g[1],g[2],g[3])\n",
    "        vals = 'stat={}, p={}'.format(stat,p)\n",
    "        if p < .05:\n",
    "            moodslist[c] = p\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SVD Truncate: To find most important features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=7,\n",
       "       random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\n",
    "svd.fit(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# best_features = [X.columns[i] for i in svd.components_[0].argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = list(moodslist.keys())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_features = ['letters_per','sent_pol','sent_subj','let1d','let1e','let1h','let1i','let1n','let1r','let1','let1s','let1t','prp_i','pos1_MD','pos1_TO','pos1_VBP','pos1_WRB','pos1_PRP',\n",
    "                 'pos1_DT',\n",
    "                 'pos1_NNP',\n",
    "                 'pos1_NNS',\n",
    "                 'pos1_IN',\n",
    "                 'let2_zh',\n",
    "                 'let2_ja',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_features =['let2_gg', 'pos2_EX-MD', 'let2_oz', 'sent_subj', 'wc', 'pos3_PRP-CD-WDT', 'let2_dj', 'pos3_IN-CC-VBP', 'sent_pol', 'pos3_IN-JJ-FW', 'pos3_JJS-VBG-PRP', 'let2_cb', 'let2_ud', 'pos2_VBZ-RBR', 'let2_cp', 'let2_fo', 'let1t', 'let2_bl', 'sc', 'let2_bc', 'pos3_VBG-RB-VB', 'let2_pg', 'pos2_RB-VB', 'let2_af', 'pos3_JJ-WRB-VB', 'pos3_IN-NN-WDT', 'let1q', 'adv_quite', 'pos3_CC-WDT-DT', 'pos3_PRP$-RB-WRB', 'let2_lt', 'pos3_RP-WDT-PRP', 'let1m', 'let2_mj', 'let2_yg', 'let2_rv', 'full_freq_score', 'pos2_CC-NNP', 'pos2_VB-NN', 'pos3_EX-IN-CC', 'let2_jy', 'pos3_FW-VBG-PRP$', 'pos3_CC-JJ-NN', 'let2_sy', 'let2_vc', 'let1i', 'let2_ru', 'let2_aw', 'pos3_PRP-NNP-WRB', 'pos3_NNS-NN-VBZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reduce Features (Unused): 14,665 to n_components __"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())  \n",
    "print(svd.singular_values_)\n",
    "svd.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split Data to Train/Test__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Using TrainTestSplit (Unused)_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[features.columns[~features.columns.str.contains('language')]], y, test_size=0.30, random_state=32)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Even Distribution Sampling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evenly_distributed_test = [60 japanese,60 english, 60 chinese, 60 korean]\n",
    "rs=33\n",
    "japset = features[features['language'] == 'Japanese'].sample(n=100, random_state=rs)\n",
    "korset = features[features['language'] == 'Korean'].sample(n=100, random_state=rs)\n",
    "chiset = features[features['language'] == 'Traditional Chinese'].sample(n=100, random_state=rs)\n",
    "engset = features[features['language'] == 'English'].sample(n=10, random_state=rs)\n",
    "testset = [japset,korset,chiset,engset]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# keep only two languages (unused)\n",
    "twolang = features[features['language'].isin(lang)]\n",
    "twolangset = [japset,chiset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.concat(testset)\n",
    "y_test = test_data['language'].values.reshape(-1, 1).ravel()\n",
    "X_test = test_data[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = features[~features.isin(test_data)].dropna()\n",
    "y_train = train_data['language'].values.reshape(-1, 1).ravel()\n",
    "X_train = train_data[best_features]\n",
    "# X_train = svd.transform(train_data[train_data.columns[~train_data.columns.str.contains('language')]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Function for Comparing Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['name','time','total','precision','recall','f1']\n",
    "\n",
    "model_set = pd.DataFrame(columns=cols)\n",
    "models_stored = []\n",
    "pattern = \"%.2f\"\n",
    "\n",
    "lang = list(set(y_train))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cpu_time(s):\n",
    "    return (((s.stdout.splitlines())[0].split(','))[0].split('user '))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,name):\n",
    "    global model_set\n",
    "    m = model\n",
    "    m.fit(X_train, y_train)\n",
    "    start = time.time()\n",
    "\n",
    "    total_score = m.score(X_test,y_test)\n",
    "    pscore = [pattern % i for i in list(metrics.precision_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    rscore = [pattern % i for i in list(metrics.recall_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    fscore = [pattern % i for i in list(metrics.f1_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    end = time.time()\n",
    "    t= pattern % (end - start)\n",
    "\n",
    "    r = dict(zip(cols,[name,t,total_score,pscore,rscore,fscore]))\n",
    "    print('Check for Overfitting: {}\\n'.format(m.score(X_train,y_train)))\n",
    "    print('Test Score is: {}\\n'.format(total_score))\n",
    "    print(classification_report(y_test, m.predict(X_test)))\n",
    "    \n",
    "    model_set = model_set.append(r,ignore_index=True)\n",
    "    return r,m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">A. LR - Logistic Regression</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Target is binary so logistic regression will operate on probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lreg_data,lreg = run_model(linear_model.LogisticRegression(),'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secb1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sece\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">E. K Nearest Neighbors</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can handle discrete values for target <br>Quantitative values are limited (not continuous) and might be problematic for nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "neighbors_data,neighbors = run_model(KNeighborsClassifier(n_neighbors=10),'K Nearest Neighbor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">F. Naive Bayes - Bernoulli</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bnb_data,bnb = run_model(GaussianNB(),'Naive Bayes - Bernoulli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">G. Decision Tree</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.744862518089725\n",
      "\n",
      "Test Score is: 0.4838709677419355\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            English       1.00      0.10      0.18        10\n",
      "           Japanese       0.40      0.94      0.56       100\n",
      "             Korean       0.87      0.20      0.33       100\n",
      "Traditional Chinese       0.66      0.35      0.46       100\n",
      "\n",
      "        avg / total       0.66      0.48      0.44       310\n",
      "\n",
      "CPU times: user 1.72 s, sys: 171 ms, total: 1.89 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dt_data,dt = run_model(tree.DecisionTreeClassifier(criterion='entropy',max_depth=6),'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Render tree.\n",
    "dot_data = tree.export_graphviz(\n",
    "    dt, \n",
    "    out_file=None,\n",
    "    feature_names=X_train.columns,\n",
    "    label= 'root',\n",
    "    proportion=False,\n",
    "    rounded=True,\n",
    "    class_names=lang,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n",
    "\n",
    "graph.write_png('decision_tree.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dimportance = list(zip(best_features,dt.feature_importances_))\n",
    "dimportance = dict(dimportance)\n",
    "a1_sorted_keys = sorted(dimportance, key=dimportance.get, reverse=True)\n",
    "p = []\n",
    "for r in a1_sorted_keys:\n",
    "    if dimportance[r] != 0:\n",
    "        p.append(r)\n",
    "#         print(r, dimportance[r])\n",
    "        \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Good visualization of important features and presentation of entropy weighting_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sech\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">H. Random Forest</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Runs decision tree multiple times for best output <br>Longest processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.7549204052098408\n",
      "\n",
      "Test Score is: 0.4774193548387097\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            English       0.00      0.00      0.00        10\n",
      "           Japanese       0.39      0.98      0.56       100\n",
      "             Korean       0.88      0.15      0.26       100\n",
      "Traditional Chinese       0.80      0.35      0.49       100\n",
      "\n",
      "        avg / total       0.67      0.48      0.42       310\n",
      "\n",
      "CPU times: user 1min 25s, sys: 702 ms, total: 1min 26s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_data,rf = run_model(ensemble.RandomForestClassifier(n_estimators=80,\n",
    "                                                       criterion='entropy',\n",
    "                                                       max_features=len(X_train.columns),\n",
    "                                                       max_depth=6),'Random Forest')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cvs = cross_val_score(rf, X_test, y_test, cv=5)\n",
    "print(cvs)\n",
    "print(cvs.sum()/len(cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_\n",
    "importance = dict(list(zip(X.columns,rf.feature_importances_)))\n",
    "importance_sorted = sorted(importance, key=importance.get, reverse=True)\n",
    "# for r in importance_sorted:\n",
    "#     if importance[r] >0:\n",
    "#         print(r, importance[r])\n",
    "print(importance_sorted[0:100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i_tree = 0\n",
    "for tree_in_forest in rf.estimators_:\n",
    "    dot_data = tree.export_graphviz(tree_in_forest, out_file=None,feature_names=X_train.columns,label= 'root',\n",
    "                                    proportion=False,rounded=True,class_names=lang,filled=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    Image(graph.create_png())\n",
    "    graph.write_png('decision_tree'+str(i_tree)+'.png')\n",
    "    i_tree = i_tree+1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">2. Model Comparison</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>total</th>\n",
       "      <th>prec: | JA | CH | KO | EN |</th>\n",
       "      <th>rec: | JA | CH | KO | EN |</th>\n",
       "      <th>f1: | JA | CH | KO | EN |</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.441935</td>\n",
       "      <td>[1.00, 0.38, 0.68, 1.00]</td>\n",
       "      <td>[0.01, 0.96, 0.39, 0.10]</td>\n",
       "      <td>[0.02, 0.55, 0.50, 0.18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K Nearest Neighbor</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.316129</td>\n",
       "      <td>[0.00, 0.32, 0.20, 0.00]</td>\n",
       "      <td>[0.00, 0.96, 0.02, 0.00]</td>\n",
       "      <td>[0.00, 0.48, 0.04, 0.00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes - Bernoulli</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.329032</td>\n",
       "      <td>[0.34, 0.35, 0.25, 0.00]</td>\n",
       "      <td>[0.29, 0.67, 0.06, 0.00]</td>\n",
       "      <td>[0.31, 0.46, 0.10, 0.00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes - Bernoulli</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.329032</td>\n",
       "      <td>[0.34, 0.35, 0.25, 0.00]</td>\n",
       "      <td>[0.29, 0.67, 0.06, 0.00]</td>\n",
       "      <td>[0.31, 0.46, 0.10, 0.00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>[0.87, 0.40, 0.66, 1.00]</td>\n",
       "      <td>[0.20, 0.94, 0.35, 0.10]</td>\n",
       "      <td>[0.33, 0.56, 0.46, 0.18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.477419</td>\n",
       "      <td>[0.88, 0.39, 0.80, 0.00]</td>\n",
       "      <td>[0.15, 0.98, 0.35, 0.00]</td>\n",
       "      <td>[0.26, 0.56, 0.49, 0.00]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name  time     total prec: | JA | CH | KO | EN |  \\\n",
       "0      Logistic Regression  0.04  0.441935    [1.00, 0.38, 0.68, 1.00]   \n",
       "1       K Nearest Neighbor  1.17  0.316129    [0.00, 0.32, 0.20, 0.00]   \n",
       "2  Naive Bayes - Bernoulli  0.12  0.329032    [0.34, 0.35, 0.25, 0.00]   \n",
       "3  Naive Bayes - Bernoulli  0.13  0.329032    [0.34, 0.35, 0.25, 0.00]   \n",
       "4            Decision Tree  0.02  0.483871    [0.87, 0.40, 0.66, 1.00]   \n",
       "5            Random Forest  0.06  0.477419    [0.88, 0.39, 0.80, 0.00]   \n",
       "\n",
       "  rec: | JA | CH | KO | EN | f1: | JA | CH | KO | EN |  \n",
       "0   [0.01, 0.96, 0.39, 0.10]  [0.02, 0.55, 0.50, 0.18]  \n",
       "1   [0.00, 0.96, 0.02, 0.00]  [0.00, 0.48, 0.04, 0.00]  \n",
       "2   [0.29, 0.67, 0.06, 0.00]  [0.31, 0.46, 0.10, 0.00]  \n",
       "3   [0.29, 0.67, 0.06, 0.00]  [0.31, 0.46, 0.10, 0.00]  \n",
       "4   [0.20, 0.94, 0.35, 0.10]  [0.33, 0.56, 0.46, 0.18]  \n",
       "5   [0.15, 0.98, 0.35, 0.00]  [0.26, 0.56, 0.49, 0.00]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_set.columns = ['name','time','total','prec: | JA | CH | KO | EN |','rec: | JA | CH | KO | EN |','f1: | JA | CH | KO | EN |']\n",
    "model_set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pprint(best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cpu_time(s):\n",
    "    return (((s.stdout.splitlines())[0].split(','))[0].split('user '))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
