{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><strong>Capstone #3:</strong> <span style=\"color:darkred\">Supervised Learning</span> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:darkred\">__Part 1: Data Exploration__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-exploration.ipynb </span><br><br><span style=\"color:darkred\">__Part 2: Models__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-models.ipynb </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Part 2: </span><span style=\"color:darkblue\">L1 Prediction from ELL Writing Samples</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Ryan Harper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#ov'>Overview</a><br>\n",
    "<a href='#exp'>Experiment</a><br>\n",
    "<a href='#sec1'>1. Models:</a><br>\n",
    "><a href='#seca'>A. LR - Ordinary Least Squares</a><br>\n",
    "<a href='#secb'>B. LR - Logistic Regression</a> <a href='#secb1'> (Lasso)</a> <a href='#secb2'> (Ridge)</a><br>\n",
    "<a href='#secc'>C. NN - K Nearest Neighbors</a><br>\n",
    "<a href='#secd'>D. NN - Naive Bayes</a><br>\n",
    "<a href='#sece'>E. NN - Decision Tree</a><br>\n",
    "<a href='#secf'>F. Ensemble - Random Forest</a><br>\n",
    "\n",
    "<a href='#sec2'>2. Model Comparison</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ov\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Overview</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Source:__\n",
    "> http://lang-8.com/ [scraped with Beautiful Soup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../data/language/lang8.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary:__\n",
    "> In my previous profession, I have been teaching English to a diverse range of students of all ages, language background, and country origin. During my professional development, I started to observe that different students with different L1s (1st Language) tended to display different patterns of communication that appeared to have some connection to either education in their country of origin or a connection to the linguistic structure of their first language. Different ELL (English Language Learners) needed to focus on different aspects of the English language depending on their background. The purpose of this project is to use a large number of blog posts from a language practicing website and explore whether or not the L1 has any significant impact on the blog writing style of the English learner.<br><br>Part 1: Explore the data to find any noteworthy trends in linguistic structure: <ol><li> vocabulary (word freq, collocations, and cognates) <li>syntax (sentence structure)<li>grammar (i.e. grammar complexity of sentences) <li>errors (types of errors) <li> parts of speech (NLTK Abbreviations: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/)</ol><br>Part 2: Use linguistic trends to determine whether or not a learner's first language can be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variables:__\n",
    ">__id:__ _User ID_<br>\n",
    "__time:__ _Time the blog post was scraped (in order of user posted time)_ <br>\n",
    "__title:__ _Title of the blog post_<br>\n",
    "__content:__ _The blog post_<br>\n",
    "__language:__ _User's self-reported first language_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Experiment</span> <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothesis:__ \n",
    "> L1 (first language) experience and academic environment influences ELLs' (English Language Learners') writing style. The L1 of ELLs can be predicted by looking at English blog posts and identifying patterns unique to their L1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__\n",
    "><li> --<li>--<li>--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Method:__\n",
    "> Using multiple different models. The aim of this project is to explore how different models can handle the data (target and features) and to see what information can be gained from using multiple different models. Ultimately, the goal is to determine which models are appropriate for a binary (discrete) target with features that are both qualitative (discrete) and quantitative (ranked/continuous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">1. Models:</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iPython/Jupyter Notebook\n",
    "import time\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "from IPython.display import Image\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import plotly as plo\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import difflib\n",
    "\n",
    "# Preparing Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Ensemble\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Visualization\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "# import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'language'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b97909615e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'blogfeatures.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4371\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'language'"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('blogfeatures.csv').sample(frac=1.0)\n",
    "\n",
    "lang = list(features.language.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = features['language'].values.reshape(-1, 1).ravel()\n",
    "X = features[features.columns[~features.columns.str.contains('language')]]\n",
    "X.head()\n",
    "\n",
    "print(np.shape(y))\n",
    "print(np.shape(X))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=39)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:gray\">A. LR - Ordinary Least Squares _(not used)_</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Target is discrete so this model may not be appropriate <br>Many features are binary so model may not be appropriate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%capture timeA --no-stderr\n",
    "%%time\n",
    "\n",
    "# Instantiate our model.\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Fit our model to our data.\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "regr.coef_\n",
    "# Display the attributes\n",
    "print('Coefficients: \\n')\n",
    "pprint(list(zip(X.columns,regr.coef_)))\n",
    "print('\\nIntercept: \\n', regr.intercept_)\n",
    "coef = list(zip(X.columns,regr.coef_))\n",
    "\n",
    "# Visualization of gender approximation\n",
    "x = list(range(len(y_test[0:30])))\n",
    "y =[0.5]*len(y_test[0:30])\n",
    "\n",
    "predY = regr.predict(X_test)\n",
    "plt.scatter(range(len(y_test[0:30])),predY[0:30],c='red',s=1)\n",
    "plt.scatter(range(len(y_test[0:30])),y_test[0:30],c='blue',s=1)\n",
    "plt.plot(x,y,c='black')\n",
    "plt.legend(['Threshold','Actual','Predicted'],loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# score is low because of approximate values with lin regression - values need to be rounded\n",
    "print(regr.score(X_test,y_test))\n",
    "\n",
    "# create vectorizer function for numpy\n",
    "vfunc =  np.vectorize(lambda val: int(round(abs(val))))\n",
    "\n",
    "# create final copy\n",
    "final = X_test.copy()\n",
    "final['gender'] = y_test\n",
    "final['guessval'] = vfunc(regr.predict(X_test).ravel())\n",
    "final.index.name = 'index'\n",
    "finaln = names.copy()\n",
    "finaln.index.name = 'index'\n",
    "final = final.join(finaln['name'], how='left', on='index')\n",
    "\n",
    "print('{0:f}% of names were correctly identified by gender'.format(\n",
    "    100*len(final.loc[final['gender'] == final['guessval']]) / len(final)))\n",
    "scoreA = len(final.loc[final['gender'] == final['guessval']]) / len(final)\n",
    "del final\n",
    "del finaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secb\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">B. LR - Logistic Regression</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Target is binary so logistic regression will operate on probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture timeB --no-stderr\n",
    "%%time\n",
    "\n",
    "# Instantiate our model.\n",
    "lregr = linear_model.LogisticRegression()\n",
    "\n",
    "# Fit our model to our data.\n",
    "lregr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "lregr.coef_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Display the attributes\n",
    "print('Coefficients: \\n')\n",
    "pprint(list(zip(X.columns,lregr.coef_[0])))\n",
    "print('\\nIntercept: \\n', lregr.intercept_)\n",
    "coef = list(zip(X.columns,lregr.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lregr.score(X_test,y_test))\n",
    "scoreB = lregr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secb1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:gray\">C. Lasso _(not used)_</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "lamvalues = [.1,.25,.5,.75,1,3,5,10]\n",
    "\n",
    "for lam in lamvalues:\n",
    "    # Instantiate our model.\n",
    "    lasso = linear_model.Lasso(alpha=lam,fit_intercept=False)\n",
    "    \n",
    "    # Fit our model to our data.\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    print('\\u03bb={} \\tLasso Score: {}'.format(lam, lasso.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secb2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:gray\">D. Ridge _(not used)_</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lamvalues = [.1,.5,.75,1,3,5,10]\n",
    "\n",
    "for lam in lamvalues:\n",
    "    # Instantiate our model.\n",
    "    ridg = linear_model.Ridge(alpha=lam,fit_intercept=False)\n",
    "    \n",
    "    # Fit our model to our data.\n",
    "    ridg.fit(X_train, y_train)\n",
    "    \n",
    "    print('\\u03bb={} \\tRidge Score: {}'.format(lam, ridg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lasso and Ridge are not good predictors so should I just be using them for parameter manipulation?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sece\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">E. K Nearest Neighbors</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can handle discrete values for target <br>Quantitative values are limited (not continuous) and might be problematic for nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture timeE --no-stderr\n",
    "%%time\n",
    "neighbors = KNeighborsClassifier(n_neighbors=5)\n",
    "neighbors.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neighbors.score(X_test,y_test))\n",
    "scoreE = neighbors.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">F. Naive Bayes - Bernoulli</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Should be best for boolean classification but has lowest prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture timeF --no-stderr\n",
    "%%time\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bnb.score(X_test,y_test))\n",
    "scoreF = bnb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">G. Decision Tree</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visualizes most important features by hierarchy <br>Longer processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture timeG --no-stderr\n",
    "%%time\n",
    "\n",
    "# Initialize and train our tree.\n",
    "decision_tree = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=6)\n",
    "\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render our tree.\n",
    "dot_data = tree.export_graphviz(\n",
    "    decision_tree, out_file=None,\n",
    "    feature_names=X.columns,\n",
    "    class_names=['Japanese', 'Korean', 'Traditional Chinese', 'English'],\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n",
    "\n",
    "graph.write_png('decision_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreG=decision_tree.score(X_test,y_test)\n",
    "decision_tree.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Good visualization of important features and presentation of entropy weighting_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sech\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">H. Random Forest</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Runs decision tree multiple times for best output <br>Longest processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture timeH --no-stderr\n",
    "%%time\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=20)\n",
    "rfc.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvs = cross_val_score(rfc, X_test, y_test.ravel(), cv=5)\n",
    "print(cvs)\n",
    "scoreH = cvs.sum()/len(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.feature_importances_\n",
    "importance = list(zip(X.columns,rfc.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">2. Model Comparison</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimportance = dict(importance)\n",
    "\n",
    "# a1_sorted_keys = sorted(dimportance, key=dimportance.get, reverse=True)\n",
    "# for r in a1_sorted_keys:\n",
    "#     print(r, dimportance[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3373e05e9d73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Logistic Regression'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'K Nearest Neigbbors'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Naive Bayes Bernoulli'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Decision Tree'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Random Forest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtimeB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtimesfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtimesfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timeB' is not defined"
     ]
    }
   ],
   "source": [
    "models = ['Logistic Regression','K Nearest Neigbbors','Naive Bayes Bernoulli','Decision Tree','Random Forest']\n",
    "times = [timeB,timeE,timeF,timeG,timeH]\n",
    "timesfinal = []\n",
    "for time in times:\n",
    "    timesfinal.append((((time.stdout.splitlines())[0].split(','))[0].split('user '))[1])\n",
    "score = [scoreB,scoreE,scoreF,scoreG,scoreH]\n",
    "\n",
    "dfModels = pd.DataFrame(index=models)\n",
    "dfModels['cpu time'] = timesfinal\n",
    "dfModels['score'] = score\n",
    "dfModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
