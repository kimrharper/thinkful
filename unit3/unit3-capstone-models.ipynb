{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><strong>Capstone #3:</strong> <span style=\"color:darkred\">Supervised Learning</span> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:darkred\">__Part 1: Data Exploration__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-exploration.ipynb </span><br><br><span style=\"color:darkred\">__Part 2: Models__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-models.ipynb </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Part 2: </span><span style=\"color:darkblue\">L1 Prediction from ELL Writing Samples</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Ryan Harper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#ov'>Overview</a><br>\n",
    "<a href='#exp'>Experiment</a><br>\n",
    "<a href='#sec1'>1. Models:</a><br>\n",
    "><a href='#seca'>A. LR - Ordinary Least Squares</a><br>\n",
    "<a href='#secb'>B. LR - Logistic Regression</a> <a href='#secb1'> (Lasso)</a> <a href='#secb2'> (Ridge)</a><br>\n",
    "<a href='#secc'>C. NN - K Nearest Neighbors</a><br>\n",
    "<a href='#secd'>D. NN - Naive Bayes</a><br>\n",
    "<a href='#sece'>E. NN - Decision Tree</a><br>\n",
    "<a href='#secf'>F. Ensemble - Random Forest</a><br>\n",
    "\n",
    "<a href='#sec2'>2. Model Comparison</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ov\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">1. Models:</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iPython/Jupyter Notebook\n",
    "import time\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "from IPython.display import Image\n",
    "\n",
    "import time\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import plotly as plo\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import difflib\n",
    "\n",
    "# Stats\n",
    "from sklearn.metrics import classification_report, roc_curve,roc_auc_score,accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Preparing Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "# Models\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB,GaussianNB\n",
    "\n",
    "# Ensemble\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Visualization\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "# import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import Features + Target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('blogfeatures.csv').sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features = features[features['sc']<15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "del features['Unnamed: 0']\n",
    "del features['id']\n",
    "del features['content']\n",
    "del features['pos']\n",
    "del features['pos2']\n",
    "del features['pos3']\n",
    "del features['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Japanese               9536\n",
       "Traditional Chinese    3407\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.language.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Declare X,y Variables__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y = features['language'].values.reshape(-1, 1).ravel()\n",
    "X = features[features.columns[~features.columns.str.contains('language')]]\n",
    "X.head()\n",
    "\n",
    "print(np.shape(y))\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:darkblue\">C. Statistical Significance <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import median_test,mannwhitneyu,f_oneway\n",
    "def mw_test(a,b):\n",
    "    stat,p = mannwhitneyu(a,b, use_continuity=True, alternative=None)\n",
    "    return stat,p\n",
    "\n",
    "def moods_median_test(vals):\n",
    "    stat, p, med, tbl = median_test(*vals)\n",
    "    return stat,p\n",
    "\n",
    "def f1way_test(a,b,c,d):\n",
    "    f,b = f_oneway(a,b,c,d)\n",
    "    return f,b\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B. Moodâ€™s Median test (2+ Non-Normally Distributed Independent Samples)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Traditional Chinese', 'Japanese']\n"
     ]
    }
   ],
   "source": [
    "lang = list(features.language.unique())\n",
    "print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "moodslist = {}\n",
    "\n",
    "for c in features.columns: \n",
    "    if c not in exclude: \n",
    "        g = [(features[c][features.language == l]) for l in lang]\n",
    "        stat,p = moods_median_test(g)\n",
    "        vals = 'stat={}, p={}'.format(stat,p)\n",
    "        if p < .05:\n",
    "            moodslist[c] = p\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nonbinary = ['language','letters_per','wc','sc','sent_pol','sent_subj','cap_let','punc_count','freq_score','full_freq_score']\n",
    "binary = features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for b in binary.columns:\n",
    "    if b not in nonbinary:\n",
    "        binary[b] = binary[b].apply(lambda val: 0 if val == 0 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SVD Truncate: To find most important features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=7,\n",
       "       random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\n",
    "svd.fit(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# best_features = [X.columns[i] for i in svd.components_[0].argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = list(moodslist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['pos3_JJ-FW-FW', 'sc', 'pos3_DT-JJS-VBZ', 'sent_subj', 'punc_count', 'pos3_NN-POS-WRB', 'wc', 'pos3_JJR-JJ-VBZ', 'prep_like', 'pos3_PRP$-VBG-CC', 'pos3_JJ-PRP-DT', 'pos3_NNS-WRB-JJ', 'pos3_PDT-DT-NN', 'pos3_VBD-CD-WRB', 'pos3_PRP-FW-PRP', 'pos3_NNP-VBD-JJ', 'pos3_RB-PRP$-MD', 'pos3_VBZ-JJR-JJ', 'pos3_JJ-TO-PRP', 'letters_per', 'prep_across', 'prp_he', 'pos3_JJ-CD-JJ', 'pos3_VB-VBG-NNS', 'pos3_VBZ-VBG-JJR', 'pos3_VBZ-PRP$-IN', 'sent_pol', 'pos3_RB-VBG-VB', 'pos3_NN-VBD-MD', 'full_freq_score', 'cap_let', 'freq_score', 'pos3_VB-RB-MD', 'pos3_PRP-VBP-VBD', 'pos3_NNP-NNS-NNP', 'pos3_NN-UH-VB', 'pos3_PRP$-NN-POS', 'pos3_PRP-IN-JJS', 'pos3_PRP$-RP-IN', 'prep_while', 'pos3_TO-CC-WRB', 'pos3_NN-CC-VBD', 'pos3_VBD-CD-JJ', 'pos3_RB-PRP$-CD', 'pos3_VB-PRP-WRB', 'pos3_PRP-RBR-PRP', 'pos3_VB-NNP-MD', 'pos3_VB-JJ-SYM', 'pos3_VBZ-VB-DT', 'pos3_NNP-NN-RP', 'pos3_CD-NN-RB', 'pos3_RP-NN-RB', 'pos3_VBG-NNS-RP', 'pos3_WDT-PRP-VBD', 'pos3_RP-RB-TO', 'pos3_WRB-PRP-NNS', 'pos3_JJS-JJ-VBP', 'pos3_VBZ-JJR-DT', 'pos3_RB-NNS-VBD', 'pos3_CD-JJR-CD', 'pos3_NNPS-VBN-IN', 'pos3_VBZ-CC-VBG', 'pos3_WRB-CC-VB', 'pos3_NNS-RB-RB', 'pos3_JJ-POS-MD', 'pos3_RBR-CC-VB', 'pos3_CC-JJ-VBD', 'pos3_MD-IN-DT', 'prep_between', 'pos3_CC-CC-PRP', 'pos3_VBP-NN-VBN', 'pos3_PRP-CD-JJ', 'pos3_RP-NNS-WP', 'pos3_RB-MD-RBR', 'pos3_JJ-WRB-PRP$', 'pos3_PRP-WP-PRP', 'pos3_JJ-RB-VBZ', 'pos3_NNS-DT-VBP', 'pos3_VB-VB-CC', 'pos3_VBN-VBD-TO', 'pos3_RP-RP-NN', 'pos3_RB-WP-NN', 'pos3_VBN-EX-VBZ', 'pos3_VB-TO-RBR', 'pos3_JJS-DT-VBZ', 'pos3_RB-IN-PRP$', 'pos3_CC-WDT-MD', 'pos3_RB-JJ-PDT', 'pos3_VBN-PRP-RP', 'pos3_VBZ-NNP-DT', 'pos3_CC-VBN-RB', 'pos3_VBP-NNS-NNS', 'pos3_VBN-PRP$-JJS', 'pos3_JJ-CD-CC', 'pos3_CC-VB-JJR', 'pos3_VBP-POS-DT', 'pos3_SYM-CD-NNP', 'cc_et', 'pos3_RP-RB-PRP', 'pos3_JJ-TO-PRP$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = features.columns[~features.columns.str.contains('language')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reduce Features (Unused): 14,665 to n_components __"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())  \n",
    "print(svd.singular_values_)\n",
    "svd.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split Data to Train/Test__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Even Distribution Sampling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Chinese\n",
      "Japanese\n"
     ]
    }
   ],
   "source": [
    "# evenly_distributed_test = [60 japanese,60 english, 60 chinese, 60 korean]\n",
    "langlist = []\n",
    "rs=53\n",
    "for l in lang:\n",
    "    print(l)\n",
    "    langlist.append(features[features['language'] == l].sample(n=300, random_state=rs))\n",
    "\n",
    "testset = langlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.concat(testset)\n",
    "y_test = test_data['language'].values.reshape(-1, 1).ravel()\n",
    "X_test = test_data[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 s, sys: 10.3 s, total: 31.7 s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = features[~features.isin(test_data)].dropna()\n",
    "y_train = train_data['language'].values.reshape(-1, 1).ravel()\n",
    "X_train = train_data[best_features]\n",
    "# X_train = svd.transform(train_data[train_data.columns[~train_data.columns.str.contains('language')]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Function for Comparing Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['name','time','total','precision','recall','f1']\n",
    "\n",
    "model_set = pd.DataFrame(columns=cols)\n",
    "models_stored = []\n",
    "pattern = \"%.2f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,name):\n",
    "    global model_set\n",
    "    m = model\n",
    "    m.fit(X_train, y_train)\n",
    "    start = time.time()\n",
    "\n",
    "    total_score = m.score(X_test,y_test)\n",
    "    pscore = [pattern % i for i in list(metrics.precision_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    rscore = [pattern % i for i in list(metrics.recall_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    fscore = [pattern % i for i in list(metrics.f1_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    end = time.time()\n",
    "    t= pattern % (end - start)\n",
    "\n",
    "    r = dict(zip(cols,[name,t,total_score,pscore,rscore,fscore]))\n",
    "    print('Check for Overfitting: {}\\n'.format(m.score(X_train,y_train)))\n",
    "    print('Test Score is: {}\\n'.format(total_score))\n",
    "    print(classification_report(y_test, m.predict(X_test)))\n",
    "    \n",
    "    model_set = model_set.append(r,ignore_index=True)\n",
    "    return r,m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">A. LR - Logistic Regression</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Target is binary so logistic regression will operate on probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.8164951794539415\n",
      "\n",
      "Test Score is: 0.655\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Japanese       0.59      0.98      0.74       300\n",
      "Traditional Chinese       0.93      0.33      0.49       300\n",
      "\n",
      "        avg / total       0.76      0.66      0.62       600\n",
      "\n",
      "CPU times: user 6.41 s, sys: 1.23 s, total: 7.65 s\n",
      "Wall time: 5.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lreg_data,lreg = run_model(linear_model.LogisticRegression(),'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secb1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sece\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">E. K Nearest Neighbors</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can handle discrete values for target <br>Quantitative values are limited (not continuous) and might be problematic for nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.7531394312565827\n",
      "\n",
      "Test Score is: 0.49666666666666665\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Japanese       0.50      0.98      0.66       300\n",
      "Traditional Chinese       0.42      0.02      0.03       300\n",
      "\n",
      "        avg / total       0.46      0.50      0.35       600\n",
      "\n",
      "CPU times: user 1min 7s, sys: 3.75 s, total: 1min 11s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "neighbors_data,neighbors = run_model(KNeighborsClassifier(n_neighbors=2),'K Nearest Neighbor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">F. Naive Bayes - Bernoulli</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.7416349347808474\n",
      "\n",
      "Test Score is: 0.6433333333333333\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Japanese       0.61      0.82      0.70       300\n",
      "Traditional Chinese       0.72      0.47      0.57       300\n",
      "\n",
      "        avg / total       0.66      0.64      0.63       600\n",
      "\n",
      "CPU times: user 13.2 s, sys: 4.61 s, total: 17.8 s\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bnb_data,bnb = run_model(BernoulliNB(),'Naive Bayes - Bernoulli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">G. Decision Tree</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.8186826541359475\n",
      "\n",
      "Test Score is: 0.645\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Japanese       0.59      0.95      0.73       300\n",
      "Traditional Chinese       0.87      0.34      0.49       300\n",
      "\n",
      "        avg / total       0.73      0.65      0.61       600\n",
      "\n",
      "CPU times: user 5.28 s, sys: 774 ms, total: 6.05 s\n",
      "Wall time: 6.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dt_data,dt = run_model(tree.DecisionTreeClassifier(criterion='entropy',max_depth=7),'Decision Tree')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Render tree.\n",
    "dot_data = tree.export_graphviz(\n",
    "    dt, \n",
    "    out_file=None,\n",
    "    feature_names=X_train.columns,\n",
    "    label= 'root',\n",
    "    proportion=False,\n",
    "    rounded=True,\n",
    "    class_names=lang,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n",
    "\n",
    "graph.write_png('decision_tree.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dimportance = list(zip(best_features,dt.feature_importances_))\n",
    "dimportance = dict(dimportance)\n",
    "a1_sorted_keys = sorted(dimportance, key=dimportance.get, reverse=True)\n",
    "p = []\n",
    "for r in a1_sorted_keys:\n",
    "    if dimportance[r] != 0:\n",
    "        p.append(r)\n",
    "#         print(r, dimportance[r])\n",
    "        \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Good visualization of important features and presentation of entropy weighting_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sech\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">H. Random Forest</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Runs decision tree multiple times for best output <br>Longest processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.7609171190148262\n",
      "\n",
      "Test Score is: 0.5133333333333333\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Japanese       0.51      1.00      0.67       300\n",
      "Traditional Chinese       1.00      0.03      0.05       300\n",
      "\n",
      "        avg / total       0.75      0.51      0.36       600\n",
      "\n",
      "CPU times: user 7.23 s, sys: 724 ms, total: 7.95 s\n",
      "Wall time: 8.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_data,rf = run_model(ensemble.RandomForestClassifier(n_estimators=150,\n",
    "                                                       criterion='entropy',\n",
    "#                                                        max_features=len(X_train.columns),\n",
    "                                                       max_depth=8),'Random Forest')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cvs = cross_val_score(rf, X_test, y_test, cv=5)\n",
    "print(cvs)\n",
    "print(cvs.sum()/len(cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_\n",
    "importance = dict(list(zip(X.columns,rf.feature_importances_)))\n",
    "importance_sorted = sorted(importance, key=importance.get, reverse=True)\n",
    "# for r in importance_sorted:\n",
    "#     if importance[r] >0:\n",
    "#         print(r, importance[r])\n",
    "print(importance_sorted[0:100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i_tree = 0\n",
    "for tree_in_forest in rf.estimators_:\n",
    "    dot_data = tree.export_graphviz(tree_in_forest, out_file=None,feature_names=X_train.columns,label= 'root',\n",
    "                                    proportion=False,rounded=True,class_names=lang,filled=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    Image(graph.create_png())\n",
    "    graph.write_png('decision_tree'+str(i_tree)+'.png')\n",
    "    i_tree = i_tree+1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">2. Model Comparison</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>total</th>\n",
       "      <th>prec: | JA | CH | KO | EN |</th>\n",
       "      <th>rec: | JA | CH | KO | EN |</th>\n",
       "      <th>f1: | JA | CH | KO | EN |</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.556667</td>\n",
       "      <td>[0.84, 0.53]</td>\n",
       "      <td>[0.14, 0.97]</td>\n",
       "      <td>[0.24, 0.69]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K Nearest Neighbor</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>[0.30, 0.50]</td>\n",
       "      <td>[0.01, 0.98]</td>\n",
       "      <td>[0.02, 0.66]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes - Bernoulli</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>[0.72, 0.53]</td>\n",
       "      <td>[0.15, 0.94]</td>\n",
       "      <td>[0.24, 0.67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>[0.67, 0.52]</td>\n",
       "      <td>[0.13, 0.93]</td>\n",
       "      <td>[0.22, 0.67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>[1.00, 0.52]</td>\n",
       "      <td>[0.06, 1.00]</td>\n",
       "      <td>[0.11, 0.68]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name  time     total prec: | JA | CH | KO | EN |  \\\n",
       "0      Logistic Regression  0.02  0.556667                [0.84, 0.53]   \n",
       "1       K Nearest Neighbor  0.04  0.493333                [0.30, 0.50]   \n",
       "2  Naive Bayes - Bernoulli  0.01  0.545000                [0.72, 0.53]   \n",
       "3            Decision Tree  0.01  0.533333                [0.67, 0.52]   \n",
       "4            Random Forest  0.07  0.530000                [1.00, 0.52]   \n",
       "\n",
       "  rec: | JA | CH | KO | EN | f1: | JA | CH | KO | EN |  \n",
       "0               [0.14, 0.97]              [0.24, 0.69]  \n",
       "1               [0.01, 0.98]              [0.02, 0.66]  \n",
       "2               [0.15, 0.94]              [0.24, 0.67]  \n",
       "3               [0.13, 0.93]              [0.22, 0.67]  \n",
       "4               [0.06, 1.00]              [0.11, 0.68]  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_set.columns = ['name','time','total','prec: | JA | CH | KO | EN |','rec: | JA | CH | KO | EN |','f1: | JA | CH | KO | EN |']\n",
    "model_set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pprint(best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cpu_time(s):\n",
    "    return (((s.stdout.splitlines())[0].split(','))[0].split('user '))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
