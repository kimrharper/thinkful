{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><strong>Capstone #3:</strong> <span style=\"color:darkred\">Supervised Learning</span> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:darkred\">__Part 1: Data Exploration__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-exploration.ipynb </span><br><br><span style=\"color:darkred\">__Part 2: Models__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-models.ipynb </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Part 2: </span><span style=\"color:darkblue\">L1 Prediction from ELL Writing Samples</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Ryan Harper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#ov'>Overview</a><br>\n",
    "<a href='#exp'>Experiment</a><br>\n",
    "<a href='#sec1'>1. Models:</a><br>\n",
    "><a href='#seca'>A. LR - Ordinary Least Squares</a><br>\n",
    "<a href='#secb'>B. LR - Logistic Regression</a> <a href='#secb1'> (Lasso)</a> <a href='#secb2'> (Ridge)</a><br>\n",
    "<a href='#secc'>C. NN - K Nearest Neighbors</a><br>\n",
    "<a href='#secd'>D. NN - Naive Bayes</a><br>\n",
    "<a href='#sece'>E. NN - Decision Tree</a><br>\n",
    "<a href='#secf'>F. Ensemble - Random Forest</a><br>\n",
    "\n",
    "<a href='#sec2'>2. Model Comparison</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ov\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Overview</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Source:__\n",
    "> http://lang-8.com/ [scraped with Beautiful Soup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../data/language/lang8.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary:__\n",
    "> In my previous profession, I have been teaching English to a diverse range of students of all ages, language background, and country origin. During my professional development, I started to observe that different students with different L1s (1st Language) tended to display different patterns of communication that appeared to have some connection to either education in their country of origin or a connection to the linguistic structure of their first language. Different ELL (English Language Learners) needed to focus on different aspects of the English language depending on their background. The purpose of this project is to use a large number of blog posts from a language practicing website and explore whether or not the L1 has any significant impact on the blog writing style of the English learner.<br><br>Part 1: Explore the data to find any noteworthy trends in linguistic structure: <ol><li> vocabulary (word freq, collocations, and cognates) <li>syntax (sentence structure)<li>grammar (i.e. grammar complexity of sentences) <li>errors (types of errors) <li> parts of speech (NLTK Abbreviations: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/)</ol><br>Part 2: Use linguistic trends to determine whether or not a learner's first language can be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variables:__\n",
    ">__id:__ _User ID_<br>\n",
    "__time:__ _Time the blog post was scraped (in order of user posted time)_ <br>\n",
    "__title:__ _Title of the blog post_<br>\n",
    "__content:__ _The blog post_<br>\n",
    "__language:__ _User's self-reported first language_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Experiment</span> <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothesis:__ \n",
    "> L1 (first language) experience and academic environment influences ELLs' (English Language Learners') writing style. The L1 of ELLs can be predicted by looking at English blog posts and identifying patterns unique to their L1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__\n",
    "><li> --<li>--<li>--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Method:__\n",
    "> Using multiple different models. The aim of this project is to explore how different models can handle the data (target and features) and to see what information can be gained from using multiple different models. Ultimately, the goal is to determine which models are appropriate for a binary (discrete) target with features that are both qualitative (discrete) and quantitative (ranked/continuous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">1. Models:</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iPython/Jupyter Notebook\n",
    "import time\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "from IPython.display import Image\n",
    "\n",
    "import time\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import plotly as plo\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import difflib\n",
    "\n",
    "# Stats\n",
    "from sklearn.metrics import classification_report, roc_curve,roc_auc_score,accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Preparing Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "# Models\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB,GaussianNB\n",
    "\n",
    "# Ensemble\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Visualization\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "# import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import Features + Target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('blogfeatures.csv').sample(frac=1.0)\n",
    "del features['Unnamed: 0']\n",
    "del features['id']\n",
    "del features['content']\n",
    "del features['pos']\n",
    "del features['pos2']\n",
    "del features['pos3']\n",
    "del features['tokens']\n",
    "\n",
    "lang = list(features.language.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Japanese               9536\n",
       "Traditional Chinese    3407\n",
       "Korean                 1122\n",
       "English                  65\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.language.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Declare X,y Variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14130,)\n",
      "(14130, 14835)\n"
     ]
    }
   ],
   "source": [
    "y = features['language'].values.reshape(-1, 1).ravel()\n",
    "X = features[features.columns[~features.columns.str.contains('language')]]\n",
    "X.head()\n",
    "\n",
    "print(np.shape(y))\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SVD Truncate: To find most important features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=7,\n",
       "       random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\n",
    "svd.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = [X.columns[i] for i in svd.components_[0].argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_features = ['letters_per',\n",
    "                 'sent_pol',\n",
    "                 'sent_subj',\n",
    "                 'let1d',\n",
    "                 'let1e',\n",
    "                 'let1h',\n",
    "                 'let1i',\n",
    "                 'let1n',\n",
    "                 'let1r',\n",
    "                 'let1',\n",
    "                 'let1s',\n",
    "                 'let1t',\n",
    "                 'prp_i',\n",
    "                 'pos1_MD',\n",
    "                 'pos1_TO',\n",
    "                 'pos1_VBP',\n",
    "                 'pos1_WRB',\n",
    "                 'pos1_PRP',\n",
    "                 'pos1_DT',\n",
    "                 'pos1_NNP',\n",
    "                 'pos1_NNS',\n",
    "                 'pos1_IN',\n",
    "                 'let2_zh',\n",
    "                 'let2_ja',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reduce Features (Unused): 14,665 to n_components __"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())  \n",
    "print(svd.singular_values_)\n",
    "svd.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split Data to Train/Test__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Using TrainTestSplit (Unused)_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[features.columns[~features.columns.str.contains('language')]], y, test_size=0.30, random_state=32)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Even Distribution Sampling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evenly_distributed_test = [60 japanese,60 english, 60 chinese, 60 korean]\n",
    "rs=33\n",
    "japset = features[features['language'] == 'Japanese'].sample(n=30, random_state=rs)\n",
    "korset = features[features['language'] == 'Korean'].sample(n=30, random_state=rs)\n",
    "chiset = features[features['language'] == 'Traditional Chinese'].sample(n=30, random_state=rs)\n",
    "engset = features[features['language'] == 'English'].sample(n=30, random_state=rs)\n",
    "testset = [japset,korset,chiset,engset]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# keep only two languages (unused)\n",
    "twolang = features[features['language'].isin(['Japanese','Traditional Chinese'])]\n",
    "twolangset = [japset,chiset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.concat(testset)\n",
    "y_test = test_data['language'].values.reshape(-1, 1).ravel()\n",
    "X_test = test_data[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = features[~features.isin(test_data)].dropna()\n",
    "y_train = train_data['language'].values.reshape(-1, 1).ravel()\n",
    "X_train = train_data[best_features]\n",
    "# X_train = svd.transform(train_data[train_data.columns[~train_data.columns.str.contains('language')]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Function for Comparing Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['name','time','total','precision','recall','f1']\n",
    "\n",
    "model_set = pd.DataFrame(columns=cols)\n",
    "models_stored = []\n",
    "pattern = \"%.2f\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cpu_time(s):\n",
    "    return (((s.stdout.splitlines())[0].split(','))[0].split('user '))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,name):\n",
    "    global model_set\n",
    "    m = model\n",
    "    m.fit(X_train, y_train)\n",
    "    start = time.time()\n",
    "\n",
    "    total_score = m.score(X_test,y_test)\n",
    "    pscore = [pattern % i for i in list(metrics.precision_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    rscore = [pattern % i for i in list(metrics.recall_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    fscore = [pattern % i for i in list(metrics.f1_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    end = time.time()\n",
    "    t= pattern % (end - start)\n",
    "\n",
    "    r = dict(zip(cols,[name,t,total_score,pscore,rscore,fscore]))\n",
    "    print('Check for Overfitting: {}\\n'.format(m.score(X_train,y_train)))\n",
    "    print('Test Score is: {}\\n'.format(total_score))\n",
    "    print(classification_report(y_test, m.predict(X_test)))\n",
    "    \n",
    "    model_set = model_set.append(r,ignore_index=True)\n",
    "    return r,m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">A. LR - Logistic Regression</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Target is binary so logistic regression will operate on probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lreg_data,lreg = run_model(linear_model.LogisticRegression(),'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lreg.predict_proba(X_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test,lreg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.std(X)*lreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secb1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sece\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">E. K Nearest Neighbors</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can handle discrete values for target <br>Quantitative values are limited (not continuous) and might be problematic for nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "neighbors_data,neighbors = run_model(KNeighborsClassifier(n_neighbors=5),'K Nearest Neighbor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">F. Naive Bayes - Bernoulli</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Multinomial (Default prior distribution)\n",
    "Bernoulli (Binomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Should be best for boolean classification but not for multiple discrete values (multinomial should be better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bnb_data,bnb = run_model(GaussianNB(),'Naive Bayes - Bernoulli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">G. Decision Tree</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visualizes most important features by hierarchy <br>Longer processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_data,dt = run_model(tree.DecisionTreeClassifier(criterion='entropy',max_depth=5),'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render tree.\n",
    "dot_data = tree.export_graphviz(\n",
    "    dt, \n",
    "    out_file=None,\n",
    "    feature_names=X_train.columns,\n",
    "    label= 'root',\n",
    "    proportion=False,\n",
    "    rounded=True,\n",
    "    class_names=lang,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n",
    "\n",
    "graph.write_png('decision_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dimportance = list(zip(best_features,dt.feature_importances_))\n",
    "dimportance = dict(dimportance)\n",
    "a1_sorted_keys = sorted(dimportance, key=dimportance.get, reverse=True)\n",
    "p = []\n",
    "for r in a1_sorted_keys:\n",
    "    if dimportance[r] != 0:\n",
    "        p.append(r)\n",
    "#         print(r, dimportance[r])\n",
    "        \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Good visualization of important features and presentation of entropy weighting_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sech\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">H. Random Forest</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Runs decision tree multiple times for best output <br>Longest processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_data,rf = run_model(ensemble.RandomForestClassifier(n_estimators=8,criterion='entropy',max_features=len(X_train.columns),max_depth=5),'Random Forest')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cvs = cross_val_score(rf, X_test, y_test, cv=5)\n",
    "print(cvs)\n",
    "print(cvs.sum()/len(cvs))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rf.feature_importances_\n",
    "importance = list(zip(X.columns,rf.feature_importances_))\n",
    "\n",
    "dimportance = dict(importance)\n",
    "\n",
    "a1_sorted_keys = sorted(dimportance, key=dimportance.get, reverse=True)\n",
    "# for r in a1_sorted_keys:\n",
    "#     if dimportance[r] >0:\n",
    "#         print(r, dimportance[r])\n",
    "\n",
    "print(a1_sorted_keys[0:36])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i_tree = 0\n",
    "for tree_in_forest in rf.estimators_:\n",
    "    dot_data = tree.export_graphviz(\n",
    "        tree_in_forest, \n",
    "        out_file=None,\n",
    "        feature_names=X_train.columns,\n",
    "        label= 'root',\n",
    "        proportion=False,\n",
    "        rounded=True,\n",
    "        class_names=lang,\n",
    "        filled=True\n",
    "    )\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    Image(graph.create_png())\n",
    "\n",
    "    graph.write_png('decision_tree'+str(i_tree)+'.png')\n",
    "    i_tree = i_tree+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "featureImportance = clf.feature_importances_\n",
    "for i in xrange(len(featureImportance)):\n",
    "    print featureImportance[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">2. Model Comparison</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_set.columns = ['name','time','total','prec: | JA | CH | KO | EN |','rec: | JA | CH | KO | EN |','f1: | JA | CH | KO | EN |']\n",
    "model_set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pprint(best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cpu_time(s):\n",
    "    return (((s.stdout.splitlines())[0].split(','))[0].split('user '))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
