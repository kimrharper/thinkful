{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><strong>Capstone #3:</strong> <span style=\"color:darkred\">Supervised Learning</span> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### </span>__Part 1: Data Exploration__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-exploration.ipynb </span><br><br><span>__Part 2: Analysis__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-analysis.ipynb </span><br><br><span>__Part 3: Models__ https://github.com/kimrharper/thinkful/blob/master/unit3/unit3-capstone-models.ipynb </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">L1 Prediction from ELL Writing Samples</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Part 3: </span><span style=\"color:darkblue\">Models</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Ryan Harper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#ov'>Overview</a><br>\n",
    "<a href='#exp'>Experiment</a><br>\n",
    "<a href='#sec1'>1. Models:</a><br>\n",
    "><a href='#seca'>A. LR - Ordinary Least Squares</a><br>\n",
    "<a href='#secb'>B. LR - Logistic Regression</a> <a href='#secb1'> (Lasso)</a> <a href='#secb2'> (Ridge)</a><br>\n",
    "<a href='#secc'>C. NN - K Nearest Neighbors</a><br>\n",
    "<a href='#secd'>D. NN - Naive Bayes</a><br>\n",
    "<a href='#sece'>E. NN - Decision Tree</a><br>\n",
    "<a href='#secf'>F. Ensemble - Random Forest</a><br>\n",
    "\n",
    "<a href='#sec2'>2. Model Comparison</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">1. Models:</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iPython/Jupyter Notebook\n",
    "import time\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "from IPython.display import Image\n",
    "\n",
    "import time\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import plotly as plo\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import difflib\n",
    "\n",
    "# Stats\n",
    "from sklearn.metrics import classification_report, roc_curve,roc_auc_score,accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Preparing Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "# Models\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB,GaussianNB\n",
    "\n",
    "# Ensemble\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Visualization\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "# import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import Features + Target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadf = False\n",
    "if loadf:\n",
    "    features = pd.read_csv('blogfeatures.csv').sample(frac=1.0)\n",
    "    del features['Unnamed: 0']\n",
    "else:\n",
    "    %store -r blog\n",
    "    %store -r keepfeatures\n",
    "    features = blog\n",
    "\n",
    "lang = list(features.language.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del features['id']\n",
    "del features['content']\n",
    "del features['pos']\n",
    "del features['pos2']\n",
    "del features['pos3']\n",
    "del features['tokens']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features = features[features['sc']<15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Japanese               9536\n",
       "Traditional Chinese    3407\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.language.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Set X,Y__ (UNUSED)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y = features['language'].values.reshape(-1, 1).ravel()\n",
    "X = features[features.columns[~features.columns.str.contains('language')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Select Features:__ Features with biggest differences between L1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectfeatures = True\n",
    "if selectfeatures:\n",
    "    nonbinary = ['letters_per','wc','sc','sent_pol','sent_subj','cap_let','punc_count','freq_score','full_freq_score']\n",
    "    best_features = keepfeatures + nonbinary\n",
    "else:\n",
    "    best_features = features.columns[~features.columns.str.contains('language')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Truncated SVD:__ Determine best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdbool = False\n",
    "if svdbool:\n",
    "    svd = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\n",
    "    svd.fit(features[best_features])\n",
    "    # print(svd.explained_variance_ratio_)\n",
    "    # print(svd.explained_variance_ratio_.sum())  \n",
    "    # print(svd.singular_values_)\n",
    "    # svd.get_params\n",
    "    # best_features = [X.columns[i] for i in svd.components_[0].argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split Data to Train/Test__: Even Distribution Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Chinese\n",
      "Japanese\n"
     ]
    }
   ],
   "source": [
    "# evenly_distributed_test = [60 japanese,60 english, 60 chinese, 60 korean]\n",
    "langlist = []\n",
    "rs=21\n",
    "for l in lang:\n",
    "    print(l)\n",
    "    langlist.append(features[features['language'] == l].sample(n=200, random_state=rs))\n",
    "\n",
    "testset = langlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.concat(testset)\n",
    "y_test = test_data['language'].values.reshape(-1, 1).ravel()\n",
    "X_test = test_data[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 s, sys: 8.14 s, total: 29 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = features[~features.isin(test_data)].dropna()\n",
    "y_train = train_data['language'].values.reshape(-1, 1).ravel()\n",
    "X_train = train_data[best_features]\n",
    "# X_train = svd.transform(train_data[train_data.columns[~train_data.columns.str.contains('language')]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Function for Comparing Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['name','time','total','precision','recall','f1']\n",
    "\n",
    "model_set = pd.DataFrame(columns=cols)\n",
    "models_stored = []\n",
    "pattern = \"%.2f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,name):\n",
    "    global model_set\n",
    "    m = model\n",
    "    m.fit(X_train, y_train)\n",
    "    start = time.time()\n",
    "\n",
    "    total_score = m.score(X_test,y_test)\n",
    "    pscore = [pattern % i for i in list(metrics.precision_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    rscore = [pattern % i for i in list(metrics.recall_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    fscore = [pattern % i for i in list(metrics.f1_score(y_test, m.predict(X_test),labels=lang,average=None))]\n",
    "    end = time.time()\n",
    "    t= pattern % (end - start)\n",
    "\n",
    "    r = dict(zip(cols,[name,t,total_score,pscore,rscore,fscore]))\n",
    "    print('Check for Overfitting: {}\\n'.format(m.score(X_train,y_train)))\n",
    "    print('Test Score is: {}\\n'.format(total_score))\n",
    "    print(classification_report(y_test, m.predict(X_test)))\n",
    "    \n",
    "    model_set = model_set.append(r,ignore_index=True)\n",
    "    return r,m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">A. LR - Logistic Regression</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Target is binary so logistic regression will operate on probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.7455951526747987\n",
      "\n",
      "Test Score is: 0.51\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Japanese       0.51      1.00      0.67       200\n",
      "Traditional Chinese       1.00      0.02      0.04       200\n",
      "\n",
      "        avg / total       0.75      0.51      0.36       400\n",
      "\n",
      "CPU times: user 324 ms, sys: 41.4 ms, total: 365 ms\n",
      "Wall time: 348 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lreg_data,lreg = run_model(linear_model.LogisticRegression(),'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sece\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">E. K Nearest Neighbors</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can handle discrete values for target <br>Quantitative values are limited (not continuous) and might be problematic for nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.753647452762497\n",
      "\n",
      "Test Score is: 0.5025\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Japanese       0.50      0.97      0.66       200\n",
      "Traditional Chinese       0.55      0.03      0.06       200\n",
      "\n",
      "        avg / total       0.52      0.50      0.36       400\n",
      "\n",
      "CPU times: user 2.26 s, sys: 51.1 ms, total: 2.32 s\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "neighbors_data,neighbors = run_model(KNeighborsClassifier(n_neighbors=8),'K Nearest Neighbor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">F. Naive Bayes - Bernoulli</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.7950251136091844\n",
      "\n",
      "Test Score is: 0.71\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Japanese       0.66      0.87      0.75       200\n",
      "Traditional Chinese       0.81      0.55      0.65       200\n",
      "\n",
      "        avg / total       0.73      0.71      0.70       400\n",
      "\n",
      "CPU times: user 235 ms, sys: 63.5 ms, total: 299 ms\n",
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bnb_data,bnb = run_model(BernoulliNB(),'Naive Bayes - Bernoulli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letters_per -0.00031167212345195594\n",
      "wc -0.00031167212345195594\n",
      "sc -0.00031167212345195594\n",
      "freq_score -0.00031167212345195594\n",
      "full_freq_score -0.00031167212345195594\n",
      "\n",
      "Stored 'bnb_sorted' (list)\n"
     ]
    }
   ],
   "source": [
    "importance = dict(list(zip(X_train.columns,bnb.coef_[0])))\n",
    "bnb_sorted = sorted(importance, key=importance.get, reverse=True)\n",
    "for r in bnb_sorted[0:5]:\n",
    "    print(r, importance[r])\n",
    "print('')\n",
    "%store bnb_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"secg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">G. Decision Tree</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.7766084668739536\n",
      "\n",
      "Test Score is: 0.565\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Japanese       0.54      0.99      0.69       200\n",
      "Traditional Chinese       0.93      0.14      0.24       200\n",
      "\n",
      "        avg / total       0.73      0.56      0.47       400\n",
      "\n",
      "CPU times: user 199 ms, sys: 16.7 ms, total: 216 ms\n",
      "Wall time: 215 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dt_data,dt = run_model(tree.DecisionTreeClassifier(criterion='entropy',max_depth=4),'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Render tree.\n",
    "dot_data = tree.export_graphviz(\n",
    "    dt, \n",
    "    out_file=None,\n",
    "    feature_names=X_train.columns,\n",
    "    label= 'root',\n",
    "    proportion=False,\n",
    "    rounded=True,\n",
    "    class_names=lang,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n",
    "\n",
    "graph.write_png('decision_tree.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dimportance = list(zip(best_features,dt.feature_importances_))\n",
    "dimportance = dict(dimportance)\n",
    "a1_sorted_keys = sorted(dimportance, key=dimportance.get, reverse=True)\n",
    "p = []\n",
    "for r in a1_sorted_keys:\n",
    "    if dimportance[r] != 0:\n",
    "        p.append(r)\n",
    "#         print(r, dimportance[r])\n",
    "        \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Good visualization of important features and presentation of entropy weighting_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sech\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">H. Random Forest</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Runs decision tree multiple times for best output <br>Longest processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Overfitting: 0.815833532647692\n",
      "\n",
      "Test Score is: 0.645\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Japanese       0.59      0.98      0.74       200\n",
      "Traditional Chinese       0.95      0.30      0.46       200\n",
      "\n",
      "        avg / total       0.77      0.65      0.60       400\n",
      "\n",
      "CPU times: user 24.4 s, sys: 85.8 ms, total: 24.5 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_data,rf = run_model(ensemble.RandomForestClassifier(n_estimators=150,\n",
    "                                                       criterion='entropy',\n",
    "                                                       max_features=len(X_train.columns),\n",
    "                                                       max_depth=6),'Random Forest')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cvs = cross_val_score(rf, X_test, y_test, cv=5)\n",
    "print(cvs.sum()/len(cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let2_ja 0.25818371468532586\n",
      "sc 0.1766203065751496\n",
      "wc 0.11116070145076383\n",
      "cap_let 0.05450068918071202\n",
      "adv_just 0.05124158545354391\n",
      "\n",
      "Stored 'rf_sorted' (list)\n"
     ]
    }
   ],
   "source": [
    "rf.feature_importances_\n",
    "importance = dict(list(zip(X_train.columns,rf.feature_importances_)))\n",
    "rf_sorted = sorted(importance, key=importance.get, reverse=True)\n",
    "for r in rf_sorted[0:5]:\n",
    "    if importance[r] >0:\n",
    "        print(r, importance[r])\n",
    "print('')\n",
    "%store rf_sorted"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i_tree = 0\n",
    "for tree_in_forest in rf.estimators_:\n",
    "    dot_data = tree.export_graphviz(tree_in_forest, out_file=None,feature_names=X_train.columns,label= 'root',\n",
    "                                    proportion=False,rounded=True,class_names=lang,filled=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    Image(graph.create_png())\n",
    "    graph.write_png('decision_tree'+str(i_tree)+'.png')\n",
    "    i_tree = i_tree+1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">2. Model Comparison</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before TruncatedSVD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>total</th>\n",
       "      <th>prec: | JA | CH |</th>\n",
       "      <th>rec: | JA | CH |</th>\n",
       "      <th>f1: | JA | CH |</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5100</td>\n",
       "      <td>[1.00, 0.51]</td>\n",
       "      <td>[0.02, 1.00]</td>\n",
       "      <td>[0.04, 0.67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K Nearest Neighbor</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.5025</td>\n",
       "      <td>[0.55, 0.50]</td>\n",
       "      <td>[0.03, 0.97]</td>\n",
       "      <td>[0.06, 0.66]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes - Bernoulli</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.7100</td>\n",
       "      <td>[0.81, 0.66]</td>\n",
       "      <td>[0.55, 0.87]</td>\n",
       "      <td>[0.65, 0.75]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>[0.93, 0.54]</td>\n",
       "      <td>[0.14, 0.99]</td>\n",
       "      <td>[0.24, 0.69]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6450</td>\n",
       "      <td>[0.95, 0.59]</td>\n",
       "      <td>[0.30, 0.98]</td>\n",
       "      <td>[0.46, 0.74]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name  time   total prec: | JA | CH | rec: | JA | CH |  \\\n",
       "0      Logistic Regression  0.01  0.5100      [1.00, 0.51]     [0.02, 1.00]   \n",
       "1       K Nearest Neighbor  0.21  0.5025      [0.55, 0.50]     [0.03, 0.97]   \n",
       "2  Naive Bayes - Bernoulli  0.01  0.7100      [0.81, 0.66]     [0.55, 0.87]   \n",
       "3            Decision Tree  0.01  0.5650      [0.93, 0.54]     [0.14, 0.99]   \n",
       "4            Random Forest  0.05  0.6450      [0.95, 0.59]     [0.30, 0.98]   \n",
       "\n",
       "  f1: | JA | CH |  \n",
       "0    [0.04, 0.67]  \n",
       "1    [0.06, 0.66]  \n",
       "2    [0.65, 0.75]  \n",
       "3    [0.24, 0.69]  \n",
       "4    [0.46, 0.74]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_set.columns = ['name','time','total','prec: | JA | CH |','rec: | JA | CH |','f1: | JA | CH |']\n",
    "print('Before TruncatedSVD')\n",
    "model_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save = model_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
