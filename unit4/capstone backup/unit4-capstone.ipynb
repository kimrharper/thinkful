{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CSV --> Transcoder\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/\n",
    "\n",
    "https://www.quora.com/What-is-meant-by-feature-maps-in-convolutional-neural-networks\n",
    "\n",
    "http://www.evanmiller.org/bayesian-average-ratings.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/34033785/normalize-values-in-dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ranking Data:\n",
    "data.groupby('user').transform(lambda x: x - x.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mixture Models - Compute Models (n_count)\n",
    "Count Log Likelihood of each\n",
    "\n",
    "Akaike Information Criterion\n",
    "2*n_features - 2*maxlikelihood_est\n",
    "Bayesian Information Criterion\n",
    "ln(n_samples)*n_features-2*maxlikelihood_est\n",
    "\n",
    "Normalize Ratings Data by Rater\n",
    "Compute Weighted Bayesian Average\n",
    "\n",
    "V-measure (determining goodness of clusters) - Similar to precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "import textblob\n",
    "from gensim.models import word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels import robust\n",
    "from string import punctuation\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from string import punctuation as punct\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# for mac only: frog,blow,funk,glass,tink,submarine,purr,sosumi\n",
    "def beep(audio): \n",
    "    os.system('afplay /System/Library/Sounds/' + audio +'.aiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Add Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_csv('../../data/boardgame/boardgame-comments-english.csv').sample(frac=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.columns = 'reviewer_id', 'game_id', 'rating', 'comment'\n",
    "unique_id = review.reviewer_id.unique()\n",
    "sample_id = random.sample(unique_id.tolist(),200)\n",
    "reviewsample = review[review['reviewer_id'].isin(unique_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = review.comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Round Ratings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RATINGS ADJUSTMENT: ceiling >= .5 [or] floor < .5\n",
    "review['rating'] = review.rating.apply(round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Show ReviewerID and GameID Counts__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "review.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gamereview = review.groupby('reviewer_id')\n",
    "# dflist = []\n",
    "# for r in gamereview:\n",
    "#     dflist.append(r[1])  \n",
    "review.game_id.value_counts()[0:3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check for null values\n",
    "review.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __2. Ratings Distribution__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, normalize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ratings = np.array(review['rating']).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y_normalized = normalize(ratings, norm='l2')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. SK Learn: Normalize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.distplot(ratings);\n",
    "plt.show()\n",
    "sns.distplot(Y_normalized);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_A. Bayesian Average_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "global constant normalization\n",
    "bayesian average rating with regression prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "normalize with bayesian average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Min/Max Transform (Sklearn Preprocessing Doesn't Apply)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "c = scaler.fit_transform(np.array(review['rating']).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_C. Normalize by User Ratings (Not Working Yet)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/34033785/normalize-values-in-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_D. Log Probability Distribution [NOT CORRECT]_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(9.2,4))\n",
    "plt.plot([review.rating[review.rating == i].sum()/review.rating.sum() for i in n])\n",
    "plt.title('Probability Rating Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(9.2,4))\n",
    "plt.plot(np.log([review.rating[review.rating == i].sum()/review.rating.sum() for i in n]))\n",
    "plt.title('Log Probability Rating Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del review['game_id']\n",
    "del review['reviewer_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Natural Language Processing Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for finding percentage frequency (capital letters/punctuation)\n",
    "def per_check(string_value, total):\n",
    "    percentage = len(string_value)\n",
    "    if percentage != 0:\n",
    "        percentage = float(total / percentage) * 100\n",
    "    else:\n",
    "        percentage = 0\n",
    "    return percentage\n",
    "\n",
    "def punc_count(string_value):\n",
    "    count = 0\n",
    "    for c in string_value:\n",
    "        if c in punctuation:\n",
    "            count+= 1\n",
    "    return per_check(string_value, count)\n",
    "\n",
    "def caplet_count(string_value):\n",
    "    count = 0\n",
    "    for c in string_value:\n",
    "        if c.isupper():\n",
    "            count+= 1\n",
    "    return per_check(string_value, count)      \n",
    "\n",
    "review['c_len'] = review.comment.apply(len)\n",
    "review['punc_count'] = review.comment.apply(punc_count)\n",
    "review['caplet_count'] = review.comment.apply(caplet_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Spacy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 660 µs, sys: 96 µs, total: 756 µs\n",
      "Wall time: 1.28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs = []\n",
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "deps = []\n",
    "ents = []\n",
    "sentences = []\n",
    "\n",
    "def insert_null(l):\n",
    "    return [(w if w else '0') for w in l]\n",
    "\n",
    "pipeline = nlp.pipe(review['comment'].astype('unicode').values,\n",
    "                    batch_size = 10, \n",
    "                    n_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-df41065b58cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbeep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ping'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-282220484c85>\u001b[0m in \u001b[0;36mbeep\u001b[0;34m(audio)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# for mac only: frog,blow,funk,glass,tink,submarine,purr,sosumi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbeep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'afplay /System/Library/Sounds/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.aiff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Create doc\n",
    "review['doc'] = [doc if doc.is_parsed else None for doc in pipeline]\n",
    "beep('ping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply features\n",
    "review['w_len'] = review.doc.apply(len)\n",
    "review['tokens'] = review.doc.apply(lambda doc: insert_null([tok.text for tok in doc]))\n",
    "review['lemma'] = review.doc.apply(lambda doc: insert_null([tok.lemma_ for tok in doc]))\n",
    "review['pos'] = review.doc.apply(lambda doc: insert_null([tok.pos_ for tok in doc]))\n",
    "review['deps'] = review.doc.apply(lambda doc: insert_null([tok.dep_ for tok in doc]))\n",
    "review['ents'] = review.doc.apply(lambda doc: insert_null([tok.ent_type_ for tok in doc]))\n",
    "beep('ping')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "review.c_len.max()\n",
    "print(review.comment[review.c_len == 7977].iloc[0][0:300],end=''); print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _TextBlob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "blobs = review.comment.apply(lambda val: textblob.TextBlob(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review['sent_pol'] = blobs.apply(lambda val: val.sentiment[0])\n",
    "review['sent_subj'] = blobs.apply(lambda val: val.sentiment[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Pol = Sentiment Polarity (positive or negative word choice)_ <br>\n",
    "_Subj = Sentiment Subjectivity (objective or subjective word choice)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Textblob Features (Not included because of Spacy)\n",
    "review['wc'] = blobs.apply(lambda val: len(val.words))\n",
    "review['sc'] = blobs.apply(lambda val: len(val.sentences))\n",
    "review['tokens'] = blobs.apply(lambda val: [w.lower() for w in val.words])\n",
    "review['pos'] = blobs.apply(lambda val: [v[1] for v in val.tags])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DataFrame With New Spacy and TextBlob Features\n",
    "review.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visuals"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def MEDIAN_reject_outliers(data, m=3):\n",
    "    data = data[abs(data - np.median(data)) < m*robust.mad(data)]\n",
    "    return data[~np.isnan(data)].sort_values()\n",
    "\n",
    "\n",
    "def MEAN_reject_outliers(data, m=3):\n",
    "    data = data[abs(data - np.mean(data)) <= m*np.std(data)]\n",
    "    return data[~np.isnan(data)].sort_values()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Ratings\n",
    "plt.figure(figsize=(9.2,4))\n",
    "plt.hist(review.rating,bins=10)\n",
    "plt.title('Rating Distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "plt.subplot(221)\n",
    "sns.distplot(MEDIAN_reject_outliers(review.c_len))\n",
    "# plt.title('Average Char Length');\n",
    "\n",
    "plt.subplot(222)\n",
    "sns.distplot(MEDIAN_reject_outliers(review.punc_count))\n",
    "plt.xlabel('Rating');\n",
    "plt.ylabel('Punc Percentatage');\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.scatter(review.rating,review.c_len)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Comment Length')\n",
    "xlist = []\n",
    "ylist = []\n",
    "m,b = np.polyfit(review.rating, review.c_len, 1)\n",
    "for i in range(0,11):\n",
    "    ylist.append(i*m + b)\n",
    "    xlist.append(i)\n",
    "plt.plot(xlist,ylist,color='r')\n",
    "# plt.title('Ratings by Review Len')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.scatter(review.rating,review.punc_count);\n",
    "xlist = []\n",
    "ylist = []\n",
    "m,b = np.polyfit(review.rating, review.punc_count, 1)\n",
    "for i in range(0,11):\n",
    "    ylist.append(i*m + b)\n",
    "    xlist.append(i)\n",
    "plt.plot(xlist,ylist,color='r')\n",
    "# plt.title('Ratings by Punctuation Percentage');\n",
    "plt.xlabel('Rating');\n",
    "plt.ylabel('Punct Percentage');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['c_len','caplet_count','punc_count','rating','sent_pol','sent_subj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y = review['rating']\n",
    "X = review[features].drop('rating',axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Cluster Model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tries to find clusters in the data but doesnt predict anything_ (Not currently relevant)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Akaike Information Criterion\n",
    "2*n_features - 2*maxlikelihood_est\n",
    "\n",
    "Bayesian Information Criterion\n",
    "ln(n_samples)*n_features-2*maxlikelihood_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_train_pca_df = pd.DataFrame(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "c_range = range(2,13)\n",
    "predict = []\n",
    "for i in c_range:\n",
    "    # Declare and fit the model.\n",
    "    sc = SpectralClustering(n_clusters=i)\n",
    "    predict.append(sc.fit_predict(X_train_pca_df))\n",
    "\n",
    "    #Predicted clusters.\n",
    "    print('{} completed'.format(i))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Principal Components Analysis PCA (reduces features to 2 for visualization)\n",
    "\n",
    "X_train_pca_df.columns = ['comp_1','comp_2']\n",
    "X_train_pca_df['hue'] = predict[3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot([i for i in range(1,11)],list(X_train_pca_df.hue.value_counts()),color ='b');\n",
    "plt.plot([i for i in range(1,11)],list(review.rating.value_counts()),color ='g');\n",
    "\n",
    "plt.legend(['Predicted Cluster','Actual Ratings']);\n",
    "plt.title('Cluster Prediction of Counts by Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize=(18,10))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "for i in range(0, 9):\n",
    "    X_train_pca_df['hue'] = predict[i]\n",
    "    ax = fig.add_subplot(3, 3, i+1)\n",
    "    ax = sns.lmplot(x='comp_1',y='comp_2', hue='hue',data=X_train_pca_df,fit_reg=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(22,22));\n",
    "sns.lmplot(x='comp_1',y='comp_2', hue='hue',data=X_train_pca_df,fit_reg=False)\n",
    "plt.xlim(-250,1000)\n",
    "plt.ylim(-5,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _V-Measurement Test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=[c_range],y=[v_measure_score(y_train,v) for v in predict])\n",
    "plt.title('Accuracy of Cluster Predictions')\n",
    "plt.xlabel('Cluster Count')\n",
    "plt.show()\n",
    "\n",
    "# for i,v in enumerate(predict):\n",
    "#     print(clusters[i])\n",
    "#     print(v_measure_score(y_train,predict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Likelihood Function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(Y\\mid\\theta) = \\prod_i^n {p({y_i}\\mid\\theta)} $$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3 -- review.ratings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n = [i for i in range(1,11)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tot_li = np.product([review.rating[review.rating == i].sum()/review.rating.sum() for i in n]);tot_li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Maximum Likelihood Function_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Need to know the pdf (probability density function)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parameters which maximize likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikimedia.org/api/rest_v1/media/math/render/svg/9dc95691ee450e85995f5e3263600cb904323ee8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{n}\\sum_i^n \\ln{p({y_i}\\mid\\theta)} $$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "max_li = (np.sum(np.log([review.rating[review.rating == i].sum()/review.rating.sum() for i in n])))/len(n); max_li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Logistic Regression_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Operates on probabilities_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "print('Check for overfitting:')\n",
    "print(lr.score(X_train,y_train)*100)\n",
    "print('')\n",
    "# Print Model Score Estimation on Same Data\n",
    "print('Percentage of ratings guessed correctly:')\n",
    "print(lr.score(X_test,y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Word Embedding - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Comments: 842\n",
      "CPU times: user 2.73 s, sys: 562 ms, total: 3.29 s\n",
      "Wall time: 4.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "review = pd.read_csv('../../data/boardgame/boardgame-comments-english.csv').sample(frac=.001,random_state=42)\n",
    "review.columns = 'reviewer_id', 'game_id', 'rating', 'comment'\n",
    "\n",
    "# RATINGS ADJUSTMENT: ceiling >= .5 [or] floor < .5\n",
    "review['rating'] = review.rating.map(round)\n",
    "\n",
    "print('Total Comments: {}'.format(review.comment.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 627 ms, sys: 19.5 ms, total: 647 ms\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenize = lambda val: [b.lower() for b in textblob.TextBlob(val).words]\n",
    "sentences_blob = review.comment.map(tokenize)\n",
    "review['token'] = sentences_blob\n",
    "beep('ping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 464 ms, sys: 19.7 ms, total: 483 ms\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "load_model = False\n",
    "\n",
    "if load_model:\n",
    "    # # load model\n",
    "    word_vec = word2vec.Word2Vec.load('full_word2vec_blob.bin')\n",
    "    vec_size = word_vec.layer1_size\n",
    "else: \n",
    "    vec_size = 50\n",
    "    word_vec = word2vec.Word2Vec(\n",
    "        sentences_blob,\n",
    "        workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "        min_count=5,  # Minimum word count threshold.\n",
    "        window=6,      # Number of words around target word to consider.\n",
    "        sg=0,          # Use CBOW because our corpus is small.\n",
    "        sample=1e-3 ,  # Penalize frequent words.\n",
    "        size=vec_size,      # Word vector length.\n",
    "        hs=1           # Use hierarchical softmax.\n",
    "    )\n",
    "    \n",
    "    # save model\n",
    "    word_vec.save('full_word2vec_blob.bin')\n",
    "\n",
    "# List of words in model.\n",
    "vocab = word_vec.wv.vocab.keys()\n",
    "beep('ping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 209 ms, sys: 5.93 ms, total: 215 ms\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec_new = np.array([.5 for i in range(0,vec_size)])\n",
    "review['vectors'] = review.token.apply(lambda val: [word_vec[w] if w in vocab else vec_new for w in val])\n",
    "beep('ping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,w2,w3 = 'easy','player','good'\n",
    "print(word_vec.most_similar(positive=[w1, w2], negative=[w3], topn=1))\n",
    "\n",
    "w1 = 'easy'\n",
    "print(word_vec.wv.most_similar(positive=w1,topn=3))\n",
    "\n",
    "w1 = 'hard'\n",
    "print(word_vec.wv.most_similar(positive=w1,topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cosine Similarity Function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cos(\\theta) = \\frac{A \\bullet B} {\\Vert A \\Vert \\Vert B \\Vert} =  \\frac{\\sum_{i=1}^n A_i B_i}{\\sqrt{ \\sum_{i=1}^n A^2} \\sqrt{ \\sum_{i=1}^n B^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Version A. Raw Code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_norm = lambda m: np.sqrt(np.array([a*a for a in m]).sum())\n",
    "def similarity_vec(a,b):\n",
    "    return (np.dot(a,b))/(euclidean_norm(a)*euclidean_norm(b))\n",
    "\n",
    "hard_easy = similarity_vec(word_vec['hard'],word_vec['easy'])\n",
    "hard_cat = similarity_vec(word_vec['hard'],word_vec['cat'])\n",
    "easy_cat = similarity_vec(word_vec['easy'],word_vec['cat'])\n",
    "easy_simple = similarity_vec(word_vec['easy'],word_vec['simple'])\n",
    "\n",
    "print('HARD - EASY: {}'.format(hard_easy))\n",
    "print('HARD - CAT: {}'.format(hard_cat))\n",
    "print('EASY - CAT: {}'.format(easy_cat))\n",
    "print('EASY - SIMPLE: {}'.format(easy_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Version B. SKLearn__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Comprehensive Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Slow Method_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "padding, max_words = [0 for i in range(0,vec_size)], 100\n",
    "review.vectors = list(keras.preprocessing.sequence.pad_sequences(review.vectors, \n",
    "                                                     maxlen=max_words, \n",
    "                                                     padding='post', \n",
    "                                                     dtype = 'float',\n",
    "                                                     truncating='post', \n",
    "                                                     value=padding))\n",
    "beep('ping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fast Method_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pad, max_words = [0 for i in range(0,vec_size)], 100\n",
    "def manual_pad(val):\n",
    "    empty = max_words-len(val)\n",
    "    for i in range(0,empty):\n",
    "        val.append(pad)\n",
    "    \n",
    "    return [i for i in val[0:max_words+1]]\n",
    "\n",
    "review.vectors=review.vectors.map(manual_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create Train/Test Data_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Jon's Code\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# version for separated X and y arrays, X is vectorized text \n",
    "def balance_data_vec(X, y):\n",
    "    minsamples = y.shape[0]\n",
    "    ratings = list(range(1,11))\n",
    "\n",
    "    # get min number of samples per rating level: \n",
    "    #   we'll use this as our sample size for every rating level.\n",
    "    for rating in ratings:\n",
    "        sampsize = (y==rating).sum()\n",
    "        if sampsize < minsamples:\n",
    "            minsamples = sampsize\n",
    "\n",
    "    # take same number of samples for each rating level.\n",
    "    # sample randomly, create a new dataframe with all samples\n",
    "    X_bal = []\n",
    "    y_bal = []\n",
    "    for rating in ratings:\n",
    "        X_r = X['rating'==rating]\n",
    "        y_r = y[y==rating]\n",
    "        idx = list(range(X_r.shape[0]))\n",
    "        np.random.shuffle( idx )\n",
    "        \n",
    "        # create the output arrays\n",
    "        if type(X_bal) == list: \n",
    "            X_bal = X_r[idx[:minsamples],:]\n",
    "            y_bal = np.array([rating]*minsamples)\n",
    "            \n",
    "        # add samples to output arrays\n",
    "        else:\n",
    "            X_bal = vstack((X_bal, X_r[idx[:minsamples],:]))\n",
    "            y_bal = np.append(y_bal, np.array([rating]*minsamples), axis=0 )\n",
    "            \n",
    "    # shuffle all samples\n",
    "    idx = list(range(X_bal.shape[0])) \n",
    "    np.random.shuffle( idx )\n",
    "    X_bal = X_bal[idx,:]\n",
    "    y_bal = y_bal[idx]\n",
    "    \n",
    "    return X_bal, pd.Series(y_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y = review['rating']\n",
    "X = pd.DataFrame([list(i[0]) for i in review.vectors])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.astype(int).ravel(), test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_bal = X\n",
    "X_bal['rating'] = review['rating'].values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "linear scaling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_bal,y_bal = balance_data_vec(X_bal,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _C - Keras Sequential NN_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.ravel()\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(11, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "beep('ping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jon's Code\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion(y, y_pred, title):\n",
    "    # rating levels\n",
    "    ratings = list(range(1,11))\n",
    "\n",
    "    # generate confusion matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    # normalize matrix\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # plot matrix\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.magma)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(ratings))\n",
    "    plt.xticks(tick_marks, ratings, rotation=45)\n",
    "    plt.yticks(tick_marks, ratings)\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual rating')\n",
    "    plt.xlabel('Predicted rating');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import argmax as kargmax\n",
    "y_predict = np.argmax(model.predict(X_test),axis=1)\n",
    "final_score = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion(final_score,y_predict,'Check')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity Visualization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = word_vec[word_vec.wv.vocab]\n",
    "graph_tsne = TSNE(n_components=2)\n",
    "result = graph_tsne.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(word_vec.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "# plt.ylim(-0.006,0.008)\n",
    "# plt.xlim(-.02,.04)\n",
    "plt.show()\n",
    "\n",
    "graph_pca = PCA(n_components=2)\n",
    "result = graph_pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(word_vec.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "# plt.ylim(-0.006,0.008)\n",
    "# plt.xlim(-.02,.04)\n",
    "plt.show()\n",
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Root Mean Squared Error_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ RMSE_{y} =  \\sqrt{\\frac{\\sum_{i=1}^n {(\\hat{y}_{i}- y_{i})}^2}{N}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.sum(np.square(np.subtract(y_predict,y_actual)))/len(y_actual))\n",
    "# print('Root Mean Squared Error: {}'.format(RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Notes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathjacks / Tex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
