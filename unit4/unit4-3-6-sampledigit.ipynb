{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DOESNT WORK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -d DATASET -t TEST [-s SEARCH]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -d/--dataset, -t/--test\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "def load_digits(datasetPath):\n",
    "\t# build the dataset and then split it into data\n",
    "\t# and labels\n",
    "\tX = np.genfromtxt(datasetPath, delimiter = \",\", dtype = \"uint8\")\n",
    "\ty = X[:, 0]\n",
    "\tX = X[:, 1:]\n",
    "\n",
    "\t# return a tuple of the data and targets\n",
    "\treturn (X, y)\n",
    "\n",
    "def scale(X, eps = 0.001):\n",
    "\t# scale the data points s.t the columns of the feature space\n",
    "\t# (i.e the predictors) are within the range [0, 1]\n",
    "\treturn (X - np.min(X, axis = 0)) / (np.max(X, axis = 0) + eps)\n",
    "\n",
    "def nudge(X, y):\n",
    "\t# initialize the translations to shift the image one pixel\n",
    "\t# up, down, left, and right, then initialize the new data\n",
    "\t# matrix and targets\n",
    "\ttranslations = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
    "\tdata = []\n",
    "\ttarget = []\n",
    "\n",
    "\t# loop over each of the digits\n",
    "\tfor (image, label) in zip(X, y):\n",
    "\t\t# reshape the image from a feature vector of 784 raw\n",
    "\t\t# pixel intensities to a 28x28 'image'\n",
    "\t\timage = image.reshape(28, 28)\n",
    "\n",
    "\t\t# loop over the translations\n",
    "\t\tfor (tX, tY) in translations:\n",
    "\t\t\t# translate the image\n",
    "\t\t\tM = np.float32([[1, 0, tX], [0, 1, tY]])\n",
    "\t\t\ttrans = cv2.warpAffine(image, M, (28, 28))\n",
    "\n",
    "\t\t\t# update the list of data and target\n",
    "\t\t\tdata.append(trans.flatten())\n",
    "\t\t\ttarget.append(label)\n",
    "\n",
    "\t# return a tuple of the data matrix and targets\n",
    "\treturn (np.array(data), np.array(target))\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", required = True,\n",
    "\thelp = \"path to the dataset file\")\n",
    "ap.add_argument(\"-t\", \"--test\", required = True, type = float,\n",
    "\thelp = \"size of test split\")\n",
    "ap.add_argument(\"-s\", \"--search\", type = int, default = 0,\n",
    "\thelp = \"whether or not a grid search should be performed\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# load the digits dataset, convert the data points from integers\n",
    "# to floats, and then scale the data s.t. the predictors (columns)\n",
    "# are within the range [0, 1] -- this is a requirement of the\n",
    "# Bernoulli RBM\n",
    "(X, y) = load_digits(args[\"dataset\"])\n",
    "X = X.astype(\"float32\")\n",
    "X = scale(X)\n",
    "\n",
    "# construct the training/testing split\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y,\n",
    "\ttest_size = args[\"test\"], random_state = 42)\n",
    "\n",
    "# check to see if a grid search should be done\n",
    "if args[\"search\"] == 1:\n",
    "\t# perform a grid search on the 'C' parameter of Logistic\n",
    "\t# Regression\n",
    "\tprint(\"SEARCHING LOGISTIC REGRESSION\")\n",
    "\tparams = {\"C\": [1.0, 10.0, 100.0]}\n",
    "\tstart = time.time()\n",
    "\tgs = GridSearchCV(LogisticRegression(), params, n_jobs = -1, verbose = 1)\n",
    "\tgs.fit(trainX, trainY)\n",
    "\n",
    "\t# print diagnostic information to the user and grab the\n",
    "\t# best model\n",
    "\tprint(\"done in %0.3fs\" % (time.time() - start))\n",
    "\tprint(\"best score: %0.3f\" % (gs.best_score_))\n",
    "\tprint(\"LOGISTIC REGRESSION PARAMETERS\")\n",
    "\tbestParams = gs.best_estimator_.get_params()\n",
    "\n",
    "\t# loop over the parameters and print each of them out\n",
    "\t# so they can be manually set\n",
    "\tfor p in sorted(params.keys()):\n",
    "\t\tprint(\"\\t %s: %f\" % (p, bestParams[p]))\n",
    "\n",
    "\t# initialize the RBM + Logistic Regression pipeline\n",
    "\trbm = BernoulliRBM()\n",
    "\tlogistic = LogisticRegression()\n",
    "\tclassifier = Pipeline([(\"rbm\", rbm), (\"logistic\", logistic)])\n",
    "\n",
    "\t# perform a grid search on the learning rate, number of\n",
    "\t# iterations, and number of components on the RBM and\n",
    "\t# C for Logistic Regression\n",
    "\tprint(\"SEARCHING RBM + LOGISTIC REGRESSION\")\n",
    "\tparams = {\n",
    "\t\t\"rbm__learning_rate\": [0.1, 0.01, 0.001],\n",
    "\t\t\"rbm__n_iter\": [20, 40, 80],\n",
    "\t\t\"rbm__n_components\": [50, 100, 200],\n",
    "\t\t\"logistic__C\": [1.0, 10.0, 100.0]}\n",
    "\n",
    "\t# perform a grid search over the parameter\n",
    "\tstart = time.time()\n",
    "\tgs = GridSearchCV(classifier, params, n_jobs = -1, verbose = 1)\n",
    "\tgs.fit(trainX, trainY)\n",
    "\n",
    "\t# print diagnostic information to the user and grab the\n",
    "\t# best model\n",
    "\tprint(\"\\ndone in %0.3fs\" % (time.time() - start))\n",
    "\tprint(\"best score: %0.3f\" % (gs.best_score_))\n",
    "\tprint(\"RBM + LOGISTIC REGRESSION PARAMETERS\")\n",
    "\tbestParams = gs.best_estimator_.get_params()\n",
    "\n",
    "\t# loop over the parameters and print each of them out\n",
    "\t# so they can be manually set\n",
    "\tfor p in sorted(params.keys()):\n",
    "\t\tprint(\"\\t %s: %f\" % (p, bestParams[p]))\n",
    "\n",
    "\t# show a reminder message\n",
    "\tprint(\"\\nIMPORTANT\")\n",
    "\tprint(\"Now that your parameters have been searched, manually set\")\n",
    "\tprint(\"them and re-run this script with --search 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
