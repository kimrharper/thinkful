{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# <span style=\"color:darkred\">Unsupervised NLP Analysis of Novelists</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Ryan Harper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#ov'>Overview</a><br>\n",
    "<a href='#exp'>Experiment</a><br>\n",
    "<a href='#sec1'>1. Cleaning Data</a><br>\n",
    "<a href='#sec2'>2. IDF Vectorization</a><br>\n",
    "<a href='#sec3'>3. SVD Component Analysis</a><br>\n",
    "<a href='#sec4'>4. Cluster Modeling</a><br>\n",
    "<a href='#sec5'>5. Cluster Evaluation</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ov\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Overview</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Source:__\n",
    "> NLTK Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary:__\n",
    "> This project explores the variation in vocabulary and writing structure of three different authors by employing Word2Vec, IDF, and a gaussian mixture cluster model to examine the sentences in three different novels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variables:__\n",
    ">__author:__ _author of the novel_<br>\n",
    "__sentence:__ _sentence from the novel/author_ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Experiment</span> <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothesis:__ \n",
    "> Authors will use different vocabulary and collocations that will help models predict differences between the authors in an unsupervised environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__\n",
    "><li> Proper nouns (names and places) are the strongest indicator of different authors (via their specific novels)<br><br>  <li> Some frequently used 1 and 2 word expressions (i.e.'Oh' and 'was said') are a good indicator of different authors <br><br>\n",
    "  <li> Clustering models on a Truncated SVD IDF vector does not appear to produce meaningful results. Future experiments might need to explore other ways to better vectorize sentences for cluster models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Method:__\n",
    "> Imported and merged the three novels into a data set.<br><br>\n",
    "> Split up the novels by sentences and random sampled the data.<br><br>\n",
    "> Used IDF and SVD to assess important components and then matched them to the original sentences. <br><br>\n",
    "> Used the Gaussian Mixture model (cluster model) to look at the three combined samples and to determine if clusters could be determined from sentences.<br><br>\n",
    "> Used v_measurement and aikaike to assess the effectiveness of the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">1. Cleaning the Data</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from textblob import TextBlob\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    text = str(text)\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub('\\n',\" \",text)\n",
    "    text = re.sub('\\r',\" \",text)\n",
    "    return text\n",
    "\n",
    "def mlist(story):\n",
    "    return [text_cleaner(i) for i in TextBlob(re.sub(r'Chapter \\d+', '', story)).sentences]\n",
    "\n",
    "def mtokens(story):\n",
    "    return [list(i.words.lower()) for i in TextBlob(re.sub(r'Chapter \\d+', '', story)).sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data.\n",
    "chestertonbrown = mlist(gutenberg.raw('chesterton-brown.txt'))\n",
    "edgeworth = mlist(gutenberg.raw('edgeworth-parents.txt'))\n",
    "alice = mlist(gutenberg.raw('carroll-alice.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = chestertonbrown + alice + edgeworth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl(name,data):\n",
    "    return [name for i in range(len(data))]\n",
    "name = nl('chesterton',chestertonbrown)+nl('edgeworth',edgeworth)+nl('carroll',alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add second book to control for book differences (instead of author differences)\n",
    "chestertonball = mlist(gutenberg.raw('chesterton-ball.txt'))\n",
    "sentences+=chestertonball\n",
    "name+=nl('chesterton',chestertonball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = pd.DataFrame([sentences,name]).T.sample(frac=1,random_state=42)\n",
    "stories.columns = ['sentence','author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">2. IDF Vectorization</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 9040\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.3,\n",
    "                             min_df=2,\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #lower case\n",
    "                             use_idf=True,#inverse document frequencies\n",
    "                             norm=u'l1', #longer and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies\n",
    "                            )\n",
    "\n",
    "X_train, y_train = stories.sentence,stories.author\n",
    "\n",
    "#Applying the vectorizer\n",
    "stories_paras_tfidf=vectorizer.fit_transform(X_train)\n",
    "print(\"Number of features: %d\" % stories_paras_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = stories_paras_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Examine Features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: sentence    cried her mother; \"come to me, and tell me  wh...\n",
      "author                                              edgeworth\n",
      "Name: 8214, dtype: object\n",
      "Tf_idf vector: {'said': 0.3388014198339718, 'archer': 0.6611985801660282}\n"
     ]
    }
   ],
   "source": [
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0\n",
    "print('Original sentence:', stories.iloc[0])\n",
    "print('Tf_idf vector:', tfidf_bypara[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">3. SVD Component Analysis</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Features: 1 | # of Features: 501 | # of Features: 1001 | # of Features: 1501 | # of Features: 2001 | # of Features: 2501 | # of Features: 3001 | # of Features: 3501 | # of Features: 4001 | # of Features: 4501 | # of Features: 5001 | # of Features: 5501 | "
     ]
    }
   ],
   "source": [
    "variances = []\n",
    "r = range (1,7001,500)\n",
    "for i in r:\n",
    "    svd= TruncatedSVD(i)\n",
    "    lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "    # Run SVD on the training data, then project the training data.\n",
    "    X_train_lsa = lsa.fit_transform(X_train_tfidf_csr)\n",
    "    variance_explained=svd.explained_variance_ratio_\n",
    "    total_variance = variance_explained.sum()\n",
    "    variances.append(total_variance)\n",
    "    print('# of Features: {}'.format(i),end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(r,variances)\n",
    "plt.title('Truncated SVD: Explained Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd= TruncatedSVD(400)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf_csr)\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at what sorts of paragraphs our solution considers similar\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train.index)\n",
    "paras_by_component['author'] = stories.author\n",
    "paras_by_component['sentence'] = stories.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.width = 100\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    component = i\n",
    "    print('Component {}:'.format(component))\n",
    "    df = pd.DataFrame(data = paras_by_component[[component,'author','sentence']]).sort_values(by=component,ascending=False)[0:40:5]\n",
    "    df = df.reset_index();del df['index'];df.columns=['Similarity','Author','Sentence']\n",
    "    display(df)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Manual Similarity Discovery\n",
    "for j in range(100,110):\n",
    "    for i in range(len(similarity)):\n",
    "    \n",
    "        if similarity[j][i] > .7:\n",
    "            print(stories.sentence.iloc[i])\n",
    "    print('\\n')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = range(0,20)\n",
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[r[0]:r[-1]+1,r[0]:r[-1]+1]\n",
    "#Making a plot\n",
    "plt.figure(figsize=(10,8))\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=r)\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in r:\n",
    "    j = i-r[0]\n",
    "    print(\"{}-{}: {}\".format(i,stories.author.iloc[i],sim_matrix.index[j][0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">4. Cluster Model - Gaussian Mixture</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Because four novels (and three authors) were merged into a single data set, a mixture model is helpful for distinguising between_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gm_range = range(2,8,1)\n",
    "gm_aic = []\n",
    "gm_predict = []\n",
    "gm_score = []\n",
    "\n",
    "gm_x = X_train_lsa\n",
    "gm_y = y_train\n",
    "for i in gm_range:\n",
    "    # Declare and fit the model.\n",
    "    gm = GaussianMixture(n_components=i,n_init=10,init_params='random')\n",
    "    gm.fit(gm_x)\n",
    "    # gm_predict.append(km.predict_fit(X_test_tfidf))\n",
    "    gm_aic.append(gm.aic(gm_x))\n",
    "    gm_predict.append(gm.predict(gm_x))\n",
    "    gm_score.append(gm.score(gm_x,gm_y))\n",
    "    \n",
    "    \n",
    "    #Predicted clusters.\n",
    "    print('{} completed'.format(i),end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(2,8,1),gm_aic)\n",
    "plt.savefig('aic.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cluster Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Test GuassianMixture Model\n",
    "2. Improve Run Time of the Model\n",
    "3. AIC criterion - lowest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _V-Measurement Criterion_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(y_train, gm_predict[1],margins = True)\n",
    "table.index = list(y_train.unique()) + ['total'];table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Word Embedding - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.tokens = stories.sentence.apply(tokenize).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "load_model = False\n",
    " \n",
    "vec_size = 200\n",
    "word_vec = word2vec.Word2Vec(\n",
    "    stories.tokens,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=5,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=vec_size,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "\n",
    "# List of words in model.\n",
    "vocab = word_vec.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,w2,w3 = 'easy','player','good'\n",
    "print(word_vec.most_similar(positive=[w1, w2], negative=[w3], topn=1))\n",
    "\n",
    "w1 = 'easy'\n",
    "print(word_vec.wv.most_similar(positive=w1,topn=3))\n",
    "\n",
    "w1 = 'hard'\n",
    "print(word_vec.wv.most_similar(positive=w1,topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cosine Similarity Function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cos(\\theta) = \\frac{A \\bullet B} {\\Vert A \\Vert \\Vert B \\Vert} =  \\frac{\\sum_{i=1}^n A_i B_i}{\\sqrt{ \\sum_{i=1}^n A^2} \\sqrt{ \\sum_{i=1}^n B^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Version A. Raw Code__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "euclidean_norm = lambda m: np.sqrt(np.array([a*a for a in m]).sum())\n",
    "def similarity_vec(a,b):\n",
    "    return (np.dot(a,b))/(euclidean_norm(a)*euclidean_norm(b))\n",
    "\n",
    "hard_easy = similarity_vec(word_vec['hard'],word_vec['easy'])\n",
    "hard_cat = similarity_vec(word_vec['hard'],word_vec['cat'])\n",
    "easy_cat = similarity_vec(word_vec['easy'],word_vec['cat'])\n",
    "easy_simple = similarity_vec(word_vec['easy'],word_vec['simple'])\n",
    "\n",
    "print('HARD - EASY: {}'.format(hard_easy))\n",
    "print('HARD - CAT: {}'.format(hard_cat))\n",
    "print('EASY - CAT: {}'.format(easy_cat))\n",
    "print('EASY - SIMPLE: {}'.format(easy_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Version B. SKLearn__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity Visualization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = word_vec[word_vec.wv.vocab]\n",
    "graph_tsne = TSNE(n_components=2)\n",
    "result = graph_tsne.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(word_vec.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "# plt.ylim(-0.006,0.008)\n",
    "# plt.xlim(-.02,.04)\n",
    "plt.show()\n",
    "\n",
    "graph_pca = PCA(n_components=2)\n",
    "result = graph_pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(word_vec.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "# plt.ylim(-0.006,0.008)\n",
    "# plt.xlim(-.02,.04)\n",
    "plt.show()\n",
    "beep('ping')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
