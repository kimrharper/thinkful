{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# <span style=\"color:darkred\">Unsupervised NLP Analysis of Novelists</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Ryan Harper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#ov'>Overview</a><br>\n",
    "<a href='#exp'>Experiment</a><br>\n",
    "<a href='#sec1'>1. Cleaning Data</a><br>\n",
    "<a href='#sec2'>2. Word2Vec Embedding</a><br>\n",
    "<a href='#sec2'>3. IDF Vectorization</a><br>\n",
    "<a href='#sec3'>4. SVD Component Analysis</a><br>\n",
    "<a href='#sec4'>5. Cluster Modeling</a><br>\n",
    "<a href='#sec5'>6. Cluster Evaluation</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ov\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Overview</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Source:__\n",
    "> NLTK Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary:__\n",
    "> This project explores the variation in vocabulary and writing structure of three different authors by employing Word2Vec, IDF, and a gaussian mixture cluster model to examine the sentences in three different novels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variables:__\n",
    ">__author:__ _author of the novel_<br>\n",
    "__sentence:__ _sentence from the novel/author_ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">Experiment</span> <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothesis:__ \n",
    "> Authors will use different vocabulary and collocations that will help models predict differences between the authors in an unsupervised environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__\n",
    "><li> Proper nouns (names and places) are the strongest indicator of different authors (via their specific novels)<br><br>  <li> Some frequently used 1 and 2 word expressions (i.e.'Oh' and 'was said') are a good indicator of different authors <br><br>\n",
    "  <li> Clustering models on a Truncated SVD IDF vector does not appear to produce meaningful results. Future experiments might need to explore other ways to better vectorize sentences for cluster models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Method:__\n",
    "> Imported and merged the three novels into a data set - added a fourth novel from the first author to control for differences in novels<br><br>\n",
    "> Split up the novels by sentences and random sampled the data.<br><br>\n",
    "> Used IDF and SVD to assess important components and then matched them to the original sentences. <br><br>\n",
    "> Used the Gaussian Mixture model (cluster model) to look at the three combined samples and to determine if clusters could be determined from sentences.<br><br>\n",
    "> Used v_measurement and aikaike to assess the effectiveness of the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">1. Cleaning the Data</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np;import pandas as pd;import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import re\n",
    "import sklearn;from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import gutenberg\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore\n",
    "gr = Fore.GREEN\n",
    "re = Fore.RED\n",
    "bl = Fore.RESET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    text = str(text)\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub('\\n',\" \",text)\n",
    "    text = re.sub('\\r',\" \",text)\n",
    "    return text\n",
    "\n",
    "def mlist(story):\n",
    "    return [text_cleaner(i) for i in TextBlob(re.sub(r'Chapter \\d+', '', story)).sentences]\n",
    "\n",
    "def mtokens(story):\n",
    "    return [list(i.words.lower()) for i in TextBlob(re.sub(r'Chapter \\d+', '', story)).sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data.\n",
    "chestertonbrown = mlist(gutenberg.raw('chesterton-brown.txt'))\n",
    "edgeworth = mlist(gutenberg.raw('edgeworth-parents.txt'))\n",
    "alice = mlist(gutenberg.raw('carroll-alice.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = chestertonbrown + alice + edgeworth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl(name,data):\n",
    "    return [name for i in range(len(data))]\n",
    "name = nl('chesterton',chestertonbrown)+nl('edgeworth',edgeworth)+nl('carroll',alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add second book to control for book differences (instead of author differences)\n",
    "chestertonball = mlist(gutenberg.raw('chesterton-ball.txt'))\n",
    "sentences+=chestertonball\n",
    "name+=nl('chesterton',chestertonball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = pd.DataFrame([sentences,name]).T.sample(frac=1,random_state=42)\n",
    "stories.columns = ['sentence','author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embedding - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.tokens = stories.sentence.apply(tokenize).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.91 s, sys: 43.9 ms, total: 6.95 s\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "load_model = False\n",
    " \n",
    "vec_size = 200\n",
    "word_vec = word2vec.Word2Vec(\n",
    "    stories.tokens,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=5,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=vec_size,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "\n",
    "# List of words in model.\n",
    "vocab = word_vec.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cosine Similarity Function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cos(\\theta) = \\frac{A \\bullet B} {\\Vert A \\Vert \\Vert B \\Vert} =  \\frac{\\sum_{i=1}^n A_i B_i}{\\sqrt{ \\sum_{i=1}^n A^2} \\sqrt{ \\sum_{i=1}^n B^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Version A. Cosine Similarity Function on Word2Vec Matrix (with numpy)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-480ec64d0dd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbore_hard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bore'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hard'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'ladyship - Miss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mladyship_miss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'ladyship - statue: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mladyship_statue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'statue - Miss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatue_miss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
     ]
    }
   ],
   "source": [
    "euclidean_norm = lambda m: np.sqrt(np.array([a*a for a in m]).sum())\n",
    "def similarity_vec(a,b):\n",
    "    return (np.dot(a,b))/(euclidean_norm(a)*euclidean_norm(b))\n",
    "\n",
    "ladyship_miss = similarity_vec(word_vec['ladyship'],word_vec['Miss'])\n",
    "ladyship_statue = similarity_vec(word_vec['ladyship'],word_vec['statue'])\n",
    "statue_miss = similarity_vec(word_vec['statue'],word_vec['Miss'])\n",
    "bore_hard = similarity_vec(word_vec['bore'],word_vec['hard'])\n",
    "\n",
    "print(gr+'ladyship - Miss: {}'.format(ladyship_miss)+bl)\n",
    "print(re+'ladyship - statue: {}'.format(ladyship_statue)+bl)\n",
    "print(re+'statue - Miss: {}'.format(statue_miss)+bl)\n",
    "print(gr+'bore - hard: {}'.format(bore_hard)+bl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Version B. Gensim__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cousin', 0.7558534145355225)]\n",
      "[('unmistakable', 0.8760355114936829), ('detective', 0.8586433529853821), ('apple', 0.8544533848762512)]\n",
      "[('useful', 0.7820581197738647), ('emphatic', 0.7500126361846924), ('bore', 0.7369396686553955)]\n"
     ]
    }
   ],
   "source": [
    "w1,w2,w3 = 'Miss','ladyship','men'\n",
    "print(word_vec.wv.most_similar(positive=[w1, w2], negative=[w3], topn=1))\n",
    "\n",
    "w1 = 'statue'\n",
    "print(word_vec.wv.most_similar(positive=w1,topn=3))\n",
    "\n",
    "w1 = 'hard'\n",
    "print(word_vec.wv.most_similar(positive=w1,topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Version C. SKlearn__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1., -1., ..., -1.,  1., -1.],\n",
       "       [ 1., -1., -1., ..., -1.,  1., -1.],\n",
       "       [-1.,  1.,  1., ...,  1., -1.,  1.],\n",
       "       ...,\n",
       "       [ 1., -1., -1., ..., -1.,  1., -1.],\n",
       "       [ 1., -1., -1., ..., -1.,  1., -1.],\n",
       "       [-1.,  1.,  1., ...,  1., -1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(word_vec['ladyship'].reshape(-1,1),word_vec['Miss'].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Returns a matrix for first part of cosine similarity equation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_words,colors = ['statue','Miss','ladyship'],['red','green','green']\n",
    "words = list(word_vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = word_vec[word_vec.wv.vocab]\n",
    "graph_tsne = TSNE(n_components=2)\n",
    "result = graph_tsne.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.scatter(result[:, 0], result[:, 1],color='gray')\n",
    "for i, word in enumerate(color_words):\n",
    "    if word in color_words:\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]),size=18,color=colors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">3. IDF Vectorization</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 9040\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.3,\n",
    "                             min_df=2,\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #lower case\n",
    "                             use_idf=True,#inverse document frequencies\n",
    "                             norm=u'l1', #longer and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies\n",
    "                            )\n",
    "\n",
    "X_train, y_train = stories.sentence,stories.author\n",
    "\n",
    "#Applying the vectorizer\n",
    "stories_paras_tfidf=vectorizer.fit_transform(X_train)\n",
    "print(\"Number of features: %d\" % stories_paras_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = stories_paras_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Examine Features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: sentence    cried her mother; \"come to me, and tell me  wh...\n",
      "author                                              edgeworth\n",
      "Name: 8214, dtype: object\n",
      "Tf_idf vector: {'said': 0.3388014198339718, 'archer': 0.6611985801660282}\n"
     ]
    }
   ],
   "source": [
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0\n",
    "print('Original sentence:', stories.iloc[0])\n",
    "print('Tf_idf vector:', tfidf_bypara[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">4. SVD Component Analysis</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Features: 1 | # of Features: 501 | # of Features: 1001 | # of Features: 1501 | # of Features: 2001 | # of Features: 2501 | # of Features: 3001 | # of Features: 3501 | # of Features: 4001 | # of Features: 4501 | # of Features: 5001 | # of Features: 5501 | "
     ]
    }
   ],
   "source": [
    "variances = []\n",
    "r = range (1,7001,500)\n",
    "for i in r:\n",
    "    svd= TruncatedSVD(i)\n",
    "    lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "    # Run SVD on the training data, then project the training data.\n",
    "    X_train_lsa = lsa.fit_transform(X_train_tfidf_csr)\n",
    "    variance_explained=svd.explained_variance_ratio_\n",
    "    total_variance = variance_explained.sum()\n",
    "    variances.append(total_variance)\n",
    "    print('# of Features: {}'.format(i),end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(r,variances)\n",
    "plt.title('Truncated SVD: Explained Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd= TruncatedSVD(400)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf_csr)\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at what sorts of paragraphs our solution considers similar\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train.index)\n",
    "paras_by_component['author'] = stories.author\n",
    "paras_by_component['sentence'] = stories.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.width = 100\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    component = i\n",
    "    print('Component {}:'.format(component))\n",
    "    df = pd.DataFrame(data = paras_by_component[[component,'author','sentence']]).sort_values(by=component,ascending=False)[0:40:5]\n",
    "    df = df.reset_index();del df['index'];df.columns=['Similarity','Author','Sentence']\n",
    "    display(df)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Manual Similarity Discovery\n",
    "for j in range(100,110):\n",
    "    for i in range(len(similarity)):\n",
    "    \n",
    "        if similarity[j][i] > .7:\n",
    "            print(stories.sentence.iloc[i])\n",
    "    print('\\n')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = range(0,20)\n",
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[r[0]:r[-1]+1,r[0]:r[-1]+1]\n",
    "#Making a plot\n",
    "plt.figure(figsize=(10,8))\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=r)\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in r:\n",
    "    j = i-r[0]\n",
    "    print(\"{}-{}: {}\".format(i,stories.author.iloc[i],sim_matrix.index[j][0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue\">5. Cluster Model - Gaussian Mixture</span>  <a href='#top'>(top)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Because four novels (and three authors) were merged into a single data set, a mixture model is helpful for distinguising between_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gm_range = range(2,8,1)\n",
    "gm_aic = []\n",
    "gm_predict = []\n",
    "gm_score = []\n",
    "\n",
    "gm_x = X_train_lsa\n",
    "gm_y = y_train\n",
    "for i in gm_range:\n",
    "    # Declare and fit the model.\n",
    "    gm = GaussianMixture(n_components=i,n_init=10,init_params='random')\n",
    "    gm.fit(gm_x)\n",
    "    # gm_predict.append(km.predict_fit(X_test_tfidf))\n",
    "    gm_aic.append(gm.aic(gm_x))\n",
    "    gm_predict.append(gm.predict(gm_x))\n",
    "    gm_score.append(gm.score(gm_x,gm_y))\n",
    "    \n",
    "    \n",
    "    #Predicted clusters.\n",
    "    print('{} completed'.format(i),end=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _AIC (Aikaike Information Criterion)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(2,8,1),gm_aic)\n",
    "plt.savefig('aic.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _V-Measurement Criterion_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Cross Tab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(y_train, gm_predict[1],margins = True)\n",
    "table.index = list(y_train.unique()) + ['total'];table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
