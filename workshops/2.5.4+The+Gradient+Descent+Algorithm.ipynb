{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "So far when explaining how regression works, we've said that it finds the model of best fit by minimizing the squared distance between each datapoint and the line of fit.  Squaring the distance removes concerns about positive vs negative signs, and has a heavier penalty for larger distances.  \n",
    "\n",
    "The cost function for a linear regression model $y_i = \\alpha + \\beta x_i$ is:\n",
    "\n",
    "$$\\frac1{n}\\sum_{i=1}^n(y_i-(\\alpha + \\beta x_i))^2$$\n",
    "\n",
    "where $\\alpha + \\beta x_i$ is the prediction of the model $\\alpha + \\beta x$ for predictors $x_i$, $y_i$ is the actual outcome value, and $n$ is the number of distances being summed.\n",
    "\n",
    "For many linear regressions, the model is sufficiently simple that the true minimum of the cost function can be calculated by solving a system of equations.  However, many other models that we will encounter from this point forward are _too complex_ to be solved for a true minimum.  For those models it's useful to use an iterative algorithm that starts from a random set of parameters and slowly works toward optimizing the cost function.\n",
    "\n",
    "One such algorithm is **gradient descent**, which iteratively minimizes the cost function using derivatives.  This approach is robust and flexible, and can be applied to basically any differentiable function.\n",
    "\n",
    "Now we're going to get into the nuts-and-bolts of how gradient descent works (and what differentiable functions are). Hold on to your hats, we're gonna do some calculus!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "After learning about PCA, you should be comfortable with the idea of data as a multi-dimensional space.  When optimizing a linear regression, the dimensions of the space correspond to the number of parameters in the equation, plus the error function we are trying to minimize.  So a model $y_i = \\alpha + \\beta x_i$ with two parameters would yield a three-dimensional space.  Within that space is a *surface* made up of all  possible combinations of parameter values, and the error values that result when we plug those parameters into the cost function.  (In a two-dimensional space, we have lines.  In three dimensions and higher, we have surfaces.)\n",
    "\n",
    "The gradient descent algorithm works iteratively by picking a location on the surface defined by a combination of parameter values, calculating the direction from that point with the steepest 'downhill' gradient, and then moving 'downhill' a set distance.  Then the algorithm picks up the new parameter values of that location on the surface, re-calculates the direction of 'downhill' and moves a set distance again.  The algorithm will repeat this until it finds a location on the surface where all possible gradients away from that location are \"uphill\": in other words, where all other possible combinations of parameters result in higher error values.  The parameter values that define the location at the lowest point of the space represent the \"optimized\" solution to the cost function, and are what the regression returns as a solution.\n",
    "\n",
    "The direction of \"downhill\" is determined by differentiating the cost function and taking the partial derivative of each parameter of the regression equation.  A function is \"differentiable\" if a derivative can be calculated at each value of the function.  A derivative, in turn, is a measure of how sensitive a quantity is to change in another quantity.  In other words, if there is a function $f$ that contains parameters $x$ and $y$, the partial derivative for $x$ (expressed as $\\frac{\\partial}{\\partial y}$) will tell us how much $y$ will change for each unit change in $x$.  We could also calculate $\\frac{\\partial}{\\partial x}$, to find out how much a one-unit change in $y$ will impact $x$.\n",
    "\n",
    "For our two-parameter regression line model, the derivatives are:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\alpha} =\\frac2n \\sum_{i=1}^n - (y^i-(\\alpha + \\beta x_i) )$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\beta} =\\frac2n \\sum_{i=1}^n - x_i(y^i-(\\alpha + \\beta x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Decision-points in Gradient Descent\n",
    "\n",
    "There are three elements of the gradient descent algorithm that require decisions on the part of the operator.  \n",
    "\n",
    "\n",
    "### What are the starting values of the parameters?   \n",
    "\n",
    "Many implementations will start by setting all parameters to zero.  However, this isn't a requirement of the algorithm, and sometimes other starting points may be desirable.\n",
    "\n",
    "\n",
    "### How far do we \"move downhill\" after each iteration?\n",
    "\n",
    "Also called the \"learning rate.\"  A too-small learning rate means the model will be computationally inefficient and take a long time to converge (stop).  A too-large learning rate can result in overshooting the target minimum, resulting in a model that _never_ converges.  Again, most algorithm implementations have pre-determined criteria for setting the learning rate, but these can also be set manually.\n",
    "\n",
    "\n",
    "### When do we stop?\n",
    "\n",
    "In the description above, it sounds like the model runs until it reaches the \"optimal\" solution.  In reality, this isn't computationally efficient.  As the gradient flattens out and we get closer and closer to the minimum value of the error, each iteration of the algorithm will result in a smaller and smaller change in the error.  This can get really slow.  Typically some \"minimal acceptable change\" is decided on a-priori â€“ once the change in error from iteration n-1 to iteration n is smaller than the threshold, the algorithm stops.  To prevent an algorithm that never stops, there is usually also a maximum number of permitted iterations before the gradient stops, even if it hasn't achieved a change under the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guts of Gradient Descent\n",
    "\n",
    "Let's walk through programming a gradient descent algorithm in Python.  There are packages that will do this for you, but for now we'll try it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## Cost function for the linear regression that we will try to optimize.\n",
    "def LR_cost_function (alpha, beta, x, y):\n",
    "    '''Return the cost for a given line and data.\n",
    "    \n",
    "    Alpha and beta are the coeficients that describe the fit line, while\n",
    "    x and y are lists or arrays with the x and y value of each data point.\n",
    "    '''\n",
    "    \n",
    "    print('alpha: {}, beta: {}'.format(alpha,beta))\n",
    "    error = 0\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        point_error = (y[i] - (alpha + (beta * x[i]))) ** 2\n",
    "        error += point_error\n",
    "    return error / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Function we'll call each iteration (or step) of the gradient algorithm.\n",
    "def step (alpha_cur, beta_cur, learning_rate, x, y):\n",
    "    '''Move downhill from a current cost function to a new, more optimal one.'''\n",
    "    alpha = 0\n",
    "    beta = 0\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        # Partial derivative of the intercept.\n",
    "        point_alpha = -(2/n) * (y[i] - ((alpha_cur + beta_cur * x[i])))\n",
    "        alpha += point_alpha\n",
    "        \n",
    "        # Partial derivative of the slope.\n",
    "        point_beta = -(2 / n) * x[i] * (y[i] - ((alpha_cur + beta_cur * x[i])))\n",
    "        beta += point_beta\n",
    "        \n",
    "    new_alpha = alpha_cur - learning_rate * alpha \n",
    "    new_beta = beta_cur - learning_rate * beta\n",
    "    return [new_alpha, new_beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coefficients from sklearn: \n",
      " [[1.98989382]]\n",
      "\n",
      "Intercept from sklearn: \n",
      " [0.50420215]\n"
     ]
    }
   ],
   "source": [
    "# These constants correspond to the decision-points described above.\n",
    "# How many steps to take.\n",
    "stop = 1000\n",
    "\n",
    "# How far to move with each step.\n",
    "learning_rate = .005\n",
    "\n",
    "# Starting values for intercept and slope \n",
    "alpha_start = 0\n",
    "beta_start = 0\n",
    "\n",
    "# Time to make some data!\n",
    "x = np.random.normal(0, 1, 100)\n",
    "y = x * 2 + np.random.sample(100)\n",
    "\n",
    "# Fit an true minimum regression using solved equations.\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x.reshape(-1, 1), y.reshape(-1, 1))\n",
    "\n",
    "print('\\nCoefficients from sklearn: \\n', regr.coef_)\n",
    "print('\\nIntercept from sklearn: \\n', regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now fit an iteratively optimized regression using your custom gradient\n",
    "# descent algorithm.\n",
    "\n",
    "# Storing each iteration to inspect later.\n",
    "all_error=[]\n",
    "\n",
    "# Provide starting values.\n",
    "alpha = alpha_start\n",
    "beta = beta_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.004006827141661426, beta: 0.018671992020073785\n",
      "alpha: 0.007983299665909369, beta: 0.037168400256284026\n",
      "alpha: 0.01192962977568457, beta: 0.05549087963690073\n",
      "alpha: 0.015846028412834924, beta: 0.07364106945356955\n",
      "alpha: 0.019732705262591833, beta: 0.09162059350944168\n",
      "alpha: 0.023589868758078857, beta: 0.1094310602658968\n",
      "alpha: 0.027417726084851594, beta: 0.127074062987872\n",
      "alpha: 0.031216483185467794, beta: 0.14455117988781038\n",
      "alpha: 0.03498634476408662, beta: 0.16186397426824242\n",
      "alpha: 0.03872751429109614, beta: 0.17901399466301277\n",
      "alpha: 0.04244019400776794, beta: 0.19600277497716592\n",
      "alpha: 0.04612458493093806, beta: 0.2128318346255031\n",
      "alpha: 0.049780886857713105, beta: 0.22950267866982324\n",
      "alpha: 0.05340929837020077, beta: 0.24601679795486053\n",
      "alpha: 0.057010016840263716, beta: 0.26237566924293093\n",
      "alpha: 0.060583238434296006, beta: 0.2785807553472999\n",
      "alpha: 0.06412915811802111, beta: 0.29463350526428383\n",
      "alpha: 0.0676479696613107, beta: 0.3105353543040967\n",
      "alpha: 0.07113986564302328, beta: 0.32628772422045427\n",
      "alpha: 0.0746050374558619, beta: 0.34189202333894786\n",
      "alpha: 0.07804367531125002, beta: 0.3573496466841987\n",
      "alpha: 0.08145596824422482, beta: 0.37266197610580537\n",
      "alpha: 0.0848421041183471, beta: 0.38783038040309487\n",
      "alpha: 0.08820226963062688, beta: 0.40285621544868977\n",
      "alpha: 0.09153665031646417, beta: 0.4177408243109016\n",
      "alpha: 0.09484543055460391, beta: 0.4324855373749627\n",
      "alpha: 0.09812879357210447, beta: 0.4470916724631067\n",
      "alpha: 0.10138692144931886, beta: 0.46156053495350935\n",
      "alpha: 0.10461999512488816, beta: 0.47589341789809975\n",
      "alpha: 0.10782819440074612, beta: 0.4900916021392537\n",
      "alpha: 0.11101169794713454, beta: 0.5041563564253788\n",
      "alpha: 0.11417068330762863, beta: 0.5180889375254027\n",
      "alpha: 0.11730532690417154, beta: 0.5318905903421747\n",
      "alpha: 0.12041580404211777, beta: 0.5455625480247898\n",
      "alpha: 0.12350228891528435, beta: 0.5591060320798484\n",
      "alpha: 0.12656495461100953, beta: 0.5725222524816589\n",
      "alpha: 0.12960397311521812, beta: 0.5858124077813939\n",
      "alpha: 0.13261951531749314, beta: 0.5989776852152113\n",
      "alpha: 0.13561175101615272, beta: 0.612019260811349\n",
      "alpha: 0.13858084892333217, beta: 0.624938299496203\n",
      "alpha: 0.14152697667007036, beta: 0.6377359551993986\n",
      "alpha: 0.14445030081139987, beta: 0.6504133709578652\n",
      "alpha: 0.14735098683144032, beta: 0.6629716790189227\n",
      "alpha: 0.1502291991484946, beta: 0.6754120009423902\n",
      "alpha: 0.153085101120147, beta: 0.6877354477017252\n",
      "alpha: 0.15591885504836314, beta: 0.6999431197842038\n",
      "alpha: 0.158730622184591, beta: 0.7120361072901495\n",
      "alpha: 0.1615205627348625, beta: 0.7240154900312216\n",
      "alpha: 0.16428883586489526, beta: 0.7358823376277694\n",
      "alpha: 0.1670355997051939, beta: 0.7476377096052644\n",
      "alpha: 0.16976101135615063, beta: 0.7592826554898163\n",
      "alpha: 0.1724652268931444, beta: 0.7708182149027837\n",
      "alpha: 0.1751484013716383, beta: 0.7822454176544865\n",
      "alpha: 0.17781068883227488, beta: 0.7935652838370297\n",
      "alpha: 0.18045224230596854, beta: 0.8047788239162467\n",
      "alpha: 0.18307321381899505, beta: 0.8158870388227696\n",
      "alpha: 0.1856737543980775, beta: 0.8268909200422367\n",
      "alpha: 0.18825401407546824, beta: 0.8377914497046437\n",
      "alpha: 0.19081414189402657, beta: 0.848589600672847\n",
      "alpha: 0.19335428591229165, beta: 0.8592863366302286\n",
      "alpha: 0.19587459320955034, beta: 0.869882612167528\n",
      "alpha: 0.1983752098908994, beta: 0.8803793728688523\n",
      "alpha: 0.20085628109230208, beta: 0.8907775553968683\n",
      "alpha: 0.2033179509856381, beta: 0.9010780875771892\n",
      "alpha: 0.20576036278374732, beta: 0.9112818884819587\n",
      "alpha: 0.20818365874546635, beta: 0.9213898685126435\n",
      "alpha: 0.21058798018065789, beta: 0.9314029294820408\n",
      "alpha: 0.21297346745523232, beta: 0.9413219646955077\n",
      "alpha: 0.2153402599961616, beta: 0.9511478590314197\n",
      "alpha: 0.21768849629648457, beta: 0.9608814890208679\n",
      "alpha: 0.22001831392030388, beta: 0.9705237229265986\n",
      "alpha: 0.2223298495077739, beta: 0.9800754208212056\n",
      "alpha: 0.22462323878007956, beta: 0.9895374346645807\n",
      "alpha: 0.22689861654440552, beta: 0.9989106083806296\n",
      "alpha: 0.22915611669889563, beta: 1.008195777933261\n",
      "alpha: 0.23139587223760247, beta: 1.017393771401655\n",
      "alpha: 0.23361801525542625, beta: 1.0265054090548167\n",
      "alpha: 0.23582267695304343, beta: 1.035531503425425\n",
      "alpha: 0.23800998764182424, beta: 1.0444728593829806\n",
      "alpha: 0.24018007674873917, beta: 1.0533302742062591\n",
      "alpha: 0.24233307282125408, beta: 1.062104537655079\n",
      "alpha: 0.24446910353221368, beta: 1.0707964320413892\n",
      "alpha: 0.2465882956847132, beta: 1.0794067322996812\n",
      "alpha: 0.24869077521695787, beta: 1.0879362060567355\n",
      "alpha: 0.2507766672071103, beta: 1.0963856137007058\n",
      "alpha: 0.25284609587812495, beta: 1.1047557084495492\n",
      "alpha: 0.2548991846025704, beta: 1.113047236418807\n",
      "alpha: 0.256936055907438, beta: 1.1212609366887436\n",
      "alpha: 0.2589568314789381, beta: 1.1293975413708486\n",
      "alpha: 0.26096163216728213, beta: 1.1374577756737092\n",
      "alpha: 0.2629505779914517, beta: 1.1454423579682576\n",
      "alpha: 0.26492378814395395, beta: 1.1533519998524009\n",
      "alpha: 0.2668813809955624, beta: 1.1611874062150374\n",
      "alpha: 0.2688234741000445, beta: 1.1689492752994675\n",
      "alpha: 0.2707501841988741, beta: 1.1766382987662027\n",
      "alpha: 0.2726616272259301, beta: 1.1842551617551798\n",
      "alpha: 0.2745579183121803, beta: 1.1918005429473852\n",
      "alpha: 0.27643917179035, beta: 1.1992751146258964\n",
      "alpha: 0.27830550119957653, beta: 1.2066795427363435\n",
      "alpha: 0.2801570192900479, beta: 1.2140144869467986\n",
      "alpha: 0.2819938380276267, beta: 1.2212806007070987\n",
      "alpha: 0.28381606859845837, beta: 1.2284785313076043\n",
      "alpha: 0.28562382141356424, beta: 1.2356089199374036\n",
      "alpha: 0.28741720611341853, beta: 1.242672401741964\n",
      "alpha: 0.2891963315725098, beta: 1.2496696058802368\n",
      "alpha: 0.2909613059038866, beta: 1.2566011555812233\n",
      "alpha: 0.29271223646368677, beta: 1.2634676682000023\n",
      "alpha: 0.294449229855651, beta: 1.2702697552732287\n",
      "alpha: 0.2961723919356196, beta: 1.2770080225741052\n",
      "alpha: 0.29788182781601363, beta: 1.283683070166834\n",
      "alpha: 0.29957764187029867, beta: 1.2902954924605528\n",
      "alpha: 0.3012599377374328, beta: 1.296845878262759\n",
      "alpha: 0.3029288183262974, beta: 1.3033348108322291\n",
      "alpha: 0.3045843858201114, beta: 1.3097628679314357\n",
      "alpha: 0.3062267416808284, beta: 1.3161306218784687\n",
      "alpha: 0.30785598665351743, beta: 1.322438639598465\n",
      "alpha: 0.3094722207707257, beta: 1.3286874826745505\n",
      "alpha: 0.31107554335682525, beta: 1.3348777073982991\n",
      "alpha: 0.3126660530323415, beta: 1.3410098648197162\n",
      "alpha: 0.3142438477182653, beta: 1.3470845007967456\n",
      "alpha: 0.3158090246403469, beta: 1.3531021560443106\n",
      "alpha: 0.3173616803333729, beta: 1.3590633661828884\n",
      "alpha: 0.3189019106454259, beta: 1.3649686617866257\n",
      "alpha: 0.3204298107421259, beta: 1.3708185684309988\n",
      "alpha: 0.3219454751108548, beta: 1.3766136067400216\n",
      "alpha: 0.32344899756496276, beta: 1.3823542924330072\n",
      "alpha: 0.3249404712479569, beta: 1.3880411363708867\n",
      "alpha: 0.3264199886376724, beta: 1.393674644602089\n",
      "alpha: 0.32788764155042566, beta: 1.3992553184079866\n",
      "alpha: 0.3293435211451494, beta: 1.404783654347912\n",
      "alpha: 0.3307877179275101, beta: 1.410260144303746\n",
      "alpha: 0.33222032175400723, beta: 1.4156852755240845\n",
      "alpha: 0.33364142183605455, beta: 1.4210595306679892\n",
      "alpha: 0.3350511067440436, beta: 1.4263833878483205\n",
      "alpha: 0.3364494644113885, beta: 1.4316573206746637\n",
      "alpha: 0.3378365821385534, beta: 1.4368817982958464\n",
      "alpha: 0.339212546597061, beta: 1.4420572854420548\n",
      "alpha: 0.34057744383348365, beta: 1.4471842424665509\n",
      "alpha: 0.34193135927341545, beta: 1.4522631253869942\n",
      "alpha: 0.34327437772542696, beta: 1.4572943859263738\n",
      "alpha: 0.34460658338500094, beta: 1.4622784715535515\n",
      "alpha: 0.3459280598384502, beta: 1.467215825523422\n",
      "alpha: 0.34723889006681713, beta: 1.4721068869166931\n",
      "alpha: 0.34853915644975486, beta: 1.4769520906792888\n",
      "alpha: 0.34982894076939014, beta: 1.4817518676613801\n",
      "alpha: 0.3511083242141679, beta: 1.4865066446560462\n",
      "alpha: 0.3523773873826773, beta: 1.4912168444375693\n",
      "alpha: 0.35363621028745995, beta: 1.4958828857993682\n",
      "alpha: 0.354884872358799, beta: 1.5005051835915721\n",
      "alpha: 0.35612345244849075, beta: 1.5050841487582383\n",
      "alpha: 0.3573520288335972, beta: 1.5096201883742193\n",
      "alpha: 0.3585706792201806, beta: 1.5141137056816798\n",
      "alpha: 0.35977948074701965, beta: 1.5185651001262683\n",
      "alpha: 0.3609785099893072, beta: 1.5229747673929472\n",
      "alpha: 0.36216784296232973, beta: 1.527343099441483\n",
      "alpha: 0.3633475551251285, beta: 1.5316704845416012\n",
      "alpha: 0.36451772138414235, beta: 1.5359573073078086\n",
      "alpha: 0.3656784160968321, beta: 1.540203948733886\n",
      "alpha: 0.36682971307528683, beta: 1.544410786227055\n",
      "alpha: 0.3679716855898118, beta: 1.5485781936418204\n",
      "alpha: 0.3691044063724979, beta: 1.5527065413134935\n",
      "alpha: 0.3702279476207733, beta: 1.556796196091399\n",
      "alpha: 0.37134238100093625, beta: 1.5608475213717652\n",
      "alpha: 0.3724477776516702, beta: 1.564860877130306\n",
      "alpha: 0.3735442081875405, beta: 1.5688366199544932\n",
      "alpha: 0.3746317427024729, beta: 1.5727751030755226\n",
      "alpha: 0.3757104507732138, beta: 1.5766766763999804\n",
      "alpha: 0.3767804014627727, beta: 1.5805416865412079\n",
      "alpha: 0.3778416633238462, beta: 1.5843704768503697\n",
      "alpha: 0.3788943044022242, beta: 1.5881633874472296\n",
      "alpha: 0.37993839224017767, beta: 1.5919207552506336\n",
      "alpha: 0.3809739938798289, beta: 1.5956429140087058\n",
      "alpha: 0.3820011758665033, beta: 1.5993301943287583\n",
      "alpha: 0.38302000425206356, beta: 1.6029829237069184\n",
      "alpha: 0.3840305445982257, beta: 1.6066014265574748\n",
      "alpha: 0.38503286197985737, beta: 1.6101860242419481\n",
      "alpha: 0.386027020988258, beta: 1.6137370350978844\n",
      "alpha: 0.3870130857344215, beta: 1.6172547744673782\n",
      "alpha: 0.387991119852281, beta: 1.6207395547253245\n",
      "alpha: 0.3889611865019359, beta: 1.6241916853074043\n",
      "alpha: 0.38992334837286086, beta: 1.6276114727378057\n",
      "alpha: 0.3908776676870978, beta: 1.6309992206566821\n",
      "alpha: 0.3918242062024296, beta: 1.6343552298473514\n",
      "alpha: 0.3927630252155366, beta: 1.637679798263238\n",
      "alpha: 0.3936941855651354, beta: 1.6409732210545604\n",
      "alpha: 0.3946177476351004, beta: 1.6442357905947655\n",
      "alpha: 0.39553377135756723, beta: 1.6474677965067135\n",
      "alpha: 0.39644231621601966, beta: 1.650669525688616\n",
      "alpha: 0.39734344124835846, beta: 1.6538412623397276\n",
      "alpha: 0.3982372050499532, beta: 1.656983287985796\n",
      "alpha: 0.3991236657766767, beta: 1.66009588150427\n",
      "alpha: 0.4000028811479224, beta: 1.6631793191492703\n",
      "alpha: 0.40087490844960433, beta: 1.6662338745763245\n",
      "alpha: 0.4017398045371401, beta: 1.6692598188668675\n",
      "alpha: 0.40259762583841663, beta: 1.67225742055251\n",
      "alpha: 0.40344842835673916, beta: 1.6752269456390791\n",
      "alpha: 0.40429226767376303, beta: 1.6781686576304307\n",
      "alpha: 0.4051291989524085, beta: 1.681082817552036\n",
      "alpha: 0.40595927693975886, beta: 1.6839696839743477\n",
      "alpha: 0.40678255596994156, beta: 1.6868295130359423\n",
      "alpha: 0.4075990899669928, beta: 1.6896625584664458\n",
      "alpha: 0.40840893244770504, beta: 1.6924690716092419\n",
      "alpha: 0.4092121365244582, beta: 1.695249301443966\n",
      "alpha: 0.4100087549080341, beta: 1.6980034946087863\n",
      "alpha: 0.4107988399104143, beta: 1.700731895422474\n",
      "alpha: 0.41158244344756145, beta: 1.703434745906266\n",
      "alpha: 0.41235961704218427, beta: 1.7061122858055204\n",
      "alpha: 0.4131304118264862, beta: 1.7087647526111671\n",
      "alpha: 0.41389487854489754, beta: 1.7113923815809564\n",
      "alpha: 0.41465306755679154, beta: 1.7139954057605062\n",
      "alpha: 0.415405028839184, beta: 1.716574056004151\n",
      "alpha: 0.416150811989417, beta: 1.7191285609955926\n",
      "alpha: 0.4168904662278264, beta: 1.7216591472683584\n",
      "alpha: 0.4176240404003932, beta: 1.7241660392260627\n",
      "alpha: 0.41835158298137937, beta: 1.7266494591624797\n",
      "alpha: 0.4190731420759471, beta: 1.7291096272814255\n",
      "alpha: 0.41978876542276267, beta: 1.7315467617164526\n",
      "alpha: 0.42049850039658454, beta: 1.733961078550358\n",
      "alpha: 0.4212023940108353, beta: 1.7363527918345079\n",
      "alpha: 0.4219004929201586, beta: 1.738722113607978\n",
      "alpha: 0.42259284342295955, beta: 1.7410692539165158\n",
      "alpha: 0.4232794914639308, beta: 1.7433944208313212\n",
      "alpha: 0.4239604826365619, beta: 1.745697820467651\n",
      "alpha: 0.424635862185634, beta: 1.7479796570032475\n",
      "alpha: 0.42530567500969907, beta: 1.7502401326965933\n",
      "alpha: 0.4259699656635436, beta: 1.7524794479049925\n",
      "alpha: 0.42662877836063734, beta: 1.754697801102483\n",
      "alpha: 0.4272821569755666, beta: 1.756895388897578\n",
      "alpha: 0.4279301450464527, beta: 1.759072406050841\n",
      "alpha: 0.4285727857773553, beta: 1.7612290454922936\n",
      "alpha: 0.42921012204066056, beta: 1.7633654983386615\n",
      "alpha: 0.4298421963794547, beta: 1.7654819539104538\n",
      "alpha: 0.4304690510098825, beta: 1.7675785997488842\n",
      "alpha: 0.43109072782349106, beta: 1.7696556216326313\n",
      "alpha: 0.43170726838955903, beta: 1.7717132035944405\n",
      "alpha: 0.4323187139574107, beta: 1.7737515279375693\n",
      "alpha: 0.4329251054587161, beta: 1.7757707752520773\n",
      "alpha: 0.43352648350977613, beta: 1.7777711244309622\n",
      "alpha: 0.4341228884137935, beta: 1.7797527526861443\n",
      "alpha: 0.4347143601631291, beta: 1.7817158355642988\n",
      "alpha: 0.43530093844154416, beta: 1.7836605469625397\n",
      "alpha: 0.43588266262642805, beta: 1.7855870591439549\n",
      "alpha: 0.43645957179101197, beta: 1.787495542752995\n",
      "alpha: 0.4370317047065685, beta: 1.7893861668307172\n",
      "alpha: 0.437599099844597, beta: 1.7912590988298842\n",
      "alpha: 0.43816179537899513, beta: 1.7931145046299217\n",
      "alpha: 0.4387198291882164, beta: 1.7949525485517341\n",
      "alpha: 0.43927323885741365, beta: 1.7967733933723804\n",
      "alpha: 0.4398220616805691, beta: 1.7985772003396117\n",
      "alpha: 0.4403663346626104, beta: 1.8003641291862713\n",
      "alpha: 0.4409060945215131, beta: 1.8021343381445598\n",
      "alpha: 0.4414413776903895, beta: 1.8038879839601636\n",
      "alpha: 0.44197222031956385, beta: 1.805625221906252\n",
      "alpha: 0.44249865827863444, beta: 1.8073462057973413\n",
      "alpha: 0.44302072715852175, beta: 1.8090510880030277\n",
      "alpha: 0.4435384622735037, beta: 1.810740019461592\n",
      "alpha: 0.4440518986632374, beta: 1.812413149693474\n",
      "alpha: 0.44456107109476795, beta: 1.8140706268146212\n",
      "alpha: 0.4450660140645235, beta: 1.8157125975497113\n",
      "alpha: 0.44556676180029814, beta: 1.817339207245249\n",
      "alpha: 0.44606334826322086, beta: 1.8189505998825404\n",
      "alpha: 0.44655580714971227, beta: 1.820546918090544\n",
      "alpha: 0.447044171893428, beta: 1.8221283031586017\n",
      "alpha: 0.4475284756671895, beta: 1.823694895049048\n",
      "alpha: 0.4480087513849021, beta: 1.8252468324097009\n",
      "alpha: 0.4484850317034602, beta: 1.8267842525862366\n",
      "alpha: 0.4489573490246401, beta: 1.8283072916344443\n",
      "alpha: 0.44942573549697995, beta: 1.829816084332368\n",
      "alpha: 0.4498902230176477, beta: 1.831310764192333\n",
      "alpha: 0.45035084323429614, beta: 1.832791463472857\n",
      "alpha: 0.4508076275469058, beta: 1.8342583131904533\n",
      "alpha: 0.45126060710961563, beta: 1.8357114431313175\n",
      "alpha: 0.4517098128325413, beta: 1.8371509818629075\n",
      "alpha: 0.45215527538358147, beta: 1.8385770567454132\n",
      "alpha: 0.4525970251902119, beta: 1.8399897939431173\n",
      "alpha: 0.4530350924412675, beta: 1.84138931843565\n",
      "alpha: 0.4534695070887125, beta: 1.8427757540291372\n",
      "alpha: 0.45390029884939853, beta: 1.844149223367243\n",
      "alpha: 0.45432749720681126, beta: 1.8455098479421086\n",
      "alpha: 0.4547511314128048, beta: 1.8468577481051882\n",
      "alpha: 0.4551712304893247, beta: 1.848193043077982\n",
      "alpha: 0.45558782323011904, beta: 1.849515850962669\n",
      "alpha: 0.45600093820243837, beta: 1.8508262887526379\n",
      "alpha: 0.4564106037487237, beta: 1.8521244723429202\n",
      "alpha: 0.45681684798828326, beta: 1.8534105165405248\n",
      "alpha: 0.45721969881895763, beta: 1.8546845350746741\n",
      "alpha: 0.45761918391877404, beta: 1.855946640606944\n",
      "alpha: 0.4580153307475888, beta: 1.8571969447413093\n",
      "alpha: 0.458408166548719, beta: 1.8584355580340925\n",
      "alpha: 0.4587977183505628, beta: 1.8596625900038208\n",
      "alpha: 0.4591840129682088, beta: 1.860878149140988\n",
      "alpha: 0.45956707700503424, beta: 1.862082342917727\n",
      "alpha: 0.45994693685429233, beta: 1.8632752777973876\n",
      "alpha: 0.46032361870068883, beta: 1.8644570592440284\n",
      "alpha: 0.46069714852194754, beta: 1.8656277917318147\n",
      "alpha: 0.46106755209036515, beta: 1.8667875787543309\n",
      "alpha: 0.4614348549743555, beta: 1.8679365228338038\n",
      "alpha: 0.4617990825399829, beta: 1.8690747255302396\n",
      "alpha: 0.46216025995248533, beta: 1.8702022874504738\n",
      "alpha: 0.4625184121777865, beta: 1.8713193082571369\n",
      "alpha: 0.4628735639839981, beta: 1.8724258866775345\n",
      "alpha: 0.4632257399429111, beta: 1.8735221205124448\n",
      "alpha: 0.46357496443147717, beta: 1.8746081066448321\n",
      "alpha: 0.4639212616332794, beta: 1.875683941048479\n",
      "alpha: 0.46426465553999313, beta: 1.8767497187965363\n",
      "alpha: 0.4646051699528364, beta: 1.877805534069994\n",
      "alpha: 0.4649428284840106, beta: 1.8788514801660698\n",
      "alpha: 0.46527765455813064, beta: 1.8798876495065218\n",
      "alpha: 0.46560967141364545, beta: 1.8809141336458792\n",
      "alpha: 0.4659389021042486, beta: 1.8819310232795994\n",
      "alpha: 0.46626536950027875, beta: 1.882938408252144\n",
      "alpha: 0.4665890962901107, beta: 1.8839363775649831\n",
      "alpha: 0.46691010498153623, beta: 1.8849250193845195\n",
      "alpha: 0.4672284179031358, beta: 1.8859044210499427\n",
      "alpha: 0.4675440572056401, beta: 1.8868746690810054\n",
      "alpha: 0.4678570448632823, beta: 1.887835849185729\n",
      "alpha: 0.4681674026751408, beta: 1.8887880462680346\n",
      "alpha: 0.46847515226647246, beta: 1.8897313444353045\n",
      "alpha: 0.4687803150900364, beta: 1.8906658270058696\n",
      "alpha: 0.46908291242740857, beta: 1.8915915765164284\n",
      "alpha: 0.46938296539028673, beta: 1.8925086747293953\n",
      "alpha: 0.46968049492178665, beta: 1.8934172026401797\n",
      "alpha: 0.46997552179772895, beta: 1.8943172404843962\n",
      "alpha: 0.4702680666279166, beta: 1.8952088677450076\n",
      "alpha: 0.4705581498574036, beta: 1.8960921631594003\n",
      "alpha: 0.4708457917677547, beta: 1.8969672047263921\n",
      "alpha: 0.4711310124782961, beta: 1.897834069713176\n",
      "alpha: 0.4714138319473569, beta: 1.8986928346621958\n",
      "alpha: 0.4716942699735025, beta: 1.8995435753979606\n",
      "alpha: 0.4719723461967584, beta: 1.9003863670337908\n",
      "alpha: 0.4722480800998257, beta: 1.9012212839785043\n",
      "alpha: 0.4725214910092881, beta: 1.9020483999430378\n",
      "alpha: 0.4727925980968095, beta: 1.9028677879470053\n",
      "alpha: 0.4730614203803241, beta: 1.9036795203251966\n",
      "alpha: 0.4733279767252171, beta: 1.9044836687340132\n",
      "alpha: 0.4735922858454973, beta: 1.9052803041578439\n",
      "alpha: 0.4738543663049614, beta: 1.9060694969153804\n",
      "alpha: 0.4741142365183498, beta: 1.9068513166658734\n",
      "alpha: 0.47437191475249385, beta: 1.9076258324153301\n",
      "alpha: 0.47462741912745554, beta: 1.9083931125226525\n",
      "alpha: 0.47488076761765813, beta: 1.909153224705719\n",
      "alpha: 0.47513197805300933, beta: 1.9099062360474073\n",
      "alpha: 0.475381068120016, beta: 1.9106522130015624\n",
      "alpha: 0.47562805536289093, beta: 1.9113912213989066\n",
      "alpha: 0.47587295718465167, beta: 1.9121233264528947\n",
      "alpha: 0.47611579084821126, beta: 1.9128485927655146\n",
      "alpha: 0.4763565734774613, beta: 1.9135670843330321\n",
      "alpha: 0.4765953220583469, beta: 1.9142788645516822\n",
      "alpha: 0.4768320534399343, beta: 1.9149839962233068\n",
      "alpha: 0.4770667843354699, beta: 1.91568254156094\n",
      "alpha: 0.4772995313234327, beta: 1.9163745621943384\n",
      "alpha: 0.4775303108485783, beta: 1.9170601191754628\n",
      "alpha: 0.47775913922297536, beta: 1.9177392729839056\n",
      "alpha: 0.4779860326270352, beta: 1.9184120835322678\n",
      "alpha: 0.4782110071105329, beta: 1.9190786101714863\n",
      "alpha: 0.4784340785936217, beta: 1.9197389116961094\n",
      "alpha: 0.4786552628678398, beta: 1.9203930463495247\n",
      "alpha: 0.4788745755971098, beta: 1.9210410718291353\n",
      "alpha: 0.4790920323187306, beta: 1.9216830452914895\n",
      "alpha: 0.4793076484443626, beta: 1.9223190233573608\n",
      "alpha: 0.47952143926100504, beta: 1.922949062116781\n",
      "alpha: 0.4797334199319668, beta: 1.9235732171340252\n",
      "alpha: 0.4799436054978295, beta: 1.9241915434525498\n",
      "alpha: 0.4801520108774041, beta: 1.924804095599885\n",
      "alpha: 0.48035865086868, beta: 1.9254109275924804\n",
      "alpha: 0.48056354014976754, beta: 1.926012092940505\n",
      "alpha: 0.48076669327983323, beta: 1.9266076446526021\n",
      "alpha: 0.4809681247000285, beta: 1.9271976352406006\n",
      "alpha: 0.4811678487344111, beta: 1.9277821167241793\n",
      "alpha: 0.4813658795908604, beta: 1.92836114063549\n",
      "alpha: 0.4815622313619853, beta: 1.9289347580237355\n",
      "alpha: 0.48175691802602555, beta: 1.9295030194597054\n",
      "alpha: 0.48194995344774694, beta: 1.9300659750402687\n",
      "alpha: 0.4821413513793292, beta: 1.9306236743928238\n",
      "alpha: 0.4823311254612477, beta: 1.9311761666797078\n",
      "alpha: 0.48251928922314863, beta: 1.9317235006025628\n",
      "alpha: 0.48270585608471756, beta: 1.9322657244066626\n",
      "alpha: 0.48289083935654165, beta: 1.9328028858851969\n",
      "alpha: 0.4830742522409654, beta: 1.9333350323835174\n",
      "alpha: 0.4832561078329402, beta: 1.9338622108033423\n",
      "alpha: 0.4834364191208674, beta: 1.9343844676069213\n",
      "alpha: 0.48361519898743516, beta: 1.9349018488211622\n",
      "alpha: 0.48379246021044925, beta: 1.9354144000417184\n",
      "alpha: 0.4839682154636573, beta: 1.9359221664370376\n",
      "alpha: 0.4841424773175673, beta: 1.9364251927523726\n",
      "alpha: 0.4843152582402599, beta: 1.9369235233137547\n",
      "alpha: 0.48448657059819433, beta: 1.937417202031929\n",
      "alpha: 0.48465642665700887, beta: 1.9379062724062532\n",
      "alpha: 0.48482483858231484, beta: 1.93839077752856\n",
      "alpha: 0.48499181844048495, beta: 1.938870760086982\n",
      "alpha: 0.4851573781994356, beta: 1.9393462623697417\n",
      "alpha: 0.48532152972940357, beta: 1.939817326268906\n",
      "alpha: 0.4854842848037165, beta: 1.9402839932841043\n",
      "alpha: 0.4856456550995581, beta: 1.9407463045262126\n",
      "alpha: 0.48580565219872696, beta: 1.9412043007210018\n",
      "alpha: 0.48596428758839044, beta: 1.9416580222127535\n",
      "alpha: 0.4861215726618323, beta: 1.9421075089678403\n",
      "alpha: 0.48627751871919506, beta: 1.9425528005782724\n",
      "alpha: 0.4864321369682166, beta: 1.9429939362652118\n",
      "alpha: 0.48658543852496117, beta: 1.943430954882453\n",
      "alpha: 0.4867374344145453, beta: 1.9438638949198705\n",
      "alpha: 0.48688813557185767, beta: 1.9442927945068347\n",
      "alpha: 0.487037552842274, beta: 1.9447176914155944\n",
      "alpha: 0.48718569698236636, beta: 1.9451386230646297\n",
      "alpha: 0.4873325786606071, beta: 1.9455556265219707\n",
      "alpha: 0.48747820845806766, beta: 1.9459687385084872\n",
      "alpha: 0.4876225968691119, beta: 1.9463779954011458\n",
      "alpha: 0.48776575430208435, beta: 1.9467834332362375\n",
      "alpha: 0.48790769107999293, beta: 1.9471850877125745\n",
      "alpha: 0.4880484174411871, beta: 1.9475829941946572\n",
      "alpha: 0.48818794354003026, beta: 1.947977187715811\n",
      "alpha: 0.48832627944756746, beta: 1.9483677029812936\n",
      "alpha: 0.4884634351521879, beta: 1.9487545743713737\n",
      "alpha: 0.48859942056028227, beta: 1.94913783594438\n",
      "alpha: 0.4887342454968956, beta: 1.9495175214397225\n",
      "alpha: 0.48886791970637433, beta: 1.9498936642808835\n",
      "alpha: 0.48900045285300936, beta: 1.950266297578383\n",
      "alpha: 0.48913185452167357, beta: 1.9506354541327149\n",
      "alpha: 0.4892621342184548, beta: 1.9510011664372546\n",
      "alpha: 0.489391301371284, beta: 1.9513634666811412\n",
      "alpha: 0.48951936533055823, beta: 1.9517223867521316\n",
      "alpha: 0.48964633536975954, beta: 1.9520779582394279\n",
      "alpha: 0.4897722206860687, beta: 1.9524302124364774\n",
      "alpha: 0.48989703040097426, beta: 1.9527791803437486\n",
      "alpha: 0.4900207735608772, beta: 1.9531248926714786\n",
      "alpha: 0.4901434591376907, beta: 1.9534673798423952\n",
      "alpha: 0.4902650960294356, beta: 1.953806671994415\n",
      "alpha: 0.49038569306083085, beta: 1.9541427989833144\n",
      "alpha: 0.4905052589838802, beta: 1.9544757903853762\n",
      "alpha: 0.49062380247845355, beta: 1.954805675500011\n",
      "alpha: 0.4907413321528645, beta: 1.9551324833523542\n",
      "alpha: 0.490857856544443, beta: 1.9554562426958393\n",
      "alpha: 0.4909733841201042, beta: 1.955776982014745\n",
      "alpha: 0.4910879232769122, beta: 1.956094729526721\n",
      "alpha: 0.49120148234264005, beta: 1.9564095131852877\n",
      "alpha: 0.49131406957632523, beta: 1.9567213606823135\n",
      "alpha: 0.49142569316882084, beta: 1.9570302994504696\n",
      "alpha: 0.4915363612433427, beta: 1.9573363566656596\n",
      "alpha: 0.49164608185601205, beta: 1.9576395592494285\n",
      "alpha: 0.49175486299639415, beta: 1.9579399338713472\n",
      "alpha: 0.4918627125880328, beta: 1.9582375069513767\n",
      "alpha: 0.4919696384889809, beta: 1.9585323046622074\n",
      "alpha: 0.49207564849232627, beta: 1.958824352931579\n",
      "alpha: 0.4921807503267143, beta: 1.959113677444577\n",
      "alpha: 0.492284951656866, beta: 1.959400303645908\n",
      "alpha: 0.492388260084092, beta: 1.959684256742154\n",
      "alpha: 0.49249068314680305, beta: 1.9599655617040048\n",
      "alpha: 0.4925922283210162, beta: 1.9602442432684704\n",
      "alpha: 0.4926929030208571, beta: 1.9605203259410715\n",
      "alpha: 0.49279271459905866, beta: 1.9607938339980102\n",
      "alpha: 0.49289167034745546, beta: 1.9610647914883206\n",
      "alpha: 0.49298977749747475, beta: 1.9613332222359976\n",
      "alpha: 0.4930870432206233, beta: 1.9615991498421075\n",
      "alpha: 0.4931834746289707, beta: 1.9618625976868782\n",
      "alpha: 0.49327907877562893, beta: 1.9621235889317692\n",
      "alpha: 0.4933738626552279, beta: 1.9623821465215223\n",
      "alpha: 0.4934678332043878, beta: 1.962638293186194\n",
      "alpha: 0.49356099730218733, beta: 1.962892051443168\n",
      "alpha: 0.4936533617706285, beta: 1.9631434435991482\n",
      "alpha: 0.4937449333750979, beta: 1.9633924917521342\n",
      "alpha: 0.4938357188248241, beta: 1.963639217793378\n",
      "alpha: 0.49392572477333185, beta: 1.9638836434093212\n",
      "alpha: 0.49401495781889254, beta: 1.9641257900835156\n",
      "alpha: 0.4941034245049711, beta: 1.9643656790985249\n",
      "alpha: 0.4941911313206695, beta: 1.9646033315378082\n",
      "alpha: 0.49427808470116696, beta: 1.9648387682875863\n",
      "alpha: 0.4943642910281564, beta: 1.9650720100386911\n",
      "alpha: 0.4944497566302776, beta: 1.9653030772883964\n",
      "alpha: 0.49453448778354747, beta: 1.9655319903422317\n",
      "alpha: 0.49461849071178593, beta: 1.96575876931578\n",
      "alpha: 0.49470177158703943, beta: 1.965983434136458\n",
      "alpha: 0.4947843365300007, beta: 1.9662060045452787\n",
      "alpha: 0.4948661916104254, beta: 1.9664265000985999\n",
      "alpha: 0.49494734284754527, beta: 1.9666449401698531\n",
      "alpha: 0.4950277962104784, beta: 1.966861343951259\n",
      "alpha: 0.49510755761863584, beta: 1.967075730455525\n",
      "alpha: 0.49518663294212556, beta: 1.967288118517528\n",
      "alpha: 0.49526502800215283, beta: 1.9674985267959801\n",
      "alpha: 0.4953427485714175, beta: 1.9677069737750805\n",
      "alpha: 0.49541980037450856, beta: 1.9679134777661496\n",
      "alpha: 0.4954961890882949, beta: 1.9681180569092498\n",
      "alpha: 0.4955719203423139, beta: 1.96832072917479\n",
      "alpha: 0.49564699971915605, beta: 1.9685215123651147\n",
      "alpha: 0.4957214327548473, beta: 1.96872042411608\n",
      "alpha: 0.4957952249392279, beta: 1.9689174818986124\n",
      "alpha: 0.4958683817163285, beta: 1.9691127030202544\n",
      "alpha: 0.4959409084847434, beta: 1.969306104626696\n",
      "alpha: 0.4960128105980005, beta: 1.9694977037032897\n",
      "alpha: 0.4960840933649286, beta: 1.9696875170765544\n",
      "alpha: 0.49615476205002185, beta: 1.9698755614156622\n",
      "alpha: 0.49622482187380124, beta: 1.9700618532339131\n",
      "alpha: 0.49629427801317316, beta: 1.970246408890195\n",
      "alpha: 0.49636313560178524, beta: 1.9704292445904306\n",
      "alpha: 0.49643139973037936, beta: 1.9706103763890102\n",
      "alpha: 0.49649907544714195, beta: 1.9707898201902112\n",
      "alpha: 0.4965661677580513, beta: 1.9709675917496046\n",
      "alpha: 0.49663268162722246, beta: 1.9711437066754476\n",
      "alpha: 0.4966986219772491, beta: 1.9713181804300637\n",
      "alpha: 0.49676399368954294, beta: 1.97149102833121\n",
      "alpha: 0.49682880160467013, beta: 1.9716622655534306\n",
      "alpha: 0.4968930505226855, beta: 1.9718319071293993\n",
      "alpha: 0.4969567452034636, beta: 1.9719999679512472\n",
      "alpha: 0.4970198903670276, beta: 1.9721664627718798\n",
      "alpha: 0.49708249069387517, beta: 1.9723314062062807\n",
      "alpha: 0.4971445508253022, beta: 1.9724948127328035\n",
      "alpha: 0.4972060753637236, beta: 1.9726566966944517\n",
      "alpha: 0.4972670688729917, beta: 1.972817072300146\n",
      "alpha: 0.49732753587871226, beta: 1.9729759536259803\n",
      "alpha: 0.4973874808685576, beta: 1.9731333546164658\n",
      "alpha: 0.4974469082925777, beta: 1.9732892890857625\n",
      "alpha: 0.49750582256350845, beta: 1.9734437707189014\n",
      "alpha: 0.4975642280570778, beta: 1.9735968130729924\n",
      "alpha: 0.497622129112309, beta: 1.973748429578423\n",
      "alpha: 0.49767953003182214, beta: 1.9738986335400444\n",
      "alpha: 0.49773643508213244, beta: 1.974047438138348\n",
      "alpha: 0.49779284849394695, beta: 1.974194856430629\n",
      "alpha: 0.4978487744624583, beta: 1.974340901352141\n",
      "alpha: 0.4979042171476364, beta: 1.9744855857172372\n",
      "alpha: 0.49795918067451794, beta: 1.9746289222205045\n",
      "alpha: 0.4980136691334932, beta: 1.974770923437883\n",
      "alpha: 0.498067686580591, beta: 1.9749116018277781\n",
      "alpha: 0.4981212370377609, beta: 1.975050969732161\n",
      "alpha: 0.49817432449315374, beta: 1.9751890393776583\n",
      "alpha: 0.49822695290139934, beta: 1.9753258228766322\n",
      "alpha: 0.4982791261838824, beta: 1.9754613322282502\n",
      "alpha: 0.4983308482290162, beta: 1.9755955793195452\n",
      "alpha: 0.4983821228925136, beta: 1.9757285759264653\n",
      "alpha: 0.49843295399765664, beta: 1.9758603337149134\n",
      "alpha: 0.4984833453355634, beta: 1.9759908642417776\n",
      "alpha: 0.49853330066545287, beta: 1.9761201789559522\n",
      "alpha: 0.4985828237149081, beta: 1.9762482891993476\n",
      "alpha: 0.49863191818013647, beta: 1.9763752062078932\n",
      "alpha: 0.4986805877262286, beta: 1.9765009411125287\n",
      "alpha: 0.4987288359874147, beta: 1.9766255049401869\n",
      "alpha: 0.4987766665673193, beta: 1.9767489086147674\n",
      "alpha: 0.4988240830392135, beta: 1.9768711629581015\n",
      "alpha: 0.49887108894626553, beta: 1.9769922786909069\n",
      "alpha: 0.49891768780178913, beta: 1.9771122664337344\n",
      "alpha: 0.49896388308948997, beta: 1.9772311367079058\n",
      "alpha: 0.49900967826371007, beta: 1.9773488999364426\n",
      "alpha: 0.4990550767496705, beta: 1.9774655664449854\n",
      "alpha: 0.49910008194371164, beta: 1.9775811464627064\n",
      "alpha: 0.49914469721353205, beta: 1.9776956501232115\n",
      "alpha: 0.499188925898425, beta: 1.977809087465435\n",
      "alpha: 0.4992327713095134, beta: 1.9779214684345259\n",
      "alpha: 0.49927623672998256, beta: 1.978032802882725\n",
      "alpha: 0.4993193254153115, beta: 1.978143100570235\n",
      "alpha: 0.49936204059350175, beta: 1.9782523711660815\n",
      "alpha: 0.4994043854653051, beta: 1.9783606242489669\n",
      "alpha: 0.4994463632044487, beta: 1.9784678693081146\n",
      "alpha: 0.4994879769578591, beta: 1.9785741157441072\n",
      "alpha: 0.4995292298458841, beta: 1.9786793728697158\n",
      "alpha: 0.4995701249625126, beta: 1.9787836499107214\n",
      "alpha: 0.4996106653755931, beta: 1.9788869560067295\n",
      "alpha: 0.49965085412705046, beta: 1.978989300211976\n",
      "alpha: 0.4996906942331003, beta: 1.9790906914961262\n",
      "alpha: 0.49973018868446256, beta: 1.9791911387450658\n",
      "alpha: 0.49976934044657245, beta: 1.9792906507616859\n",
      "alpha: 0.49980815245979043, beta: 1.9793892362666579\n",
      "alpha: 0.4998466276396101, beta: 1.9794869038992042\n",
      "alpha: 0.49988476887686434, beta: 1.97958366221786\n",
      "alpha: 0.49992257903793025, beta: 1.9796795197012271\n",
      "alpha: 0.49996006096493184, beta: 1.9797744847487235\n",
      "alpha: 0.4999972174759415, beta: 1.979868565681323\n",
      "alpha: 0.5000340513651796, beta: 1.9799617707422899\n",
      "alpha: 0.5000705654032128, beta: 1.9800541080979046\n",
      "alpha: 0.5001067623371503, beta: 1.9801455858381858\n",
      "alpha: 0.500142644890839, beta: 1.9802362119776027\n",
      "alpha: 0.5001782157650565, beta: 1.9803259944557816\n",
      "alpha: 0.5002134776377031, beta: 1.980414941138207\n",
      "alpha: 0.5002484331639918, beta: 1.9805030598169147\n",
      "alpha: 0.5002830849766373, beta: 1.9805903582111781\n",
      "alpha: 0.5003174356860427, beta: 1.9806768439681903\n",
      "alpha: 0.5003514878804856, beta: 1.9807625246637373\n",
      "alpha: 0.5003852441263016, beta: 1.9808474078028662\n",
      "alpha: 0.5004187069680676, beta: 1.9809315008205473\n",
      "alpha: 0.5004518789287824, beta: 1.9810148110823287\n",
      "alpha: 0.5004847625100467, beta: 1.9810973458849868\n",
      "alpha: 0.500517360192241, beta: 1.9811791124571683\n",
      "alpha: 0.5005496744347027, beta: 1.9812601179600284\n",
      "alpha: 0.5005817076759009, beta: 1.981340369487862\n",
      "alpha: 0.5006134623336111, beta: 1.9814198740687279\n",
      "alpha: 0.500644940805087, beta: 1.9814986386650697\n",
      "alpha: 0.5006761454672318, beta: 1.9815766701743287\n",
      "alpha: 0.5007070786767679, beta: 1.9816539754295521\n",
      "alpha: 0.5007377427704052, beta: 1.9817305611999951\n",
      "alpha: 0.5007681400650081, beta: 1.9818064341917176\n",
      "alpha: 0.500798272857761, beta: 1.981881601048175\n",
      "alpha: 0.5008281434263325, beta: 1.9819560683508037\n",
      "alpha: 0.5008577540290384, beta: 1.9820298426196012\n",
      "alpha: 0.5008871069050033, beta: 1.9821029303137008\n",
      "alpha: 0.5009162042743205, beta: 1.9821753378319402\n",
      "alpha: 0.5009450483382115, beta: 1.9822470715134257\n",
      "alpha: 0.5009736412791832, beta: 1.982318137638091\n",
      "alpha: 0.5010019852611843, beta: 1.98238854242725\n",
      "alpha: 0.5010300824297609, beta: 1.9824582920441451\n",
      "alpha: 0.5010579349122094, beta: 1.9825273925944902\n",
      "alpha: 0.50108554481773, beta: 1.9825958501270087\n",
      "alpha: 0.5011129142375774, beta: 1.9826636706339662\n",
      "alpha: 0.5011400452452112, beta: 1.9827308600516989\n",
      "alpha: 0.5011669398964445, beta: 1.9827974242611353\n",
      "alpha: 0.5011936002295917, beta: 1.9828633690883155\n",
      "alpha: 0.5012200282656151, beta: 1.9829287003049039\n",
      "alpha: 0.5012462260082696, beta: 1.9829934236286972\n",
      "alpha: 0.5012721954442476, beta: 1.983057544724129\n",
      "alpha: 0.5012979385433208, beta: 1.9831210692027677\n",
      "alpha: 0.5013234572584829, beta: 1.9831840026238108\n",
      "alpha: 0.5013487535260895, beta: 1.9832463504945752\n",
      "alpha: 0.5013738292659977, beta: 1.9833081182709817\n",
      "alpha: 0.5013986863817044, beta: 1.9833693113580355\n",
      "alpha: 0.5014233267604833, beta: 1.9834299351103022\n",
      "alpha: 0.501447752273521, beta: 1.983489994832379\n",
      "alpha: 0.5014719647760518, beta: 1.9835494957793625\n",
      "alpha: 0.5014959661074915, beta: 1.9836084431573109\n",
      "alpha: 0.5015197580915699, beta: 1.9836668421237025\n",
      "alpha: 0.5015433425364629, beta: 1.9837246977878895\n",
      "alpha: 0.5015667212349224, beta: 1.9837820152115486\n",
      "alpha: 0.5015898959644061, beta: 1.9838387994091258\n",
      "alpha: 0.5016128684872057, beta: 1.9838950553482781\n",
      "alpha: 0.5016356405505743, beta: 1.9839507879503115\n",
      "alpha: 0.5016582138868527, beta: 1.984006002090613\n",
      "alpha: 0.5016805902135947, beta: 1.9840607025990804\n",
      "alpha: 0.5017027712336909, beta: 1.9841148942605478\n",
      "alpha: 0.5017247586354924, beta: 1.9841685818152062\n",
      "alpha: 0.5017465540929328, beta: 1.9842217699590208\n",
      "alpha: 0.5017681592656491, beta: 1.984274463344145\n",
      "alpha: 0.5017895757991024, beta: 1.9843266665793284\n",
      "alpha: 0.5018108053246967, beta: 1.9843783842303244\n",
      "alpha: 0.5018318494598969, beta: 1.98442962082029\n",
      "alpha: 0.5018527098083466, beta: 1.9844803808301852\n",
      "alpha: 0.5018733879599842, beta: 1.9845306686991668\n",
      "alpha: 0.5018938854911575, beta: 1.9845804888249794\n",
      "alpha: 0.5019142039647394, beta: 1.9846298455643416\n",
      "alpha: 0.50193434493024, beta: 1.9846787432333304\n",
      "alpha: 0.5019543099239199, beta: 1.98472718610776\n",
      "alpha: 0.5019741004689012, beta: 1.9847751784235588\n",
      "alpha: 0.5019937180752789, beta: 1.9848227243771417\n",
      "alpha: 0.5020131642402297, beta: 1.9848698281257793\n",
      "alpha: 0.5020324404481213, beta: 1.9849164937879635\n",
      "alpha: 0.5020515481706206, beta: 1.9849627254437705\n",
      "alpha: 0.5020704888668, beta: 1.9850085271352187\n",
      "alpha: 0.502089263983244, beta: 1.9850539028666252\n",
      "alpha: 0.5021078749541548, beta: 1.9850988566049574\n",
      "alpha: 0.5021263232014557, beta: 1.9851433922801824\n",
      "alpha: 0.5021446101348956, beta: 1.9851875137856125\n",
      "alpha: 0.5021627371521515, beta: 1.9852312249782476\n",
      "alpha: 0.5021807056389297, beta: 1.9852745296791143\n",
      "alpha: 0.5021985169690674, beta: 1.9853174316736026\n",
      "alpha: 0.5022161725046327, beta: 1.985359934711798\n",
      "alpha: 0.502233673596024, beta: 1.9854020425088117\n",
      "alpha: 0.5022510215820684, beta: 1.9854437587451073\n",
      "alpha: 0.5022682177901192, beta: 1.9854850870668244\n",
      "alpha: 0.5022852635361532, beta: 1.9855260310860994\n",
      "alpha: 0.5023021601248665, beta: 1.9855665943813832\n",
      "alpha: 0.5023189088497698, beta: 1.985606780497755\n",
      "alpha: 0.502335510993283, beta: 1.9856465929472356\n",
      "alpha: 0.5023519678268287, beta: 1.9856860352090944\n",
      "alpha: 0.5023682806109254, beta: 1.9857251107301568\n",
      "alpha: 0.5023844505952796, beta: 1.9857638229251062\n",
      "alpha: 0.502400479018877, beta: 1.985802175176785\n",
      "alpha: 0.5024163671100733, beta: 1.9858401708364914\n",
      "alpha: 0.5024321160866843, beta: 1.9858778132242745\n",
      "alpha: 0.5024477271560744, beta: 1.9859151056292255\n",
      "alpha: 0.502463201515246, beta: 1.9859520513097677\n",
      "alpha: 0.5024785403509261, beta: 1.9859886534939424\n",
      "alpha: 0.5024937448396538, beta: 1.9860249153796923\n",
      "alpha: 0.5025088161478667, beta: 1.9860608401351436\n",
      "alpha: 0.502523755431986, beta: 1.9860964308988835\n",
      "alpha: 0.5025385638385015, beta: 1.9861316907802367\n",
      "alpha: 0.5025532425040556, beta: 1.986166622859538\n",
      "alpha: 0.5025677925555269, beta: 1.9862012301884038\n",
      "alpha: 0.5025822151101125, beta: 1.9862355157899996\n",
      "alpha: 0.5025965112754105, beta: 1.9862694826593064\n",
      "alpha: 0.502610682149501, beta: 1.9863031337633832\n",
      "alpha: 0.5026247288210267, beta: 1.9863364720416279\n",
      "alpha: 0.5026386523692733, beta: 1.986369500406036\n",
      "alpha: 0.5026524538642483, beta: 1.9864022217414559\n",
      "alpha: 0.5026661343667599, beta: 1.986434638905843\n",
      "alpha: 0.5026796949284952, beta: 1.98646675473051\n",
      "alpha: 0.5026931365920971, beta: 1.9864985720203765\n",
      "alpha: 0.5027064603912416, beta: 1.986530093554215\n",
      "alpha: 0.5027196673507132, beta: 1.9865613220848948\n",
      "alpha: 0.5027327584864808, beta: 1.9865922603396247\n",
      "alpha: 0.5027457348057723, beta: 1.9866229110201916\n",
      "alpha: 0.5027585973071491, beta: 1.9866532768031984\n",
      "alpha: 0.5027713469805792, beta: 1.986683360340299\n",
      "alpha: 0.5027839848075104, beta: 1.986713164258431\n",
      "alpha: 0.5027965117609426, beta: 1.986742691160047\n",
      "alpha: 0.5028089288054998, beta: 1.9867719436233426\n",
      "alpha: 0.5028212368975006, beta: 1.9868009242024836\n",
      "alpha: 0.5028334369850295, beta: 1.986829635427829\n",
      "alpha: 0.5028455300080064, beta: 1.9868580798061548\n",
      "alpha: 0.5028575168982561, beta: 1.9868862598208732\n",
      "alpha: 0.502869398579577, beta: 1.9869141779322508\n",
      "alpha: 0.5028811759678096, beta: 1.9869418365776248\n",
      "alpha: 0.5028928499709037, beta: 1.986969238171617\n",
      "alpha: 0.502904421488986, beta: 1.9869963851063464\n",
      "alpha: 0.5029158914144262, beta: 1.9870232797516378\n",
      "alpha: 0.5029272606319033, beta: 1.9870499244552318\n",
      "alpha: 0.5029385300184708, beta: 1.987076321542989\n",
      "alpha: 0.5029497004436215, beta: 1.9871024733190958\n",
      "alpha: 0.5029607727693522, beta: 1.987128382066266\n",
      "alpha: 0.5029717478502271, beta: 1.987154050045941\n",
      "alpha: 0.5029826265334414, beta: 1.987179479498489\n",
      "alpha: 0.5029934096588835, beta: 1.987204672643401\n",
      "alpha: 0.5030040980591979, beta: 1.9872296316794862\n",
      "alpha: 0.5030146925598463, beta: 1.9872543587850648\n",
      "alpha: 0.5030251939791692, beta: 1.9872788561181591\n",
      "alpha: 0.5030356031284463, beta: 1.9873031258166831\n",
      "alpha: 0.5030459208119565, beta: 1.98732716999863\n",
      "alpha: 0.503056147827038, beta: 1.9873509907622582\n",
      "alpha: 0.5030662849641471, beta: 1.9873745901862758\n",
      "alpha: 0.5030763330069168, beta: 1.9873979703300226\n",
      "alpha: 0.5030862927322154, beta: 1.9874211332336509\n",
      "alpha: 0.5030961649102036, beta: 1.9874440809183047\n",
      "alpha: 0.503105950304392, beta: 1.9874668153862969\n",
      "alpha: 0.5031156496716978, beta: 1.987489338621285\n",
      "alpha: 0.5031252637625008, beta: 1.987511652588446\n",
      "alpha: 0.5031347933206997, beta: 1.9875337592346474\n",
      "alpha: 0.5031442390837666, beta: 1.987555660488619\n",
      "alpha: 0.5031536017828027, beta: 1.9875773582611225\n",
      "alpha: 0.5031628821425917, beta: 1.9875988544451177\n",
      "alpha: 0.5031720808816548, beta: 1.9876201509159301\n",
      "alpha: 0.503181198712303, beta: 1.9876412495314149\n",
      "alpha: 0.5031902363406912, beta: 1.9876621521321194\n",
      "alpha: 0.5031991944668699, beta: 1.9876828605414456\n",
      "alpha: 0.5032080737848379, beta: 1.9877033765658096\n",
      "alpha: 0.5032168749825936, beta: 1.9877237019947998\n",
      "alpha: 0.5032255987421865, beta: 1.987743838601334\n",
      "alpha: 0.5032342457397679, beta: 1.9877637881418155\n",
      "alpha: 0.5032428166456413, beta: 1.9877835523562866\n",
      "alpha: 0.5032513121243124, beta: 1.9878031329685815\n",
      "alpha: 0.5032597328345385, beta: 1.9878225316864773\n",
      "alpha: 0.503268079429378, beta: 1.9878417502018442\n",
      "alpha: 0.5032763525562387, beta: 1.987860790190793\n",
      "alpha: 0.5032845528569264, beta: 1.9878796533138234\n",
      "alpha: 0.5032926809676925, beta: 1.9878983412159688\n",
      "alpha: 0.5033007375192818, beta: 1.9879168555269402\n",
      "alpha: 0.5033087231369795, beta: 1.9879351978612703\n",
      "alpha: 0.5033166384406577, beta: 1.9879533698184542\n",
      "alpha: 0.503324484044822, beta: 1.98797137298309\n",
      "alpha: 0.5033322605586574, beta: 1.9879892089250173\n",
      "alpha: 0.5033399685860735, beta: 1.9880068791994556\n",
      "alpha: 0.50334760872575, beta: 1.9880243853471404\n",
      "alpha: 0.5033551815711813, beta: 1.9880417288944578\n",
      "alpha: 0.503362687710721, beta: 1.9880589113535794\n",
      "alpha: 0.5033701277276257, beta: 1.9880759342225933\n",
      "alpha: 0.5033775022000988, beta: 1.9880927989856372\n",
      "alpha: 0.5033848117013339, beta: 1.988109507113027\n",
      "alpha: 0.5033920567995573, beta: 1.988126060061387\n",
      "alpha: 0.5033992380580712, beta: 1.9881424592737766\n",
      "alpha: 0.503406356035295, beta: 1.9881587061798176\n",
      "alpha: 0.5034134112848081, beta: 1.9881748021958188\n",
      "alpha: 0.5034204043553905, beta: 1.9881907487249004\n",
      "alpha: 0.5034273357910647, beta: 1.9882065471571175\n",
      "alpha: 0.5034342061311359, beta: 1.9882221988695812\n",
      "alpha: 0.5034410159102327, beta: 1.9882377052265796\n",
      "alpha: 0.5034477656583471, beta: 1.9882530675796977\n",
      "alpha: 0.5034544559008746, beta: 1.9882682872679351\n",
      "alpha: 0.5034610871586529, beta: 1.9882833656178245\n",
      "alpha: 0.5034676599480017, beta: 1.9882983039435467\n",
      "alpha: 0.5034741747807611, beta: 1.9883131035470467\n",
      "alpha: 0.50348063216433, beta: 1.9883277657181475\n",
      "alpha: 0.5034870326017046, beta: 1.988342291734663\n",
      "alpha: 0.5034933765915155, beta: 1.9883566828625099\n",
      "alpha: 0.5034996646280658, beta: 1.988370940355819\n",
      "alpha: 0.5035058972013681, beta: 1.988385065457045\n",
      "alpha: 0.5035120747971809, beta: 1.9883990593970753\n",
      "alpha: 0.5035181978970457, beta: 1.9884129233953374\n",
      "alpha: 0.503524266978323, beta: 1.9884266586599066\n",
      "alpha: 0.5035302825142279, beta: 1.9884402663876115\n",
      "alpha: 0.5035362449738664, beta: 1.9884537477641386\n",
      "alpha: 0.5035421548222698, beta: 1.9884671039641362\n",
      "alpha: 0.5035480125204305, beta: 1.9884803361513177\n",
      "alpha: 0.5035538185253361, beta: 1.9884934454785632\n",
      "alpha: 0.5035595732900043, beta: 1.9885064330880209\n",
      "alpha: 0.5035652772635169, beta: 1.9885193001112065\n",
      "alpha: 0.503570930891053, beta: 1.9885320476691026\n",
      "alpha: 0.5035765346139237, beta: 1.9885446768722577\n",
      "alpha: 0.5035820888696044, beta: 1.988557188820882\n",
      "alpha: 0.5035875940917681, beta: 1.988569584604945\n",
      "alpha: 0.5035930507103182, beta: 1.9885818653042702\n",
      "alpha: 0.5035984591514208, beta: 1.9885940319886304\n",
      "alpha: 0.5036038198375367, beta: 1.9886060857178405\n",
      "alpha: 0.5036091331874537, beta: 1.988618027541851\n",
      "alpha: 0.5036143996163178, beta: 1.9886298585008397\n",
      "alpha: 0.5036196195356646, beta: 1.988641579625303\n",
      "alpha: 0.5036247933534506, beta: 1.9886531919361456\n",
      "alpha: 0.5036299214740836, beta: 1.988664696444771\n",
      "alpha: 0.5036350042984538, beta: 1.9886760941531685\n",
      "alpha: 0.5036400422239634, beta: 1.9886873860540024\n",
      "alpha: 0.5036450356445573, beta: 1.988698573130698\n",
      "alpha: 0.5036499849507524, beta: 1.9887096563575284\n",
      "alpha: 0.5036548905296674, beta: 1.9887206366996988\n",
      "alpha: 0.5036597527650516, beta: 1.9887315151134322\n",
      "alpha: 0.5036645720373146, beta: 1.9887422925460523\n",
      "alpha: 0.5036693487235547, beta: 1.9887529699360669\n",
      "alpha: 0.5036740831975873, beta: 1.9887635482132497\n",
      "alpha: 0.5036787758299734, beta: 1.9887740282987219\n",
      "alpha: 0.5036834269880477, beta: 1.9887844111050326\n",
      "alpha: 0.503688037035946, beta: 1.9887946975362392\n",
      "alpha: 0.5036926063346334, beta: 1.988804888487986\n",
      "alpha: 0.503697135241931, beta: 1.9888149848475833\n",
      "alpha: 0.5037016241125436, beta: 1.988824987494084\n",
      "alpha: 0.5037060732980858, beta: 1.9888348972983612\n",
      "alpha: 0.5037104831471095, beta: 1.9888447151231845\n",
      "alpha: 0.5037148540051298, beta: 1.9888544418232956\n",
      "alpha: 0.5037191862146514, beta: 1.9888640782454825\n",
      "alpha: 0.5037234801151942, beta: 1.9888736252286543\n",
      "alpha: 0.5037277360433196, beta: 1.988883083603914\n",
      "alpha: 0.5037319543326555, beta: 1.988892454194632\n",
      "alpha: 0.5037361353139221, beta: 1.9889017378165172\n",
      "alpha: 0.5037402793149568, beta: 1.9889109352776895\n",
      "alpha: 0.5037443866607386, beta: 1.9889200473787492\n",
      "alpha: 0.5037484576734137, beta: 1.9889290749128483\n",
      "alpha: 0.5037524926723191, beta: 1.988938018665759\n",
      "alpha: 0.5037564919740075, beta: 1.9889468794159433\n",
      "alpha: 0.5037604558922708, beta: 1.9889556579346201\n",
      "alpha: 0.5037643847381643, beta: 1.9889643549858336\n",
      "alpha: 0.5037682788200303, beta: 1.9889729713265196\n",
      "alpha: 0.5037721384435212, beta: 1.988981507706572\n",
      "alpha: 0.5037759639116234, beta: 1.9889899648689082\n",
      "alpha: 0.5037797555246795, beta: 1.9889983435495342\n",
      "alpha: 0.5037835135804117, beta: 1.9890066444776089\n",
      "alpha: 0.5037872383739445, beta: 1.989014868375508\n",
      "alpha: 0.5037909301978266, beta: 1.9890230159588875\n",
      "alpha: 0.5037945893420539, beta: 1.9890310879367457\n",
      "alpha: 0.5037982160940909, beta: 1.9890390850114852\n",
      "alpha: 0.5038018107388931, beta: 1.989047007878975\n",
      "alpha: 0.5038053735589282, beta: 1.989054857228611\n",
      "alpha: 0.5038089048341979, beta: 1.9890626337433763\n",
      "alpha: 0.5038124048422594, beta: 1.9890703380999006\n",
      "alpha: 0.5038158738582462, beta: 1.98907797096852\n",
      "alpha: 0.5038193121548892, beta: 1.9890855330133352\n",
      "alpha: 0.5038227200025372, beta: 1.9890930248922698\n",
      "alpha: 0.5038260976691784, beta: 1.9891004472571276\n",
      "alpha: 0.5038294454204596, beta: 1.9891078007536502\n",
      "alpha: 0.5038327635197074, beta: 1.9891150860215725\n",
      "alpha: 0.5038360522279479, beta: 1.9891223036946792\n",
      "alpha: 0.5038393118039265, beta: 1.9891294544008604\n",
      "alpha: 0.503842542504128, beta: 1.9891365387621662\n",
      "alpha: 0.5038457445827957, beta: 1.9891435573948615\n",
      "alpha: 0.5038489182919513, beta: 1.989150510909479\n",
      "alpha: 0.5038520638814139, beta: 1.9891573999108734\n",
      "alpha: 0.5038551815988188, beta: 1.9891642249982742\n",
      "alpha: 0.5038582716896369, beta: 1.9891709867653375\n",
      "alpha: 0.5038613343971933, beta: 1.989177685800199\n",
      "alpha: 0.5038643699626856, beta: 1.9891843226855241\n",
      "alpha: 0.5038673786252027, beta: 1.98919089799856\n",
      "alpha: 0.5038703606217428, beta: 1.989197412311185\n",
      "alpha: 0.5038733161872317, beta: 1.9892038661899598\n",
      "alpha: 0.5038762455545407, beta: 1.9892102601961759\n",
      "alpha: 0.5038791489545043, beta: 1.9892165948859053\n",
      "alpha: 0.5038820266159378, beta: 1.989222870810049\n",
      "alpha: 0.5038848787656555, beta: 1.989229088514385\n",
      "alpha: 0.5038877056284868, beta: 1.9892352485396163\n",
      "alpha: 0.5038905074272946, beta: 1.9892413514214176\n",
      "alpha: 0.5038932843829915, beta: 1.9892473976904825\n",
      "alpha: 0.5038960367145573, beta: 1.9892533878725704\n",
      "alpha: 0.5038987646390553, beta: 1.9892593224885513\n",
      "alpha: 0.5039014683716493, beta: 1.989265202054452\n",
      "alpha: 0.5039041481256199, beta: 1.9892710270815013\n",
      "alpha: 0.5039068041123808, beta: 1.9892767980761745\n",
      "alpha: 0.5039094365414949, beta: 1.9892825155402376\n",
      "alpha: 0.5039120456206909, beta: 1.989288179970791\n",
      "alpha: 0.5039146315558785, beta: 1.9892937918603137\n",
      "alpha: 0.5039171945511645, beta: 1.9892993516967052\n",
      "alpha: 0.5039197348088685, beta: 1.989304859963329\n",
      "alpha: 0.5039222525295386, beta: 1.9893103171390543\n",
      "alpha: 0.5039247479119662, beta: 1.9893157236982981\n",
      "alpha: 0.5039272211532017, beta: 1.9893210801110672\n",
      "alpha: 0.5039296724485695, beta: 1.989326386842998\n",
      "alpha: 0.503932101991683, beta: 1.989331644355398\n",
      "alpha: 0.5039345099744597, beta: 1.9893368531052866\n",
      "alpha: 0.5039368965871351, beta: 1.9893420135454336\n",
      "alpha: 0.5039392620182785, beta: 1.9893471261243998\n",
      "alpha: 0.5039416064548065, beta: 1.9893521912865761\n",
      "alpha: 0.5039439300819978, beta: 1.989357209472222\n",
      "alpha: 0.5039462330835073, beta: 1.9893621811175044\n",
      "alpha: 0.5039485156413804, beta: 1.9893671066545353\n",
      "alpha: 0.5039507779360668, beta: 1.98937198651141\n",
      "alpha: 0.5039530201464343, beta: 1.9893768211122438\n",
      "alpha: 0.5039552424497827, beta: 1.9893816108772102\n",
      "alpha: 0.5039574450218575, beta: 1.9893863562225764\n",
      "alpha: 0.5039596280368631, beta: 1.98939105756074\n",
      "alpha: 0.5039617916674766, beta: 1.989395715300266\n",
      "alpha: 0.5039639360848607, beta: 1.9894003298459204\n",
      "alpha: 0.5039660614586772, beta: 1.9894049015987079\n",
      "alpha: 0.5039681679570998, beta: 1.989409430955905\n",
      "alpha: 0.5039702557468271, beta: 1.9894139183110962\n",
      "alpha: 0.5039723249930955, beta: 1.9894183640542071\n",
      "alpha: 0.5039743758596921, beta: 1.9894227685715395\n",
      "alpha: 0.5039764085089666, beta: 1.9894271322458046\n",
      "alpha: 0.5039784231018448, beta: 1.989431455456156\n",
      "alpha: 0.5039804197978403, beta: 1.9894357385782238\n",
      "alpha: 0.5039823987550669, beta: 1.9894399819841466\n",
      "alpha: 0.5039843601302509, beta: 1.9894441860426044\n",
      "alpha: 0.5039863040787432, beta: 1.9894483511188508\n",
      "alpha: 0.5039882307545311, beta: 1.9894524775747442\n",
      "alpha: 0.5039901403102505, beta: 1.9894565657687806\n",
      "alpha: 0.5039920328971971, beta: 1.9894606160561237\n",
      "alpha: 0.5039939086653387, beta: 1.9894646287886368\n",
      "alpha: 0.5039957677633262, beta: 1.9894686043149132\n",
      "alpha: 0.5039976103385057, beta: 1.9894725429803064\n",
      "alpha: 0.5039994365369291, beta: 1.9894764451269602\n",
      "alpha: 0.5040012465033661, beta: 1.9894803110938393\n",
      "alpha: 0.5040030403813148, beta: 1.989484141216758\n",
      "alpha: 0.5040048183130132, beta: 1.9894879358284103\n",
      "alpha: 0.50400658043945, beta: 1.9894916952583983\n",
      "alpha: 0.5040083269003752, beta: 1.989495419833261\n",
      "alpha: 0.5040100578343115, beta: 1.9894991098765034\n",
      "alpha: 0.5040117733785646, beta: 1.989502765708624\n",
      "alpha: 0.5040134736692338, beta: 1.9895063876471433\n",
      "alpha: 0.5040151588412225, beta: 1.9895099760066313\n",
      "alpha: 0.5040168290282488, beta: 1.989513531098735\n",
      "alpha: 0.504018484362856, beta: 1.9895170532322057\n",
      "alpha: 0.5040201249764221, beta: 1.9895205427129254\n",
      "alpha: 0.5040217509991709, beta: 1.9895239998439342\n",
      "alpha: 0.5040233625601813, beta: 1.9895274249254566\n",
      "alpha: 0.5040249597873977, beta: 1.9895308182549272\n",
      "alpha: 0.5040265428076398, beta: 1.9895341801270172\n",
      "alpha: 0.5040281117466126, beta: 1.98953751083366\n",
      "alpha: 0.5040296667289155, beta: 1.9895408106640762\n",
      "alpha: 0.5040312078780526, beta: 1.9895440799047996\n",
      "alpha: 0.504032735316442, beta: 1.989547318839702\n",
      "alpha: 0.5040342491654254, beta: 1.989550527750017\n",
      "alpha: 0.5040357495452773, beta: 1.9895537069143658\n",
      "alpha: 0.5040372365752143, beta: 1.9895568566087813\n",
      "alpha: 0.5040387103734045, beta: 1.9895599771067316\n",
      "alpha: 0.5040401710569767, beta: 1.9895630686791441\n",
      "alpha: 0.5040416187420294, beta: 1.9895661315944293\n",
      "alpha: 0.5040430535436393, beta: 1.9895691661185042\n",
      "alpha: 0.5040444755758712, beta: 1.9895721725148152\n",
      "alpha: 0.504045884951786, beta: 1.9895751510443616\n",
      "alpha: 0.5040472817834499, beta: 1.9895781019657175\n",
      "alpha: 0.504048666181943, beta: 1.9895810255350554\n",
      "alpha: 0.5040500382573676, beta: 1.989583922006168\n",
      "alpha: 0.5040513981188572, beta: 1.9895867916304901\n",
      "alpha: 0.5040527458745849, beta: 1.9895896346571211\n",
      "alpha: 0.5040540816317715, beta: 1.9895924513328462\n",
      "alpha: 0.5040554054966938, beta: 1.9895952419021585\n",
      "alpha: 0.5040567175746934, beta: 1.9895980066072796\n",
      "alpha: 0.5040580179701842, beta: 1.9896007456881815\n",
      "alpha: 0.504059306786661, beta: 1.9896034593826066\n",
      "alpha: 0.5040605841267072, beta: 1.9896061479260896\n",
      "alpha: 0.5040618500920029, beta: 1.989608811551977\n",
      "alpha: 0.5040631047833328, beta: 1.9896114504914477\n",
      "alpha: 0.5040643483005941, beta: 1.9896140649735339\n",
      "alpha: 0.504065580742804, beta: 1.9896166552251398\n",
      "alpha: 0.504066802208108, beta: 1.9896192214710626\n",
      "alpha: 0.5040680127937864, beta: 1.9896217639340112\n",
      "alpha: 0.5040692125962632, beta: 1.9896242828346262\n",
      "alpha: 0.5040704017111126, beta: 1.989626778391499\n",
      "alpha: 0.5040715802330669, beta: 1.9896292508211904\n",
      "alpha: 0.5040727482560236, beta: 1.9896317003382502\n",
      "alpha: 0.5040739058730529, beta: 1.9896341271552354\n",
      "alpha: 0.504075053176405, beta: 1.989636531482729\n",
      "alpha: 0.504076190257517, beta: 1.989638913529358\n",
      "alpha: 0.5040773172070203, beta: 1.989641273501812\n",
      "alpha: 0.5040784341147474, beta: 1.9896436116048613\n",
      "alpha: 0.504079541069739, beta: 1.989645928041374\n",
      "alpha: 0.5040806381602508, beta: 1.9896482230123347\n",
      "alpha: 0.504081725473761, beta: 1.989650496716861\n",
      "alpha: 0.5040828030969761, beta: 1.9896527493522216\n",
      "alpha: 0.5040838711158383, beta: 1.9896549811138533\n",
      "alpha: 0.5040849296155321, beta: 1.989657192195378\n",
      "alpha: 0.5040859786804909, beta: 1.9896593827886195\n",
      "alpha: 0.5040870183944034, beta: 1.9896615530836201\n",
      "alpha: 0.5040880488402205, beta: 1.9896637032686577\n",
      "alpha: 0.5040890701001614, beta: 1.9896658335302617\n",
      "alpha: 0.50409008225572, beta: 1.9896679440532292\n",
      "alpha: 0.5040910853876719, beta: 1.9896700350206415\n",
      "alpha: 0.5040920795760797, beta: 1.9896721066138798\n",
      "alpha: 0.5040930649002999, beta: 1.9896741590126412\n",
      "alpha: 0.504094041438989, beta: 1.989676192394954\n",
      "alpha: 0.5040950092701092, beta: 1.9896782069371937\n",
      "alpha: 0.5040959684709353, beta: 1.9896802028140979\n",
      "alpha: 0.50409691911806, beta: 1.9896821801987816\n",
      "alpha: 0.5040978612874002, beta: 1.989684139262753\n",
      "alpha: 0.5040987950542029, beta: 1.989686080175927\n",
      "alpha: 0.5040997204930509, beta: 1.9896880031066417\n",
      "alpha: 0.5041006376778687, beta: 1.989689908221672\n",
      "alpha: 0.5041015466819286, beta: 1.9896917956862437\n",
      "alpha: 0.5041024475778558, beta: 1.9896936656640494\n",
      "alpha: 0.5041033404376347, beta: 1.9896955183172615\n",
      "alpha: 0.504104225332614, beta: 1.989697353806547\n",
      "alpha: 0.5041051023335126, beta: 1.9896991722910808\n",
      "alpha: 0.5041059715104249, beta: 1.9897009739285605\n",
      "alpha: 0.5041068329328267, beta: 1.9897027588752192\n",
      "alpha: 0.5041076866695798, beta: 1.98970452728584\n",
      "alpha: 0.5041085327889385, beta: 1.9897062793137685\n",
      "alpha: 0.5041093713585539, beta: 1.9897080151109272\n",
      "alpha: 0.5041102024454799, beta: 1.9897097348278279\n",
      "alpha: 0.5041110261161779, beta: 1.989711438613585\n",
      "alpha: 0.5041118424365223, beta: 1.9897131266159291\n",
      "alpha: 0.5041126514718058, beta: 1.9897147989812192\n",
      "alpha: 0.5041134532867443, beta: 1.9897164558544551\n",
      "alpha: 0.5041142479454818, beta: 1.9897180973792914\n",
      "alpha: 0.5041150355115956, beta: 1.9897197236980486\n",
      "alpha: 0.5041158160481015, beta: 1.9897213349517262\n",
      "\n",
      "Coefficients from gradient descent algorithm: \n",
      " 1.9897213349517262\n",
      "\n",
      "Intercept from gradient descent algorithm: \n",
      " 0.5041158160481015\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAH+VJREFUeJzt3XuYHHWd7/H3JxdyASUwGTEkGQYCkhNxIThiOMCaRZGgSDhelrBGAZEYRcFVDhePixpdF9RHVkXFCCq3h8sCYsyiiEIE1AADhEgILAmXXLhkCCQQIpHA9/xR1ZOenu6ZnsnU9EzX5/U89aQuv6r6Vtekv/37/eqiiMDMzAxgSK0DMDOzgcNJwczM2jkpmJlZOycFMzNr56RgZmbtnBTMzKydk4JZGUr8XNILku6udTyVSFok6ZNVlj1M0iNZx9RNDF+SdHEtY7CuOSnkhKQnJP1N0qai4cJaxzWAHQocAUyIiINqHUxfiIg7ImLfwnT6N/GerPYnabqkNSUxfDMiqkpiVhvDah2A9asPRMTvuyskaVhEbO1uXk+3kbU+3ucewBMR8XKN4xiQJAlQRLxe61isb7mmYEg6UdKfJF0gaT3w1Qrzhkj6sqQnJa2TdJmkndNtNEsKSSdLWgXcWmY/YyUtlLRB0vOS7pA0JF02UdINktokrS/UYnqzT0nTJP053c8DkqaXHOtjkl6S9Likj5aJ82TgYuDgtEb1tXT+KZJWpLEvkLR70Toh6VRJjwKPVvicu4rrJEnL07gek/SpknVnSloi6UVJKyXNKFq8R3quXpL0O0ljK+y//Ze7pMuBJuDX6TGeWUWMiyT9u6Q/AZuBvSrFLWlH4DfA7kU1090lfVXSFUXbPEbSsnR/iyT9r6JlT0g6Q9JSSRslXSNpZLljsz4UER5yMABPAO+psOxEYCvwOZLa46gK8z4BrAD2AnYCbgAuT7fRDARwGbAjMKrMfv4DuAgYng6HAQKGAg8AF6TrjgQOTdfp0T6B8cB64H0kP3qOSKcb0zIvAvum648D3trFZ3Jn0fThwHPAgcAI4AfA7UXLA7gF2LXCsVeMK13+fmBS+nm8i+RL98B02UHAxnSdIem2JqfLFgErgbekx78IOK/CMU0H1lT6m6gixkXAKuCt6d/E8G7i7rC/dN5XgSvS8bcAL6f7GQ6cmZ7rHYriuxvYPf1clwNza/1/qd6HmgfgoZ9OdPIfbBOwoWg4JV12IrCqpHy5eX8APlM0vS/wavoF0Zx+Me7VRQzzgF8Be5fMPxhoA4aVWadH+wTOIk0aRfNuBk4gSQobgA9R5ou7zPEXJ4VLgG8VTe+UxtGcTgdweBfbqxhXhfI3Aqen4z8BLqhQbhHw5aLpzwC/rVC2w5c0nZNClzGm+5rXzedWHHeH/aXzvsq2pPBvwLVFy4YAa4HpRfHNLlr+LeCiWv9fqvfBzUf5cmxEjCkaflq0bHWZ8qXzdgeeLJp+kuTLebdutlPwbZJfgr9LmxrOTudPBJ6M8u3wPd3nHsBH0uaIDZI2kHQaj4ukf+A4YC7wtKT/ljS5i3grxhERm0h+RY+vEEepinEBSDpK0uK0aWoDya/1QjPQRJLaQCXPFI1vJklYvdFljKkOx9hN3N0p/UxfT7df/Jn21bFZldzRbAXlHpdbOu8pki+OgiaSJqZngQldbCdZEPES8EXgi5L2A26VdA/JF0GTynfQ9nSfq0l+7Z5SIYabgZsljQK+AfyUpBmrOx3iSNvMG0h+2bZvvov1K8YlaQRwPfBx4FcR8aqkG0maZArrTqoixp4qjbfLz650nSri7u4RzE8BbyvankgS4NqKa1jmXFOwnrgK+FdJe0raCfgmcE2FX/idSDpa0t7pf/6NwGvA6yTtxk8D50naUdJISYf0cp9XAB+QdKSkoem2pkuaIGm3tMN2R2ALSXNatVfPXAWcJOmA9Mvwm8BdEfFEletXjAvYgaSfog3YKuko4L1F616S7vvdSjrex/eghtOVZ0n6aqqJsZzu4n4WaFB6YUAZ1wLvT49rOMkPhi3An7fjmGw7OSnkS+FKk8Lwyx6u/zPgcuB24HHgFZKO6GrtA/ye5Mv4L8CPIuK2iHgN+ACwN0lH5hqSZp4e7zMiVgMzgS+RfFmtBv4vyd/6EOALJL9QnyfpGP10NYFHcinvv5H8Mn6a5Jf7rOoOu+u40hrUaSRfki8A/wIsKFr3buAkko74jcAf6Vh76q3/AL6cNhWd0c1nV+6Yuov7YZJk+li6j91L1n8EmE3Saf8cyd/AByLi731wbNZLSjtwzMzMXFMwM7NtnBTMzKydk4KZmbVzUjAzs3aD7j6FsWPHRnNzc63DMDMbVO69997nIqKxu3KDLik0NzfT2tpa6zDMzAYVSU92X8rNR2ZmVsRJwczM2mWeFNLb5e+XtLDMshHpM9JXSLpLUnPW8ZiZWWX9UVM4neQ56OWcDLwQEXuT3MJ/fj/EY2ZmFWSaFNIHab2f5C1W5cwELk3HrwPenT4szczMaiDrmsJ/krxNqdKTKMeTPp89ferlRpLHEXcgaY6kVkmtbW1tWcVqZpZ7mSUFSUcD6yLi3u3dVkTMj4iWiGhpbOz2MlszM+ulLGsKhwDHSHoCuBo4vPiF3am1JC/VQNIwYGeSt1llYtX6zVlt2sysLmSWFCLinIiYEBHNJM+dvzUiZpcUW0Dy7lyAD6dlMnmW96r1mzn96vudGMzMutDvdzRLmge0RsQCkjdKXS5pBclLT6p+aUlPNTWM5nuzptLUMDqrXZiZDXr9khQiYhGwKB0/t2j+K8BH+iMGwAnBzKwbubuj2c1HZmaV5SopuF/BzKxruUoK7lcwM+tarpICuF/BzKwruUsKbjoyM6ssV0nBfQpmZl3LVVJwn4KZWddylRTMzKxruUoKbj4yM+tarpKCm4/MzLqWq6QASWJwTcHMrLzcJQU3IZmZVZa7pOAmJDOzynKXFMB3NZuZVZLLpOCmIzOz8nKXFNynYGZWWWZJQdJISXdLekDSMklfK1PmREltkpakwyeziqfAfQpmZpVl+ea1LcDhEbFJ0nDgTkm/iYjFJeWuiYjPZhiHmZlVKbOaQiQ2pZPD0yGy2l+13HxkZlZZpn0KkoZKWgKsA26JiLvKFPuQpKWSrpM0Mct4wM1HZmZdyTQpRMRrEXEAMAE4SNJ+JUV+DTRHxD8AtwCXltuOpDmSWiW1trW1bXdcTghmZuX1y9VHEbEBuA2YUTJ/fURsSScvBt5eYf35EdESES2NjY19EpObj8zMOsvy6qNGSWPS8VHAEcDDJWXGFU0eAyzPKp5i7lcwMysvy6uPxgGXShpKknyujYiFkuYBrRGxADhN0jHAVuB54MQM42nnfgUzs/IySwoRsRSYWmb+uUXj5wDnZBWDmZn1TO7uaAY3H5mZVZLLpODmIzOz8nKZFMzMrLxcJgU3H5mZlZfLpODmIzOz8nKZFMzMrLxcJgU3H5mZlZfLpODmIzOz8nKZFCBJDK4pmJl1lNuk4CYkM7POcpsU3IRkZtZZbpOCmZl1ltuk4OYjM7POcpsU3HxkZtZZbpOCmZl1ltuk4OYjM7POcpsU3HxkZtZZlu9oHinpbkkPSFom6WtlyoyQdI2kFZLuktScVTzl+AY2M7OOsqwpbAEOj4j9gQOAGZKmlZQ5GXghIvYGLgDOzzCeTtyEZGbWUWZJIRKb0snh6RAlxWYCl6bj1wHvlqSsYirlJiQzs44y7VOQNFTSEmAdcEtE3FVSZDywGiAitgIbgYYy25kjqVVSa1tbW5Yhm5nlWqZJISJei4gDgAnAQZL26+V25kdES0S0NDY29ll8bj4yM+uoX64+iogNwG3AjJJFa4GJAJKGATsD6/sjJkiaj86aMdnNR2ZmqSyvPmqUNCYdHwUcATxcUmwBcEI6/mHg1ogo7XfIzKr1mzn/tw+7pmBmlhqW4bbHAZdKGkqSfK6NiIWS5gGtEbEAuAS4XNIK4HlgVobxdOKOZjOzjjJLChGxFJhaZv65ReOvAB/JKoZqFO5VcGIwM8vxHc0F7mw2M9sm90nBTUhmZtvkPimYmdk2uU8Kbj4yM9sm90nB9yqYmW2T+6TgexXMzLbJfVJwR7OZ2Ta5Twrg9yqYmRU4KeDOZjOzAicF3NlsZlbgpIA7m83MCpwUcGezmVmBk4KZmbVzUsAdzWZmBU4KuKPZzKzASQF3NJuZFTgp4I5mM7OCLN/RPFHSbZIekrRM0ullykyXtFHSknQ4t9y2+oPvajYzy7amsBX4YkRMAaYBp0qaUqbcHRFxQDrMyzCeLrmz2cwsw6QQEU9HxH3p+EvAcmB8VvvbXu5sNjPrpz4FSc3AVOCuMosPlvSApN9IemuF9edIapXU2tbWlkmM7mw2M+uHpCBpJ+B64PMR8WLJ4vuAPSJif+AHwI3lthER8yOiJSJaGhsbM4nTNQUzs4yTgqThJAnhyoi4oXR5RLwYEZvS8ZuA4ZLGZhlTJa4pmJlle/WRgEuA5RHx3Qpl3pyWQ9JBaTzrs4qpK74s1cwMhmW47UOAjwF/lbQknfcloAkgIi4CPgx8WtJW4G/ArIiIDGPqUuGyVCcGM8urzJJCRNwJqJsyFwIXZhVDTxUuS3WNwczyync0F3Fns5nlnZNCEXc2m1neOSkUcU3BzPLOSaGIawpmlndOCkUKl6WameWVk0IZfjCemeWVk0IJ9yuYWZ45KZRwv4KZ5ZmTQgnXFMwsz7pNCpKGSvpOfwQzELimYGZ51m1SiIjXgEP7IZYBwTUFM8uzapuP7pe0QNLHJH2wMGQaWY24pmBmeVbtA/FGkjzS+vCieQF0ekfCYOd7Fcwsz6pKChFxUtaBDDR+WqqZ5VFVzUeSJkj6paR16XC9pAlZB1cr7lcws7yqtk/h58ACYPd0+HU6ry65X8HM8qrapNAYET+PiK3p8AugMcO4aso1BTPLq2qTwnpJs9N7FoZKmk0371KWNFHSbZIekrRM0ullykjS9yWtkLRU0oG9OYi+5pqCmeVVtUnhE8A/A88AT5O8W7m7zuetwBcjYgowDThV0pSSMkcB+6TDHODHVcaTKV+BZGZ5VdUdzcAHI+KYiGiMiDdFxLERsaqr9SLi6Yi4Lx1/CVgOjC8pNhO4LBKLgTGSxvXuUPqen5ZqZnlT7R3Nx2/PTiQ1A1OBu0oWjQdWF02voXPiQNIcSa2SWtva2rYnlKq5X8HM8qja5qM/SbpQ0mGSDiwM1awoaSfgeuDzEfFib4KMiPkR0RIRLY2N/dO/7X4FM8ujau9oPiD9d17RvKDjHc6dSBpOkhCujIhydz+vBSYWTU9I59WcawpmlkfV9CkMAX4cEf9UMnSXEARcAiyPiO9WKLYA+Hh6FdI0YGNEPN3Tg8hCoaaweGWXF1mZmdWVavoUXgfO7MW2DwE+BhwuaUk6vE/SXElz0zI3AY8BK4CfAp/pxX4yUagpuAnJzPKk2uaj30s6A7gGeLkwMyKer7RCRNwJqKuNRkQAp1YZQ7+bNqnBTUhmlivVdjQfR/LlfTtwbzq0ZhXUQOHOZjPLm2qfkrpn1oEMRO5sNrO86bKmIOnMovGPlCz7ZlZBDRTubDazvOmu+WhW0fg5Jctm9HEsA447m80sb7pLCqowXm66Lrmz2czypLukEBXGy03XJXc2m1medNfRvL+kF0lqBaPScdLpkZlGNkC4s9nM8qTLmkJEDI2IN0bEGyJiWDpemB7eX0HW0qr1m5m3cJlrCmaWC9Xep5Bzueg+MTNzUuhOU8NoLpr99lqHYWbWL5wUqjT3ilY3IZlZ3XNSqJqbkMys/jkpVKGpYTTnHj3FVyCZWd1zUqiCH3dhZnnhpFAFP+7CzPLCSaFKftyFmeVBZklB0s8krZP0YIXl0yVtLHor27lZxdIXfBObmeVBtW9e641fABcCl3VR5o6IODrDGPqYr0Ays/qWWU0hIm4HKr6uc7DxTWxmlge17lM4WNIDkn4j6a2VCkmaI6lVUmtbW1t/xteJb2Izs3pWy6RwH7BHROwP/AC4sVLBiJgfES0R0dLY2NhvAZbz9625eGK4meVUzZJCRLwYEZvS8ZuA4ZLG1iqeau0wrNaVKzOz7NTsG07SmyUpHT8ojWVA3x3mO5vNrN5leUnqVcBfgH0lrZF0sqS5kuamRT4MPCjpAeD7wKyIGNBtM76z2czqnQb493AnLS0t0draWrP9L165nnkLl3HR7BbXGMxs0JB0b0S0dFfODeQ9tPuYUe5sNrO65aTQC+5sNrN65W+3HvJNbGZWz5wUesk3sZlZPXJS6CX3K5hZPXJS6KUdhg3hqQ1/q3UYZmZ9ykmhFwo3sflR2mZWb5wUesmXpppZPXJS2A5uQjKzeuOk0EtuQjKzeuSksB3chGRm9cZJwczM2jkpbCf3K5hZPXFS2A7uVzCzeuOksJ3cr2Bm9cRJoQ+4CcnM6oWTwnZyE5KZ1ZMsX8f5M0nrJD1YYbkkfV/SCklLJR2YVSxZcxOSmdWLLGsKvwBmdLH8KGCfdJgD/DjDWDLnJiQzqweZJYWIuB14vosiM4HLIrEYGCNpXFbxZMlNSGZWL2rZpzAeWF00vSad14mkOZJaJbW2tbX1S3A95SYkM6sHg6KjOSLmR0RLRLQ0NjbWOhwzs7pVy6SwFphYND0hnTdo7TBsCPeveqHWYZiZ9Votk8IC4OPpVUjTgI0R8XQN49kuTQ2j+dQ/7sVZNyxl8cr1tQ7HzKxXhmW1YUlXAdOBsZLWAF8BhgNExEXATcD7gBXAZuCkrGLpL1ObdmHCLqNrHYaZWa9llhQi4vhulgdwalb7r4WmhtF8Y+Z+zFu4jItmt9DU4ARhZoPLoOhoHkwKVyH5ngUzG4ycFDLy5Rsf9D0LZjboOCn0saaG0Xzj2P0AXFsws0HHSSEDu48ZBbi2YGaDj5NCBgq1hVdfe73WoZiZ9YiTQobWvbTFN7OZ2aDipJCRaZMaOO+Db+MHtz7qJiQzGzScFDK02xtHsnbDK64tmNmg4aSQIdcWzGywcVLIWKG2cMtDz9Q6FDOzbjkpZGzapAbOeO9b+PbvHvGD8sxswHNS6AdHTHkzu71xZK3DMDPrlpNCPxk+ZAhnXb/UfQtmNqA5KfSDpobRfO7wvVn30hb3LZjZgJbZo7Oto5lTx/Pcpi18+3ePMHanEcycWvZ11GZmNeWaQj8q9C1895b/cTOSmQ1ITgr9qKlhNF94z1tY99IWrm1dVetwzMw6yTQpSJoh6RFJKySdXWb5iZLaJC1Jh09mGc9AMHPqeE4+tJkf3raSS+54rNbhmJl1kFlSkDQU+CFwFDAFOF7SlDJFr4mIA9Lh4qziGUj+uaWJcWNGctlfnnQzkpkNKFnWFA4CVkTEYxHxd+BqYGaG+xs0mhpGc9aRk92MZGYDTpZJYTywumh6TTqv1IckLZV0naSJ5TYkaY6kVkmtbW1tWcTa7wrNSD9atJJ/u/GvtQ7HzAyofUfzr4HmiPgH4Bbg0nKFImJ+RLREREtjY2O/BpilM46czEff2cSVd61yYjCzASHL+xTWAsW//Cek89pFRPHDgC4GvpVhPAPS1499GwBXLF7FzqOGc8aRk2sckZnlWZY1hXuAfSTtKWkHYBawoLiApHFFk8cAyzOMZ8A65bBJNL5hB356x+O+IsnMaiqzpBARW4HPAjeTfNlfGxHLJM2TdExa7DRJyyQ9AJwGnJhVPANZU8Novj/rQBp22oFv3fyIE4OZ1YwiotYx9EhLS0u0trbWOoxMLF65ni/81xKee2kLZ82YzMmH7VXrkMysTki6NyJauitX645mKzJtUgNnHTmZiOD83z7Md25+uNYhmVnO+IF4A0zhQXn/ftND/HDRSjb+7VVOOWwSTQ2jaxyZmeWBawoD0Myp47lu7iHMfmcTVyxexf/50Z1+a5uZ9QsnhQGqqWE0Xz/2bcye1sT6l1/ltKvvcwe0mWXOzUcD3NePfRs7jxrOT/64km/893L+unYjs97RxLRJDbUOzczqkJPCIHDGkZM5dO9Grr5nFTcueYqFS59i7rsmcejejU4OZtannBQGiWmTGpg2qYEJu4zi2tbVXHjbSn5y+2PMesdEd0SbWZ/xfQqD0Kr1m7nloWf40aIVrH/5VRp2HM5npu/t13yaWUXV3qfgpDCIrVq/mZ/esZKr717Fq68n8z42rYmWPXZltzeOdNOSmbVzUsiRxSvXs+ypje01B4DhQ8Wsd0ykZY9dAVyDMMs5J4UcWrV+M/eveoHnNm3pkCAEzDxgd/5p3ze1l53atIv7IcxypNqk4I7mOtLUMLr9i/6IKW/mloeeYexOI7jtkXXcuOQpblzyVHvZ4n6I5zZtYexOIwDXKMzyzjWFnLjkjsfav/hbn3yeyxd3fg2ogNnTmmhu2LG9bHHCANxXYTZIuaZgHRQ/cXXm1PHtfQ2w7Yu/UrIoNmwIzH3XJPZ50xs6rFs6Xu0yJxmzgcU1BevgV/evrfhl/ui6l7ho0Uq29uGfTGmHeOk+e5No+nI7WSyr9f7rMe5a77+/Yjtiypt73RfojmbLxOKV63n2xVfap7fnjx3o0CFuZl0bP2YkV51ycK8Sw4BoPpI0A/geMBS4OCLOK1k+ArgMeDuwHjguIp7IMibbPn3d1FPcIV4w0H6d5Sm2wRp3rfc/GGoK1cosKUgaCvwQOAJYA9wjaUFEPFRU7GTghYjYW9Is4HzguKxisoGnqWG03zBnNoBk+ejsg4AVEfFYRPwduBqYWVJmJnBpOn4d8G5JyjAmMzPrQpZJYTywumh6TTqvbJmI2ApsBDq1T0iaI6lVUmtbW1tG4ZqZ2aB4yU5EzI+IlohoaWxsrHU4ZmZ1K8uksBaYWDQ9IZ1XtoykYcDOJB3OZmZWA1kmhXuAfSTtKWkHYBawoKTMAuCEdPzDwK0x2K6RNTOrI5ldfRQRWyV9FriZ5JLUn0XEMknzgNaIWABcAlwuaQXwPEniMDOzGsn0PoWIuAm4qWTeuUXjrwAfyTIGMzOr3qC7o1lSG/BkL1cfCzzXh+EMBj7mfPAx58P2HPMeEdHtlTqDLilsD0mt1dzmXU98zPngY86H/jjmQXFJqpmZ9Q8nBTMza5e3pDC/1gHUgI85H3zM+ZD5MeeqT8HMzLqWt5qCmZl1wUnBzMza5SYpSJoh6RFJKySdXet4+oqkiZJuk/SQpGWSTk/n7yrpFkmPpv/uks6XpO+nn8NSSQfW9gh6R9JQSfdLWphO7ynprvS4rkkfrYKkEen0inR5cy3j3h6Sxki6TtLDkpZLOriez7Okf03/ph+UdJWkkfV4niX9TNI6SQ8WzevxeZV0Qlr+UUknlNtXNXKRFIpe+HMUMAU4XtKU2kbVZ7YCX4yIKcA04NT02M4G/hAR+wB/SKch+Qz2SYc5wI/7P+Q+cTqwvGj6fOCCiNgbeIHkBU5Q9CIn4IK03GD1PeC3ETEZ2J/k+OvyPEsaD5wGtETEfiSPyim8iKvezvMvgBkl83p0XiXtCnwFeCfJu2y+UkgkPRYRdT8ABwM3F02fA5xT67gyOtZfkbzt7hFgXDpvHPBIOv4T4Pii8u3lBstA8sTdPwCHAwsBkdzlOaz0fJM8e+vgdHxYWk61PoZeHPPOwOOlsdfreWbbu1Z2Tc/bQuDIej3PQDPwYG/PK3A88JOi+R3K9WTIRU2B6l74M+ilVeapwF3AbhHxdLroGWC3dLwePov/BM4EXk+nG4ANkbyoCToeU1UvchoE9gTagJ+nzWYXS9qROj3PEbEW+A6wCnia5LzdS/2f54Kentc+O995SQp1T9JOwPXA5yPixeJlkfx0qItrjyUdDayLiHtrHUs/GwYcCPw4IqYCL7OtSQGou/O8C8nrevcEdgd2pHMTSy7093nNS1Ko5oU/g5ak4SQJ4cqIuCGd/aykcenyccC6dP5g/ywOAY6R9ATJe78PJ2lrH5O+qAk6HlO9vMhpDbAmIu5Kp68jSRL1ep7fAzweEW0R8SpwA8m5r/fzXNDT89pn5zsvSaGaF/4MSpJE8l6K5RHx3aJFxS8wOoGkr6Ew/+PpVQzTgI1F1dQBLyLOiYgJEdFMch5vjYiPAreRvKgJOh/voH+RU0Q8A6yWtG86693AQ9TpeSZpNpomaXT6N1443ro+z0V6el5vBt4raZe0lvXedF7P1bqDpR87ct4H/A+wEvh/tY6nD4/rUJKq5VJgSTq8j6Q99Q/Ao8DvgV3T8iK5Emsl8FeSqztqfhy9PPbpwMJ0fC/gbmAF8F/AiHT+yHR6Rbp8r1rHvR3HewDQmp7rG4Fd6vk8A18DHgYeBC4HRtTjeQauIuk3eZWkRnhyb84r8In0+FcAJ/U2Hj/mwszM2uWl+cjMzKrgpGBmZu2cFMzMrJ2TgpmZtXNSMDOzdk4KlluSNqX/Nkv6lz7e9pdKpv/cl9s3y4qTglnyMLIeJYWiu2or6ZAUIuJ/9zAms5pwUjCD84DDJC1Jn+E/VNK3Jd2TPrP+UwCSpku6Q9ICkrtrkXSjpHvT5/7PSeedB4xKt3dlOq9QK1G67Qcl/VXScUXbXqRt70u4Mr2T16xfdfdrxywPzgbOiIijAdIv940R8Q5JI4A/SfpdWvZAYL+IeDyd/kREPC9pFHCPpOsj4mxJn42IA8rs64MkdybvD4xN17k9XTYVeCvwFPAnkmf93Nn3h2tWmWsKZp29l+T5MktIHkPeQPJSE4C7ixICwGmSHgAWkzyQbB+6dihwVUS8FhHPAn8E3lG07TUR8TrJ40qa++RozHrANQWzzgR8LiI6PFBM0nSSR1YXT7+H5OUumyUtInkGT29tKRp/Df//tBpwTcEMXgLeUDR9M/Dp9JHkSHpL+kKbUjuTvAJys6TJJK9DLXi1sH6JO4Dj0n6LRuAfSR7gZjYg+JeIWfLU0dfSZqBfkLyfoRm4L+3sbQOOLbPeb4G5kpaTvBZxcdGy+cBSSfdF8mjvgl+SvEbyAZKn254ZEc+kScWs5vyUVDMza+fmIzMza+ekYGZm7ZwUzMysnZOCmZm1c1IwM7N2TgpmZtbOScHMzNr9f8woO6Jr08/hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run the algorithm.\n",
    "for iter in range(stop):\n",
    "    \n",
    "    # Take a step, assigning the results of our step function to feed into\n",
    "    # the next step.\n",
    "    alpha, beta = step(alpha, beta, learning_rate, x, y)\n",
    "    \n",
    "    # Calculate the error.\n",
    "    error = LR_cost_function(alpha, beta, x, y)\n",
    "    \n",
    "    # Store the error to instpect later.\n",
    "    all_error.append(error)\n",
    "\n",
    "    \n",
    "print('\\nCoefficients from gradient descent algorithm: \\n', beta)\n",
    "print('\\nIntercept from gradient descent algorithm: \\n', alpha)\n",
    "\n",
    "plt.plot(all_error, 'o', ms=.4)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Error scores for each iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Things Get Messy\n",
    "\n",
    "Linear regression is a good way to introduce the gradient descent algorithm because there is only one minimum â€“ one absolute best solution.  In other algorithms, however, there may be both a global minimum (the lowest possible value over the entire surface) and many local minima, areas on the surface that are lower than the surface around them.\n",
    "\n",
    "![local and global minima and maxima](assets/maxima_and_minima.svg)\n",
    "\n",
    "When using the gradient descent algorithm with models that have local minima the algorithm can get 'caught' in one and converge on a less-than-optimal solution.  One way to avoid this is to run the algorithm multiple times with different starting values.\n",
    "\n",
    "Still a bit confused? [This](http://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html) is a useful resource for another explanation.\n",
    "\n",
    "## Stopping rules\n",
    "\n",
    "In the implementation programmed above, the only stopping rule involves the number of iterations.  As you can see from the plot above, this might be a bit inefficient in this case.  Modify the code above by adding a stopping threshold so that the algorithm stops when the difference in error between two successive iterations is less than .001.  With that rule, how many iterations do you need before you stop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Your gradient descent algorithm with stopping threshold here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "105px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
