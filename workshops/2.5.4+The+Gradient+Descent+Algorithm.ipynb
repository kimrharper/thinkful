{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "So far when explaining how regression works, we've said that it finds the model of best fit by minimizing the squared distance between each datapoint and the line of fit.  Squaring the distance removes concerns about positive vs negative signs, and has a heavier penalty for larger distances.  \n",
    "\n",
    "The cost function for a linear regression model $y_i = \\alpha + \\beta x_i$ is:\n",
    "\n",
    "$$\\frac1{n}\\sum_{i=1}^n(y_i-(\\alpha + \\beta x_i))^2$$\n",
    "\n",
    "where $\\alpha + \\beta x_i$ is the prediction of the model $\\alpha + \\beta x$ for predictors $x_i$, $y_i$ is the actual outcome value, and $n$ is the number of distances being summed.\n",
    "\n",
    "For many linear regressions, the model is sufficiently simple that the true minimum of the cost function can be calculated by solving a system of equations.  However, many other models that we will encounter from this point forward are _too complex_ to be solved for a true minimum.  For those models it's useful to use an iterative algorithm that starts from a random set of parameters and slowly works toward optimizing the cost function.\n",
    "\n",
    "One such algorithm is **gradient descent**, which iteratively minimizes the cost function using derivatives.  This approach is robust and flexible, and can be applied to basically any differentiable function.\n",
    "\n",
    "Now we're going to get into the nuts-and-bolts of how gradient descent works (and what differentiable functions are). Hold on to your hats, we're gonna do some calculus!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "After learning about PCA, you should be comfortable with the idea of data as a multi-dimensional space.  When optimizing a linear regression, the dimensions of the space correspond to the number of parameters in the equation, plus the error function we are trying to minimize.  So a model $y_i = \\alpha + \\beta x_i$ with two parameters would yield a three-dimensional space.  Within that space is a *surface* made up of all  possible combinations of parameter values, and the error values that result when we plug those parameters into the cost function.  (In a two-dimensional space, we have lines.  In three dimensions and higher, we have surfaces.)\n",
    "\n",
    "The gradient descent algorithm works iteratively by picking a location on the surface defined by a combination of parameter values, calculating the direction from that point with the steepest 'downhill' gradient, and then moving 'downhill' a set distance.  Then the algorithm picks up the new parameter values of that location on the surface, re-calculates the direction of 'downhill' and moves a set distance again.  The algorithm will repeat this until it finds a location on the surface where all possible gradients away from that location are \"uphill\": in other words, where all other possible combinations of parameters result in higher error values.  The parameter values that define the location at the lowest point of the space represent the \"optimized\" solution to the cost function, and are what the regression returns as a solution.\n",
    "\n",
    "The direction of \"downhill\" is determined by differentiating the cost function and taking the partial derivative of each parameter of the regression equation.  A function is \"differentiable\" if a derivative can be calculated at each value of the function.  A derivative, in turn, is a measure of how sensitive a quantity is to change in another quantity.  In other words, if there is a function $f$ that contains parameters $x$ and $y$, the partial derivative for $x$ (expressed as $\\frac{\\partial}{\\partial y}$) will tell us how much $y$ will change for each unit change in $x$.  We could also calculate $\\frac{\\partial}{\\partial x}$, to find out how much a one-unit change in $y$ will impact $x$.\n",
    "\n",
    "For our two-parameter regression line model, the derivatives are:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\alpha} =\\frac2n \\sum_{i=1}^n - (y^i-(\\alpha + \\beta x_i) )$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\beta} =\\frac2n \\sum_{i=1}^n - x_i(y^i-(\\alpha + \\beta x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Decision-points in Gradient Descent\n",
    "\n",
    "There are three elements of the gradient descent algorithm that require decisions on the part of the operator.  \n",
    "\n",
    "\n",
    "### What are the starting values of the parameters?   \n",
    "\n",
    "Many implementations will start by setting all parameters to zero.  However, this isn't a requirement of the algorithm, and sometimes other starting points may be desirable.\n",
    "\n",
    "\n",
    "### How far do we \"move downhill\" after each iteration?\n",
    "\n",
    "Also called the \"learning rate.\"  A too-small learning rate means the model will be computationally inefficient and take a long time to converge (stop).  A too-large learning rate can result in overshooting the target minimum, resulting in a model that _never_ converges.  Again, most algorithm implementations have pre-determined criteria for setting the learning rate, but these can also be set manually.\n",
    "\n",
    "\n",
    "### When do we stop?\n",
    "\n",
    "In the description above, it sounds like the model runs until it reaches the \"optimal\" solution.  In reality, this isn't computationally efficient.  As the gradient flattens out and we get closer and closer to the minimum value of the error, each iteration of the algorithm will result in a smaller and smaller change in the error.  This can get really slow.  Typically some \"minimal acceptable change\" is decided on a-priori â€“ once the change in error from iteration n-1 to iteration n is smaller than the threshold, the algorithm stops.  To prevent an algorithm that never stops, there is usually also a maximum number of permitted iterations before the gradient stops, even if it hasn't achieved a change under the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guts of Gradient Descent\n",
    "\n",
    "Let's walk through programming a gradient descent algorithm in Python.  There are packages that will do this for you, but for now we'll try it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## Cost function for the linear regression that we will try to optimize.\n",
    "def LR_cost_function (alpha, beta, x, y):\n",
    "    '''Return the cost for a given line and data.\n",
    "    \n",
    "    Alpha and beta are the coeficients that describe the fit line, while\n",
    "    x and y are lists or arrays with the x and y value of each data point.\n",
    "    '''\n",
    "    print('alpha: {}, beta: {}'.format(alpha,beta))\n",
    "    error = 0\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        point_error = (y[i] - (alpha + (beta * x[i]))) ** 2\n",
    "        error += point_error\n",
    "    return error / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Function we'll call each iteration (or step) of the gradient algorithm.\n",
    "def step (alpha_cur, beta_cur, learning_rate, x, y):\n",
    "    '''Move downhill from a current cost function to a new, more optimal one.'''\n",
    "    alpha = 0\n",
    "    beta = 0\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        # Partial derivative of the intercept.\n",
    "        point_alpha = -(2/n) * (y[i] - ((alpha_cur + beta_cur * x[i])))\n",
    "        alpha += point_alpha\n",
    "        \n",
    "        # Partial derivative of the slope.\n",
    "        point_beta = -(2 / n) * x[i] * (y[i] - ((alpha_cur + beta_cur * x[i])))\n",
    "        beta += point_beta\n",
    "        \n",
    "    new_alpha = alpha_cur - learning_rate * alpha \n",
    "    new_beta = beta_cur - learning_rate * beta\n",
    "    return [new_alpha, new_beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# These constants correspond to the decision-points described above.\n",
    "# How many steps to take.\n",
    "stop = 1000\n",
    "\n",
    "# How far to move with each step.\n",
    "learning_rate = .005\n",
    "\n",
    "# Starting values for intercept and slope \n",
    "alpha_start = 0\n",
    "beta_start = 0\n",
    "\n",
    "# Time to make some data!\n",
    "x = np.random.normal(0, 1, 100)\n",
    "y = x * 2 + np.random.sample(100)\n",
    "\n",
    "# Fit an true minimum regression using solved equations.\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x.reshape(-1, 1), y.reshape(-1, 1))\n",
    "\n",
    "print('\\nCoefficients from sklearn: \\n', regr.coef_)\n",
    "print('\\nIntercept from sklearn: \\n', regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now fit an iteratively optimized regression using your custom gradient\n",
    "# descent algorithm.\n",
    "\n",
    "# Storing each iteration to inspect later.\n",
    "all_error=[]\n",
    "\n",
    "# Provide starting values.\n",
    "alpha = alpha_start\n",
    "beta = beta_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.006202237349553552, beta: 0.021460274165782257\n",
      "alpha: 0.01232731777269944, beta: 0.04268739416438931\n",
      "alpha: 0.018376177267359673, beta: 0.06368389998087781\n",
      "alpha: 0.024349740680188726, beta: 0.08445230386235426\n",
      "alpha: 0.030248921837648005, beta: 0.10499509062154341\n",
      "alpha: 0.0360746236755555, beta: 0.12531471793702795\n",
      "alpha: 0.04182773836712819, beta: 0.14541361665019617\n",
      "alpha: 0.04750914744953458, beta: 0.16529419105893373\n",
      "alpha: 0.05311972194897462, beta: 0.18495881920809495\n",
      "alpha: 0.058660322504304, beta: 0.20440985317678967\n",
      "alpha: 0.06413179948921952, beta: 0.22364961936251992\n",
      "alpha: 0.06953499313302228, beta: 0.24268041876220162\n",
      "alpha: 0.07487073363997505, beta: 0.26150452725010503\n",
      "alpha: 0.08013984130727006, beta: 0.2801241958527482\n",
      "alpha: 0.08534312664162316, beta: 0.2985416510207765\n",
      "alpha: 0.09048139047451044, beta: 0.3167590948978616\n",
      "alpha: 0.09555542407606274, beta: 0.33477870558665246\n",
      "alpha: 0.1005660092676337, beta: 0.3526026374118112\n",
      "alpha: 0.10551391853305664, beta: 0.37023302118016466\n",
      "alpha: 0.11039991512860531, beta: 0.387671964438005\n",
      "alpha: 0.11522475319167375, beta: 0.40492155172556915\n",
      "alpha: 0.11998917784818969, beta: 0.42198384482872886\n",
      "alpha: 0.12469392531877645, beta: 0.43886088302792203\n",
      "alpha: 0.12933972302367763, beta: 0.45555468334435506\n",
      "alpha: 0.1339272896864589, beta: 0.4720672407835069\n",
      "alpha: 0.138457335436501, beta: 0.48840052857596383\n",
      "alpha: 0.14293056191029813, beta: 0.5045564984156142\n",
      "alpha: 0.1473476623515752, beta: 0.5205370806952335\n",
      "alpha: 0.15170932171023785, beta: 0.5363441847394858\n",
      "alpha: 0.15601621674016858, beta: 0.5519796990353727\n",
      "alpha: 0.16026901609588254, beta: 0.5674454914601563\n",
      "alpha: 0.16446838042805584, beta: 0.5827434095067837\n",
      "alpha: 0.16861496247793972, beta: 0.5978752805068416\n",
      "alpha: 0.17270940717067335, beta: 0.6128429118510668\n",
      "alpha: 0.17675235170750786, beta: 0.6276480912074408\n",
      "alpha: 0.18074442565695453, beta: 0.6422925867368943\n",
      "alpha: 0.18468625104486916, beta: 0.656778147306647\n",
      "alpha: 0.18857844244348526, beta: 0.6711065027012105\n",
      "alpha: 0.19242160705940814, beta: 0.6852793638310787\n",
      "alpha: 0.19621634482058176, beta: 0.6992984229391309\n",
      "alpha: 0.19996324846224045, beta: 0.7131653538047742\n",
      "alpha: 0.20366290361185707, beta: 0.7268818119458478\n",
      "alpha: 0.20731588887309937, beta: 0.7404494348183158\n",
      "alpha: 0.21092277590880593, beta: 0.7538698420137717\n",
      "alpha: 0.21448412952299298, beta: 0.7671446354547787\n",
      "alpha: 0.21800050774190355, beta: 0.78027539958807\n",
      "alpha: 0.22147246189410968, beta: 0.793263701575632\n",
      "alpha: 0.22490053668967908, beta: 0.8061110914836942\n",
      "alpha: 0.22828527029841653, beta: 0.8188191024696484\n",
      "alpha: 0.23162719442719143, beta: 0.8313892509669202\n",
      "alpha: 0.23492683439636133, beta: 0.8438230368678145\n",
      "alpha: 0.23818470921530255, beta: 0.8561219437043589\n",
      "alpha: 0.24140133165705777, beta: 0.8682874388271651\n",
      "alpha: 0.2445772083321111, beta: 0.880320973582331\n",
      "alpha: 0.2477128397613007, beta: 0.8922239834864053\n",
      "alpha: 0.25080872044787866, beta: 0.9039978883994351\n",
      "alpha: 0.2538653389487285, beta: 0.9156440926961176\n",
      "alpha: 0.2568831779447496, beta: 0.9271639854350779\n",
      "alpha: 0.2598627143104185, beta: 0.9385589405262916\n",
      "alpha: 0.26280441918253666, beta: 0.9498303168966741\n",
      "alpha: 0.2657087580281735, beta: 0.9609794586538557\n",
      "alpha: 0.26857619071181504, beta: 0.9720076952481633\n",
      "alpha: 0.2714071715617265, beta: 0.9829163416328277\n",
      "alpha: 0.27420214943553844, beta: 0.993706698422436\n",
      "alpha: 0.27696156778506514, beta: 1.0043800520496486\n",
      "alpha: 0.27968586472036444, beta: 1.0149376749202008\n",
      "alpha: 0.28237547307304756, beta: 1.0253808255662045\n",
      "alpha: 0.2850308204588477, beta: 1.0357107487977733\n",
      "alpha: 0.2876523293394562, beta: 1.045928675852986\n",
      "alpha: 0.29024041708363435, beta: 1.0560358245462078\n",
      "alpha: 0.29279549602760946, beta: 1.0660333994147875\n",
      "alpha: 0.29531797353476386, beta: 1.0759225918641475\n",
      "alpha: 0.2978082520546243, beta: 1.0857045803112864\n",
      "alpha: 0.3002667291811605, beta: 1.0953805303267081\n",
      "alpha: 0.3026937977104004, beta: 1.104951594774799\n",
      "alpha: 0.30508984569737047, beta: 1.1144189139526661\n",
      "alpha: 0.30745525651236866, beta: 1.1237836157274566\n",
      "alpha: 0.30979040889657766, beta: 1.1330468156721718\n",
      "alpha: 0.31209567701702634, beta: 1.142209617199996\n",
      "alpha: 0.3143714305209069, beta: 1.1512731116971537\n",
      "alpha: 0.31661803458925486, beta: 1.1602383786543118\n",
      "alpha: 0.31883584999000014, beta: 1.1691064857965434\n",
      "alpha: 0.3210252331303954, beta: 1.1778784892118692\n",
      "alpha: 0.3231865361088296, beta: 1.1865554334783908\n",
      "alpha: 0.3253201067660341, beta: 1.1951383517900325\n",
      "alpha: 0.3274262887356877, beta: 1.2036282660809072\n",
      "alpha: 0.32950542149442774, beta: 1.212026187148319\n",
      "alpha: 0.3315578404112749, beta: 1.220333114774421\n",
      "alpha: 0.33358387679647744, beta: 1.2285500378465406\n",
      "alpha: 0.3355838579497825, beta: 1.2366779344761873\n",
      "alpha: 0.33755810720814056, beta: 1.2447177721167588\n",
      "alpha: 0.3395069439928497, beta: 1.2526705076799571\n",
      "alpha: 0.3414306838561468, beta: 1.2605370876509314\n",
      "alpha: 0.3433296385272511, beta: 1.2683184482021608\n",
      "alpha: 0.3452041159578675, beta: 1.2760155153060895\n",
      "alpha: 0.3470544203671547, beta: 1.2836292048465296\n",
      "alpha: 0.34888085228616605, beta: 1.2911604227288456\n",
      "alpha: 0.35068370860176734, beta: 1.2986100649889305\n",
      "alpha: 0.3524632826000392, beta: 1.3059790179009918\n",
      "alpha: 0.35421986400916905, beta: 1.313268158084156\n",
      "alpha: 0.3559537390418395, beta: 1.3204783526079078\n",
      "alpha: 0.35766519043711786, beta: 1.3276104590963735\n",
      "alpha: 0.35935449750185344, beta: 1.3346653258314651\n",
      "alpha: 0.3610219361515884, beta: 1.3416437918548945\n",
      "alpha: 0.36266777895098723, beta: 1.3485466870690712\n",
      "alpha: 0.3642922951537909, beta: 1.355374832336897\n",
      "alpha: 0.3658957507423012, beta: 1.362129039580467\n",
      "alpha: 0.36747840846640023, beta: 1.3688101118786922\n",
      "alpha: 0.3690405278821114, beta: 1.3754188435638521\n",
      "alpha: 0.37058236538970635, beta: 1.381956020317093\n",
      "alpha: 0.3721041742713635, beta: 1.3884224192628793\n",
      "alpha: 0.3736062047283836, beta: 1.3948188090624118\n",
      "alpha: 0.3750887039179669, beta: 1.401145950006025\n",
      "alpha: 0.3765519159895579, beta: 1.4074045941045727\n",
      "alpha: 0.3779960821207615, beta: 1.4135954851798127\n",
      "alpha: 0.37942144055283705, beta: 1.4197193589538046\n",
      "alpha: 0.3808282266257736, beta: 1.4257769431373282\n",
      "alpha: 0.3822166728129527, beta: 1.4317689575173347\n",
      "alpha: 0.38358700875540264, beta: 1.4376961140434417\n",
      "alpha: 0.38493946129564904, beta: 1.4435591169134814\n",
      "alpha: 0.38627425451116676, beta: 1.449358662658113\n",
      "alpha: 0.3875916097474376, beta: 1.4550954402245102\n",
      "alpha: 0.3888917456506184, beta: 1.4607701310591321\n",
      "alpha: 0.39017487819982394, beta: 1.4663834091895895\n",
      "alpha: 0.39144122073902954, beta: 1.4719359413056154\n",
      "alpha: 0.3926909840085972, beta: 1.4774283868391498\n",
      "alpha: 0.39392437617643045, beta: 1.4828613980435483\n",
      "alpha: 0.3951416028687613, beta: 1.4882356200719253\n",
      "alpha: 0.3963428672005745, beta: 1.4935516910546385\n",
      "alpha: 0.3975283698056724, beta: 1.4988102421759275\n",
      "alpha: 0.3986983088663858, beta: 1.504011897749714\n",
      "alpha: 0.39985288014293363, beta: 1.509157275294572\n",
      "alpha: 0.40099227700243645, beta: 1.514246985607879\n",
      "alpha: 0.4021166904475874, beta: 1.5192816328391556\n",
      "alpha: 0.403226309144985, beta: 1.524261814562604\n",
      "alpha: 0.4043213194531308, beta: 1.5291881218488514\n",
      "alpha: 0.4054019054500976, beta: 1.5340611393359118\n",
      "alpha: 0.40646824896086936, beta: 1.538881445299368\n",
      "alpha: 0.40752052958435997, beta: 1.5436496117217893\n",
      "alpha: 0.40855892472011135, beta: 1.5483662043613884\n",
      "alpha: 0.40958360959467727, beta: 1.5530317828199287\n",
      "alpha: 0.4105947572876946, beta: 1.5576469006098879\n",
      "alpha: 0.41159253875764723, beta: 1.5622121052208875\n",
      "alpha: 0.4125771228673248, beta: 1.566727938185397\n",
      "alpha: 0.4135486764089811, beta: 1.5711949351437178\n",
      "alpha: 0.41450736412919453, beta: 1.575613625908259\n",
      "alpha: 0.4154533487534352, beta: 1.5799845345271075\n",
      "alpha: 0.41638679101034104, beta: 1.5843081793469063\n",
      "alpha: 0.41730784965570694, beta: 1.5885850730750424\n",
      "alpha: 0.4182166814961903, beta: 1.5928157228411572\n",
      "alpha: 0.41911344141273593, beta: 1.597000630257983\n",
      "alpha: 0.419998282383724, beta: 1.601140291481514\n",
      "alpha: 0.42087135550784394, beta: 1.6052351972705223\n",
      "alpha: 0.42173281002669766, beta: 1.6092858330454192\n",
      "alpha: 0.4225827933471351, beta: 1.6132926789464765\n",
      "alpha: 0.4234214510633258, beta: 1.61725620989141\n",
      "alpha: 0.4242489269785684, beta: 1.6211768956323331\n",
      "alpha: 0.4250653631268428, beta: 1.6250552008120887\n",
      "alpha: 0.4258708997941062, beta: 1.6288915850199648\n",
      "alpha: 0.4266656755393375, beta: 1.6326865028468016\n",
      "alpha: 0.427449827215332, beta: 1.6364404039394964\n",
      "alpha: 0.4282234899892501, beta: 1.6401537330549116\n",
      "alpha: 0.428986797362922, beta: 1.643826930113195\n",
      "alpha: 0.42973988119291234, beta: 1.647460430250516\n",
      "alpha: 0.4304828717103464, beta: 1.6510546638712273\n",
      "alpha: 0.4312158975405017, beta: 1.6546100566994546\n",
      "alpha: 0.43193908572216716, beta: 1.6581270298301227\n",
      "alpha: 0.43265256172677247, beta: 1.661605999779426\n",
      "alpha: 0.433356449477291, beta: 1.6650473785347448\n",
      "alpha: 0.4340508713669182, beta: 1.6684515736040182\n",
      "alpha: 0.4347359482775284, beta: 1.671818988064576\n",
      "alpha: 0.4354117995979124, beta: 1.6751500206114378\n",
      "alpha: 0.43607854324179934, beta: 1.6784450656050847\n",
      "alpha: 0.4367362956656639, beta: 1.6817045131187078\n",
      "alpha: 0.4373851718863224, beta: 1.6849287489849416\n",
      "alpha: 0.43802528549832015, beta: 1.6881181548420858\n",
      "alpha: 0.4386567486911118, beta: 1.6912731081798233\n",
      "alpha: 0.4392796722660385, beta: 1.6943939823844374\n",
      "alpha: 0.43989416565310235, beta: 1.697481146783536\n",
      "alpha: 0.44050033692754265, beta: 1.7005349666902867\n",
      "alpha: 0.4410982928262142, beta: 1.7035558034471683\n",
      "alpha: 0.4416881387637717, beta: 1.7065440144692452\n",
      "alpha: 0.4422699788486614, beta: 1.7094999532869686\n",
      "alpha: 0.4428439158989228, beta: 1.71242396958851\n",
      "alpha: 0.4434100514578023, beta: 1.7153164092616318\n",
      "alpha: 0.4439684858091816, beta: 1.7181776144351022\n",
      "alpha: 0.44451931799282185, beta: 1.721007923519656\n",
      "alpha: 0.4450626458194275, beta: 1.7238076712485084\n",
      "alpha: 0.44559856588553004, beta: 1.7265771887174268\n",
      "alpha: 0.44612717358819504, beta: 1.7293168034243647\n",
      "alpha: 0.44664856313955414, beta: 1.7320268393086622\n",
      "alpha: 0.4471628275811641, beta: 1.7347076167898188\n",
      "alpha: 0.4476700587981948, beta: 1.7373594528058427\n",
      "alpha: 0.4481703475334484, beta: 1.7399826608511817\n",
      "alpha: 0.4486637834012115, beta: 1.7425775510142387\n",
      "alpha: 0.44915045490094246, beta: 1.7451444300144787\n",
      "alpha: 0.4496304494307955, beta: 1.7476836012391292\n",
      "alpha: 0.45010385330098374, beta: 1.750195364779482\n",
      "alpha: 0.4505707517469831, beta: 1.7526800174667954\n",
      "alpha: 0.4510312289425787, beta: 1.7551378529078083\n",
      "alpha: 0.45148536801275585, beta: 1.7575691615198628\n",
      "alpha: 0.4519332510464373, beta: 1.759974230565645\n",
      "alpha: 0.45237495910906866, beta: 1.762353344187547\n",
      "alpha: 0.4528105722550537, beta: 1.7647067834416525\n",
      "alpha: 0.45324016954004137, beta: 1.767034826331352\n",
      "alpha: 0.4536638290330661, beta: 1.7693377478405907\n",
      "alpha: 0.4540816278285436, beta: 1.7716158199667533\n",
      "alpha: 0.4544936420581231, beta: 1.7738693117531907\n",
      "alpha: 0.45489994690239827, beta: 1.77609848932139\n",
      "alpha: 0.4553006166024785, beta: 1.7783036159027947\n",
      "alpha: 0.45569572447142187, beta: 1.7804849518702766\n",
      "alpha: 0.45608534290553165, beta: 1.7826427547692643\n",
      "alpha: 0.45646954339551776, beta: 1.784777279348533\n",
      "alpha: 0.45684839653752507, beta: 1.7868887775906566\n",
      "alpha: 0.4572219720440299, beta: 1.7889774987421276\n",
      "alpha: 0.45759033875460636, beta: 1.7910436893431498\n",
      "alpha: 0.45795356464656367, beta: 1.7930875932571033\n",
      "alpha: 0.4583117168454569, beta: 1.7951094516996904\n",
      "alpha: 0.4586648616354716, beta: 1.7971095032677615\n",
      "alpha: 0.45901306446968443, beta: 1.7990879839678278\n",
      "alpha: 0.45935638998020106, beta: 1.801045127244261\n",
      "alpha: 0.45969490198817275, beta: 1.8029811640071873\n",
      "alpha: 0.4600286635136932, beta: 1.8048963226600756\n",
      "alpha: 0.4603577367855769, beta: 1.8067908291270247\n",
      "alpha: 0.4606821832510204, beta: 1.8086649068797538\n",
      "alpha: 0.461002063585148, beta: 1.8105187769642967\n",
      "alpha: 0.46131743770044326, beta: 1.8123526580274063\n",
      "alpha: 0.4616283647560674, beta: 1.81416676634267\n",
      "alpha: 0.4619349031670661, beta: 1.8159613158363404\n",
      "alpha: 0.4622371106134661, beta: 1.8177365181128844\n",
      "alpha: 0.46253504404926304, beta: 1.8194925824802533\n",
      "alpha: 0.4628287597113009, beta: 1.8212297159748763\n",
      "alpha: 0.4631183131280458, beta: 1.8229481233863836\n",
      "alpha: 0.4634037591282544, beta: 1.8246480072820568\n",
      "alpha: 0.4636851518495378, beta: 1.826329568031015\n",
      "alpha: 0.4639625447468238, beta: 1.8279930038281356\n",
      "alpha: 0.46423599060071663, beta: 1.8296385107177144\n",
      "alpha: 0.46450554152575707, beta: 1.8312662826168686\n",
      "alpha: 0.46477124897858324, beta: 1.8328765113386822\n",
      "alpha: 0.46503316376599346, beta: 1.8344693866151016\n",
      "alpha: 0.4652913360529122, beta: 1.8360450961195793\n",
      "alpha: 0.4655458153702607, beta: 1.8376038254894718\n",
      "alpha: 0.46579665062273296, beta: 1.8391457583481927\n",
      "alpha: 0.4660438900964782, beta: 1.8406710763271243\n",
      "alpha: 0.4662875814666911, beta: 1.8421799590872903\n",
      "alpha: 0.4665277718051111, beta: 1.8436725843407913\n",
      "alpha: 0.46676450758743143, beta: 1.8451491278720078\n",
      "alpha: 0.46699783470061873, beta: 1.8466097635585712\n",
      "alpha: 0.4672277984501454, beta: 1.8480546633921058\n",
      "alpha: 0.4674544435671344, beta: 1.849483997498745\n",
      "alpha: 0.46767781421541843, beta: 1.8508979341594238\n",
      "alpha: 0.4678979539985142, beta: 1.8522966398299503\n",
      "alpha: 0.4681149059665126, beta: 1.8536802791608569\n",
      "alpha: 0.4683287126228862, beta: 1.8550490150170365\n",
      "alpha: 0.4685394159312147, beta: 1.8564030084971643\n",
      "alpha: 0.4687470573218293, beta: 1.8577424189529073\n",
      "alpha: 0.46895167769837764, beta: 1.859067404007924\n",
      "alpha: 0.46915331744430894, beta: 1.8603781195766587\n",
      "alpha: 0.4693520164292814, beta: 1.8616747198829287\n",
      "alpha: 0.46954781401549267, beta: 1.86295735747831\n",
      "alpha: 0.46974074906393337, beta: 1.8642261832603229\n",
      "alpha: 0.46993085994056605, beta: 1.8654813464904179\n",
      "alpha: 0.4701181845224291, beta: 1.8667229948117672\n",
      "alpha: 0.4703027602036672, beta: 1.8679512742668605\n",
      "alpha: 0.47048462390148926, beta: 1.8691663293149103\n",
      "alpha: 0.4706638120620543, beta: 1.8703683028490674\n",
      "alpha: 0.47084036066628643, beta: 1.8715573362134474\n",
      "alpha: 0.47101430523561943, beta: 1.8727335692199734\n",
      "alpha: 0.47118568083767254, beta: 1.8738971401650333\n",
      "alpha: 0.4713545220918572, beta: 1.8750481858459576\n",
      "alpha: 0.47152086317491626, beta: 1.8761868415773149\n",
      "alpha: 0.4716847378263967, beta: 1.8773132412070326\n",
      "alpha: 0.47184617935405543, beta: 1.8784275171323397\n",
      "alpha: 0.47200522063920075, beta: 1.8795298003155372\n",
      "alpha: 0.4721618941419684, beta: 1.8806202202995956\n",
      "alpha: 0.47231623190653454, beta: 1.8816989052235837\n",
      "alpha: 0.47246826556626526, beta: 1.8827659818379272\n",
      "alpha: 0.47261802634880407, beta: 1.8838215755195034\n",
      "alpha: 0.47276554508109775, beta: 1.8848658102865696\n",
      "alpha: 0.4729108521943616, beta: 1.8858988088135296\n",
      "alpha: 0.4730539777289843, beta: 1.886920692445539\n",
      "alpha: 0.4731949513393737, beta: 1.8879315812129518\n",
      "alpha: 0.473333802298744, beta: 1.8889315938456088\n",
      "alpha: 0.47347055950384453, beta: 1.889920847786971\n",
      "alpha: 0.4736052514796319, beta: 1.8908994592080983\n",
      "alpha: 0.4737379063838852, beta: 1.8918675430214773\n",
      "alpha: 0.47386855201176525, beta: 1.892825212894696\n",
      "alpha: 0.47399721580031895, beta: 1.8937725812639725\n",
      "alpha: 0.47412392483292853, beta: 1.8947097593475333\n",
      "alpha: 0.4742487058437074, beta: 1.8956368571588482\n",
      "alpha: 0.47437158522184236, beta: 1.8965539835197198\n",
      "alpha: 0.4744925890158834, beta: 1.8974612460732307\n",
      "alpha: 0.47461174293798125, beta: 1.8983587512965496\n",
      "alpha: 0.4747290723680735, beta: 1.8992466045135978\n",
      "alpha: 0.4748446023580203, beta: 1.9001249099075783\n",
      "alpha: 0.4749583576356894, beta: 1.9009937705333675\n",
      "alpha: 0.47507036260899194, beta: 1.9018532883297725\n",
      "alpha: 0.47518064136986904, beta: 1.9027035641316548\n",
      "alpha: 0.47528921769823, beta: 1.9035446976819217\n",
      "alpha: 0.4753961150658429, beta: 1.9043767876433877\n",
      "alpha: 0.4755013566401774, beta: 1.9051999316105057\n",
      "alpha: 0.47560496528820156, beta: 1.9060142261209716\n",
      "alpha: 0.47570696358013176, beta: 1.9068197666672015\n",
      "alpha: 0.4758073737931377, beta: 1.9076166477076846\n",
      "alpha: 0.47590621791500176, beta: 1.9084049626782118\n",
      "alpha: 0.47600351764773424, beta: 1.909184804002983\n",
      "alpha: 0.47609929441114457, beta: 1.9099562631055924\n",
      "alpha: 0.4761935693463688, beta: 1.9107194304198947\n",
      "alpha: 0.47628636331935453, beta: 1.9114743954007527\n",
      "alpha: 0.47637769692430304, beta: 1.9122212465346684\n",
      "alpha: 0.47646759048706977, beta: 1.912960071350298\n",
      "alpha: 0.4765560640685233, beta: 1.9136909564288522\n",
      "alpha: 0.4766431374678633, beta: 1.914413987414383\n",
      "alpha: 0.47672883022589824, beta: 1.9151292490239598\n",
      "alpha: 0.47681316162828297, beta: 1.9158368250577333\n",
      "alpha: 0.4768961507087169, beta: 1.9165367984088908\n",
      "alpha: 0.4769778162521033, beta: 1.917229251073502\n",
      "alpha: 0.47705817679766976, beta: 1.9179142641602591\n",
      "alpha: 0.47713725064205087, beta: 1.9185919179001096\n",
      "alpha: 0.4772150558423332, beta: 1.9192622916557847\n",
      "alpha: 0.4772916102190629, beta: 1.919925463931224\n",
      "alpha: 0.477366931359217, beta: 1.920581512380898\n",
      "alpha: 0.4774410366191379, beta: 1.921230513819028\n",
      "alpha: 0.47751394312743256, beta: 1.9218725442287068\n",
      "alpha: 0.47758566778783584, beta: 1.9225076787709197\n",
      "alpha: 0.47765622728203905, beta: 1.9231359917934667\n",
      "alpha: 0.4777256380724838, beta: 1.923757556839789\n",
      "alpha: 0.4777939164051219, beta: 1.9243724466576984\n",
      "alpha: 0.4778610783121413, beta: 1.9249807332080109\n",
      "alpha: 0.4779271396146586, beta: 1.925582487673089\n",
      "alpha: 0.477992115925379, beta: 1.9261777804652875\n",
      "alpha: 0.4780560226512232, beta: 1.9267666812353113\n",
      "alpha: 0.47811887499592276, beta: 1.9273492588804781\n",
      "alpha: 0.47818068796258284, beta: 1.9279255815528953\n",
      "alpha: 0.4782414763562144, beta: 1.928495716667544\n",
      "alpha: 0.47830125478623475, beta: 1.9290597309102784\n",
      "alpha: 0.4783600376689378, beta: 1.9296176902457358\n",
      "alpha: 0.4784178392299339, beta: 1.930169659925162\n",
      "alpha: 0.4784746735065598, beta: 1.9307157044941508\n",
      "alpha: 0.478530554350259, beta: 1.931255887800299\n",
      "alpha: 0.4785854954289329, beta: 1.9317902730007794\n",
      "alpha: 0.4786395102292634, beta: 1.9323189225698303\n",
      "alpha: 0.4786926120590062, beta: 1.9328418983061633\n",
      "alpha: 0.47874481404925706, beta: 1.9333592613402917\n",
      "alpha: 0.4787961291566892, beta: 1.9338710721417782\n",
      "alpha: 0.4788465701657638, beta: 1.9343773905264041\n",
      "alpha: 0.4788961496909134, beta: 1.9348782756632608\n",
      "alpha: 0.47894488017869774, beta: 1.935373786081763\n",
      "alpha: 0.47899277390993417, beta: 1.9358639796785873\n",
      "alpha: 0.47903984300180097, beta: 1.936348913724534\n",
      "alpha: 0.47908609940991526, beta: 1.936828644871315\n",
      "alpha: 0.4791315549303852, beta: 1.9373032291582677\n",
      "alpha: 0.4791762212018367, beta: 1.937772722018995\n",
      "alpha: 0.479220109707416, beta: 1.9382371782879353\n",
      "alpha: 0.47926323177676644, beta: 1.938696652206858\n",
      "alpha: 0.4793055985879817, beta: 1.9391511974312914\n",
      "alpha: 0.4793472211695348, beta: 1.9396008670368783\n",
      "alpha: 0.4793881104021832, beta: 1.940045713525664\n",
      "alpha: 0.47942827702085034, beta: 1.9404857888323148\n",
      "alpha: 0.47946773161648437, beta: 1.9409211443302707\n",
      "alpha: 0.4795064846378935, beta: 1.9413518308378284\n",
      "alpha: 0.4795445463935591, beta: 1.9417778986241607\n",
      "alpha: 0.4795819270534259, beta: 1.9421993974152694\n",
      "alpha: 0.4796186366506708, beta: 1.9426163763998732\n",
      "alpha: 0.47965468508344916, beta: 1.9430288842352321\n",
      "alpha: 0.47969008211662006, beta: 1.9434369690529087\n",
      "alpha: 0.4797248373834499, beta: 1.9438406784644655\n",
      "alpha: 0.4797589603872949, beta: 1.9442400595671023\n",
      "alpha: 0.4797924605032632, beta: 1.9446351589492306\n",
      "alpha: 0.47982534697985596, beta: 1.9450260226959881\n",
      "alpha: 0.4798576289405886, beta: 1.9454126963946938\n",
      "alpha: 0.4798893153855914, beta: 1.9457952251402424\n",
      "alpha: 0.47992041519319084, beta: 1.9461736535404413\n",
      "alpha: 0.47995093712147124, beta: 1.9465480257212888\n",
      "alpha: 0.47998088980981674, beta: 1.946918385332196\n",
      "alpha: 0.4800102817804345, beta: 1.9472847755511509\n",
      "alpha: 0.48003912143985894, beta: 1.9476472390898258\n",
      "alpha: 0.48006741708043743, beta: 1.948005818198632\n",
      "alpha: 0.4800951768817973, beta: 1.9483605546717155\n",
      "alpha: 0.4801224089122949, beta: 1.9487114898519027\n",
      "alpha: 0.48014912113044655, beta: 1.949058664635588\n",
      "alpha: 0.4801753213863418, beta: 1.949402119477573\n",
      "alpha: 0.48020101742303917, beta: 1.9497418943958482\n",
      "alpha: 0.4802262168779444, beta: 1.9500780289763266\n",
      "alpha: 0.48025092728417196, beta: 1.9504105623775245\n",
      "alpha: 0.480275156071889, beta: 1.9507395333351905\n",
      "alpha: 0.48029891056964313, beta: 1.9510649801668856\n",
      "alpha: 0.4803221980056738, beta: 1.9513869407765132\n",
      "alpha: 0.4803450255092066, beta: 1.9517054526587991\n",
      "alpha: 0.4803674001117325, beta: 1.9520205529037242\n",
      "alpha: 0.48038932874827023, beta: 1.9523322782009076\n",
      "alpha: 0.48041081825861354, beta: 1.9526406648439427\n",
      "alpha: 0.48043187538856263, beta: 1.952945748734687\n",
      "alpha: 0.48045250679114054, beta: 1.9532475653875039\n",
      "alpha: 0.48047271902779404, beta: 1.953546149933459\n",
      "alpha: 0.4804925185695797, beta: 1.9538415371244715\n",
      "alpha: 0.4805119117983353, beta: 1.954133761337419\n",
      "alpha: 0.48053090500783635, beta: 1.9544228565781996\n",
      "alpha: 0.48054950440493854, beta: 1.954708856485748\n",
      "alpha: 0.48056771611070553, beta: 1.9549917943360093\n",
      "alpha: 0.480585546161523, beta: 1.9552717030458693\n",
      "alpha: 0.48060300051019855, beta: 1.9555486151770418\n",
      "alpha: 0.48062008502704817, beta: 1.9558225629399137\n",
      "alpha: 0.4806368055009686, beta: 1.9560935781973492\n",
      "alpha: 0.48065316764049687, beta: 1.9563616924684515\n",
      "alpha: 0.48066917707485624, beta: 1.956626936932284\n",
      "alpha: 0.48068483935498907, beta: 1.9568893424315519\n",
      "alpha: 0.48070015995457704, beta: 1.957148939476243\n",
      "alpha: 0.4807151442710482, beta: 1.9574057582472286\n",
      "alpha: 0.4807297976265717, beta: 1.957659828599827\n",
      "alpha: 0.48074412526904, beta: 1.957911180067326\n",
      "alpha: 0.4807581323730386, beta: 1.9581598418644686\n",
      "alpha: 0.480771824040804, beta: 1.9584058428909012\n",
      "alpha: 0.48078520530316915, beta: 1.9586492117345835\n",
      "alpha: 0.4807982811204975, beta: 1.9588899766751613\n",
      "alpha: 0.48081105638360505, beta: 1.9591281656873032\n",
      "alpha: 0.4808235359146708, beta: 1.9593638064440009\n",
      "alpha: 0.48083572446813616, beta: 1.9595969263198336\n",
      "alpha: 0.48084762673159237, beta: 1.9598275523941975\n",
      "alpha: 0.4808592473266576, beta: 1.960055711454499\n",
      "alpha: 0.48087059080984224, beta: 1.9602814299993148\n",
      "alpha: 0.48088166167340396, beta: 1.9605047342415163\n",
      "alpha: 0.48089246434619154, beta: 1.960725650111361\n",
      "alpha: 0.4809030031944783, beta: 1.96094420325955\n",
      "alpha: 0.48091328252278515, beta: 1.961160419060253\n",
      "alpha: 0.48092330657469284, beta: 1.961374322614098\n",
      "alpha: 0.4809330795336446, beta: 1.961585938751133\n",
      "alpha: 0.48094260552373813, beta: 1.9617952920337507\n",
      "alpha: 0.48095188861050786, beta: 1.9620024067595854\n",
      "alpha: 0.4809609328016973, beta: 1.9622073069643753\n",
      "alpha: 0.48096974204802184, beta: 1.9624100164247966\n",
      "alpha: 0.4809783202439213, beta: 1.9626105586612645\n",
      "alpha: 0.4809866712283038, beta: 1.9628089569407055\n",
      "alpha: 0.4809947987852797, beta: 1.963005234279298\n",
      "alpha: 0.48100270664488637, beta: 1.9631994134451853\n",
      "alpha: 0.481010398483804, beta: 1.963391516961157\n",
      "alpha: 0.48101787792606204, beta: 1.9635815671073027\n",
      "alpha: 0.4810251485437369, beta: 1.9637695859236362\n",
      "alpha: 0.48103221385764083, beta: 1.9639555952126915\n",
      "alpha: 0.4810390773380021, beta: 1.9641396165420908\n",
      "alpha: 0.48104574240513637, beta: 1.9643216712470848\n",
      "alpha: 0.48105221243010976, beta: 1.9645017804330644\n",
      "alpha: 0.48105849073539353, beta: 1.9646799649780464\n",
      "alpha: 0.48106458059551016, beta: 1.9648562455351317\n",
      "alpha: 0.48107048523767176, beta: 1.9650306425349364\n",
      "alpha: 0.48107620784240984, beta: 1.9652031761879978\n",
      "alpha: 0.48108175154419736, beta: 1.9653738664871525\n",
      "alpha: 0.48108711943206295, beta: 1.9655427332098907\n",
      "alpha: 0.48109231455019713, beta: 1.9657097959206833\n",
      "alpha: 0.481097339898551, beta: 1.9658750739732846\n",
      "alpha: 0.48110219843342716, beta: 1.9660385865130101\n",
      "alpha: 0.48110689306806315, beta: 1.966200352478989\n",
      "alpha: 0.48111142667320766, beta: 1.9663603906063933\n",
      "alpha: 0.4811158020776892, beta: 1.9665187194286413\n",
      "alpha: 0.4811200220689776, beta: 1.966675357279578\n",
      "alpha: 0.4811240893937381, beta: 1.9668303222956331\n",
      "alpha: 0.481128006758379, beta: 1.9669836324179524\n",
      "alpha: 0.48113177682959185, beta: 1.9671353053945098\n",
      "alpha: 0.4811354022348846, beta: 1.9672853587821932\n",
      "alpha: 0.48113888556310846, beta: 1.9674338099488708\n",
      "alpha: 0.4811422293649777, beta: 1.9675806760754317\n",
      "alpha: 0.4811454361535828, beta: 1.9677259741578077\n",
      "alpha: 0.48114850840489726, beta: 1.9678697210089704\n",
      "alpha: 0.48115144855827774, beta: 1.9680119332609083\n",
      "alpha: 0.48115425901695785, beta: 1.9681526273665824\n",
      "alpha: 0.4811569421485356, beta: 1.9682918196018595\n",
      "alpha: 0.4811595002854548, beta: 1.968429526067426\n",
      "alpha: 0.4811619357254799, beta: 1.9685657626906796\n",
      "alpha: 0.48116425073216523, beta: 1.9687005452276014\n",
      "alpha: 0.48116644753531795, beta: 1.9688338892646073\n",
      "alpha: 0.4811685283314549, beta: 1.9689658102203798\n",
      "alpha: 0.48117049528425404, beta: 1.9690963233476793\n",
      "alpha: 0.4811723505249998, beta: 1.9692254437351362\n",
      "alpha: 0.4811740961530226, beta: 1.9693531863090235\n",
      "alpha: 0.4811757342361331, beta: 1.9694795658350102\n",
      "alpha: 0.4811772668110505, beta: 1.9696045969198954\n",
      "alpha: 0.4811786958838257, beta: 1.9697282940133245\n",
      "alpha: 0.4811800234302586, beta: 1.9698506714094854\n",
      "alpha: 0.48118125139631035, beta: 1.9699717432487878\n",
      "alpha: 0.4811823816985101, beta: 1.9700915235195229\n",
      "alpha: 0.4811834162243567, beta: 1.9702100260595066\n",
      "alpha: 0.48118435683271515, beta: 1.9703272645577035\n",
      "alpha: 0.48118520535420783, beta: 1.9704432525558344\n",
      "alpha: 0.48118596359160076, beta: 1.970558003449966\n",
      "alpha: 0.4811866333201849, beta: 1.9706715304920823\n",
      "alpha: 0.4811872162881526, beta: 1.9707838467916416\n",
      "alpha: 0.48118771421696893, beta: 1.9708949653171137\n",
      "alpha: 0.48118812880173856, beta: 1.9710048988975024\n",
      "alpha: 0.4811884617115676, beta: 1.9711136602238506\n",
      "alpha: 0.48118871458992096, beta: 1.9712212618507297\n",
      "alpha: 0.4811888890549749, beta: 1.9713277161977123\n",
      "alpha: 0.4811889866999653, beta: 1.9714330355508292\n",
      "alpha: 0.48118900909353096, beta: 1.971537232064011\n",
      "alpha: 0.4811889577800531, beta: 1.971640317760513\n",
      "alpha: 0.48118883427998976, beta: 1.9717423045343263\n",
      "alpha: 0.4811886400902064, beta: 1.9718432041515719\n",
      "alpha: 0.4811883766843019, beta: 1.9719430282518808\n",
      "alpha: 0.48118804551293054, beta: 1.9720417883497594\n",
      "alpha: 0.4811876480041195, beta: 1.9721394958359384\n",
      "alpha: 0.4811871855635826, beta: 1.9722361619787097\n",
      "alpha: 0.4811866595750297, beta: 1.972331797925246\n",
      "alpha: 0.48118607140047226, beta: 1.972426414702909\n",
      "alpha: 0.4811854223805246, beta: 1.9725200232205404\n",
      "alpha: 0.4811847138347017, beta: 1.972612634269742\n",
      "alpha: 0.4811839470617127, beta: 1.9727042585261394\n",
      "alpha: 0.48118312333975083, beta: 1.9727949065506332\n",
      "alpha: 0.4811822439267795, beta: 1.9728845887906368\n",
      "alpha: 0.48118131006081455, beta: 1.9729733155813005\n",
      "alpha: 0.481180322960203, beta: 1.9730610971467222\n",
      "alpha: 0.48117928382389796, beta: 1.973147943601145\n",
      "alpha: 0.48117819383173005, beta: 1.9732338649501426\n",
      "alpha: 0.4811770541446754, beta: 1.9733188710917908\n",
      "alpha: 0.48117586590511974, beta: 1.9734029718178265\n",
      "alpha: 0.4811746302371196, beta: 1.9734861768147953\n",
      "alpha: 0.4811733482466596, beta: 1.973568495665185\n",
      "alpha: 0.4811720210219066, beta: 1.9736499378485484\n",
      "alpha: 0.4811706496334605, beta: 1.9737305127426121\n",
      "alpha: 0.48116923513460164, beta: 1.9738102296243762\n",
      "alpha: 0.4811677785615351, beta: 1.9738890976711987\n",
      "alpha: 0.48116628093363173, beta: 1.97396712596187\n",
      "alpha: 0.481164743253666, beta: 1.974044323477677\n",
      "alpha: 0.48116316650805063, beta: 1.9741206991034521\n",
      "alpha: 0.4811615516670684, beta: 1.9741962616286148\n",
      "alpha: 0.4811598996851006, beta: 1.9742710197481994\n",
      "alpha: 0.48115821150085264, beta: 1.9743449820638723\n",
      "alpha: 0.48115648803757677, beta: 1.9744181570849388\n",
      "alpha: 0.4811547302032915, beta: 1.9744905532293386\n",
      "alpha: 0.4811529388909987, beta: 1.9745621788246306\n",
      "alpha: 0.4811511149788972, beta: 1.9746330421089664\n",
      "alpha: 0.481149259330594, beta: 1.9747031512320548\n",
      "alpha: 0.48114737279531267, beta: 1.974772514256114\n",
      "alpha: 0.4811454562080987, beta: 1.974841139156816\n",
      "alpha: 0.48114351039002234, beta: 1.9749090338242166\n",
      "alpha: 0.4811415361483789, beta: 1.9749762060636809\n",
      "alpha: 0.4811395342768861, beta: 1.9750426635967937\n",
      "alpha: 0.48113750555587886, beta: 1.9751084140622635\n",
      "alpha: 0.48113545075250175, beta: 1.975173465016815\n",
      "alpha: 0.48113337062089856, beta: 1.975237823936072\n",
      "alpha: 0.48113126590239963, beta: 1.975301498215432\n",
      "alpha: 0.48112913732570645, beta: 1.9753644951709297\n",
      "alpha: 0.48112698560707395, beta: 1.9754268220400928\n",
      "alpha: 0.4811248114504904, beta: 1.9754884859827868\n",
      "alpha: 0.4811226155478548, beta: 1.9755494940820522\n",
      "alpha: 0.48112039857915184, beta: 1.9756098533449318\n",
      "alpha: 0.48111816121262485, beta: 1.9756695707032894\n",
      "alpha: 0.4811159041049461, beta: 1.9757286530146194\n",
      "alpha: 0.481113627901385, beta: 1.9757871070628479\n",
      "alpha: 0.481111333235974, beta: 1.9758449395591249\n",
      "alpha: 0.4811090207316722, beta: 1.975902157142608\n",
      "alpha: 0.48110669100052705, beta: 1.9759587663812386\n",
      "alpha: 0.48110434464383356, beta: 1.976014773772507\n",
      "alpha: 0.48110198225229145, beta: 1.9760701857442127\n",
      "alpha: 0.4810996044061604, beta: 1.976125008655214\n",
      "alpha: 0.481097211675413, beta: 1.9761792487961707\n",
      "alpha: 0.4810948046198856, beta: 1.976232912390278\n",
      "alpha: 0.4810923837894275, beta: 1.9762860055939933\n",
      "alpha: 0.4810899497240477, beta: 1.9763385344977542\n",
      "alpha: 0.48108750295406005, beta: 1.9763905051266897\n",
      "alpha: 0.4810850440002259, beta: 1.976441923441323\n",
      "alpha: 0.48108257337389554, beta: 1.9764927953382676\n",
      "alpha: 0.4810800915771472, beta: 1.9765431266509141\n",
      "alpha: 0.48107759910292447, beta: 1.9765929231501116\n",
      "alpha: 0.48107509643517166, beta: 1.9766421905448406\n",
      "alpha: 0.48107258404896747, beta: 1.976690934482879\n",
      "alpha: 0.4810700624106568, beta: 1.9767391605514608\n",
      "alpha: 0.48106753197798074, beta: 1.9767868742779278\n",
      "alpha: 0.48106499320020496, beta: 1.9768340811303735\n",
      "alpha: 0.48106244651824615, beta: 1.9768807865182818\n",
      "alpha: 0.48105989236479685, beta: 1.976926995793157\n",
      "alpha: 0.48105733116444865, beta: 1.9769727142491476\n",
      "alpha: 0.4810547633338136, beta: 1.9770179471236635\n",
      "alpha: 0.4810521892816439, beta: 1.9770626995979874\n",
      "alpha: 0.48104960940895036, beta: 1.9771069767978775\n",
      "alpha: 0.4810470241091187, beta: 1.977150783794165\n",
      "alpha: 0.48104443376802475, beta: 1.9771941256033463\n",
      "alpha: 0.4810418387641477, beta: 1.977237007188166\n",
      "alpha: 0.48103923946868204, beta: 1.9772794334581958\n",
      "alpha: 0.48103663624564796, beta: 1.9773214092704066\n",
      "alpha: 0.4810340294520001, beta: 1.9773629394297343\n",
      "alpha: 0.4810314194377349, beta: 1.9774040286896393\n",
      "alpha: 0.48102880654599667, beta: 1.97744468175266\n",
      "alpha: 0.48102619111318173, beta: 1.9774849032709614\n",
      "alpha: 0.4810235734690416, beta: 1.9775246978468755\n",
      "alpha: 0.4810209539367847, beta: 1.9775640700334385\n",
      "alpha: 0.4810183328331761, beta: 1.9776030243349203\n",
      "alpha: 0.4810157104686369, beta: 1.9776415652073487\n",
      "alpha: 0.48101308714734137, beta: 1.977679697059029\n",
      "alpha: 0.48101046316731316, beta: 1.9777174242510562\n",
      "alpha: 0.48100783882052, beta: 1.9777547510978235\n",
      "alpha: 0.4810052143929674, beta: 1.9777916818675239\n",
      "alpha: 0.4810025901647906, beta: 1.9778282207826468\n",
      "alpha: 0.4809999664103457, beta: 1.9778643720204698\n",
      "alpha: 0.48099734339829936, beta: 1.9779001397135447\n",
      "alpha: 0.480994721391717, beta: 1.977935527950178\n",
      "alpha: 0.4809921006481503, beta: 1.9779705407749069\n",
      "alpha: 0.4809894814197229, beta: 1.9780051821889695\n",
      "alpha: 0.4809868639532156, beta: 1.9780394561507706\n",
      "alpha: 0.48098424849014965, beta: 1.9780733665763415\n",
      "alpha: 0.4809816352668693, beta: 1.978106917339796\n",
      "alpha: 0.48097902451462343, beta: 1.978140112273781\n",
      "alpha: 0.48097641645964523, beta: 1.9781729551699212\n",
      "alpha: 0.4809738113232318, beta: 1.9782054497792612\n",
      "alpha: 0.48097120932182175, beta: 1.9782375998127006\n",
      "alpha: 0.4809686106670723, beta: 1.9782694089414263\n",
      "alpha: 0.48096601556593505, beta: 1.9783008807973383\n",
      "alpha: 0.48096342422073074, beta: 1.9783320189734726\n",
      "alpha: 0.4809608368292229, beta: 1.978362827024418\n",
      "alpha: 0.48095825358469074, beta: 1.9783933084667304\n",
      "alpha: 0.48095567467600053, beta: 1.9784234667793403\n",
      "alpha: 0.4809531002876765, beta: 1.9784533054039573\n",
      "alpha: 0.4809505305999704, beta: 1.97848282774547\n",
      "alpha: 0.4809479657889302, beta: 1.978512037172342\n",
      "alpha: 0.4809454060264678, beta: 1.9785409370170022\n",
      "alpha: 0.4809428514804259, beta: 1.978569530576233\n",
      "alpha: 0.4809403023146438, beta: 1.9785978211115525\n",
      "alpha: 0.4809377586890221, beta: 1.9786258118495934\n",
      "alpha: 0.48093522075958706, beta: 1.9786535059824775\n",
      "alpha: 0.4809326886785533, beta: 1.9786809066681867\n",
      "alpha: 0.4809301625943863, beta: 1.9787080170309295\n",
      "alpha: 0.4809276426518635, beta: 1.9787348401615035\n",
      "alpha: 0.48092512899213485, beta: 1.9787613791176544\n",
      "alpha: 0.4809226217527824, beta: 1.9787876369244308\n",
      "alpha: 0.48092012106787896, beta: 1.9788136165745354\n",
      "alpha: 0.4809176270680461, beta: 1.978839321028672\n",
      "alpha: 0.4809151398805111, beta: 1.97886475321589\n",
      "alpha: 0.4809126596291634, beta: 1.978889916033923\n",
      "alpha: 0.4809101864346099, beta: 1.9789148123495255\n",
      "alpha: 0.48090772041422974, beta: 1.9789394449988056\n",
      "alpha: 0.480905261682228, beta: 1.978963816787554\n",
      "alpha: 0.48090281034968924, beta: 1.9789879304915694\n",
      "alpha: 0.48090036652462925, beta: 1.9790117888569803\n",
      "alpha: 0.48089793031204725, beta: 1.9790353946005634\n",
      "alpha: 0.4808955018139764, beta: 1.9790587504100592\n",
      "alpha: 0.480893081129534, beta: 1.9790818589444827\n",
      "alpha: 0.480890668354971, beta: 1.9791047228344327\n",
      "alpha: 0.4808882635837207, beta: 1.979127344682396\n",
      "alpha: 0.48088586690644664, beta: 1.9791497270630494\n",
      "alpha: 0.4808834784110899, beta: 1.9791718725235579\n",
      "alpha: 0.4808810981829159, beta: 1.9791937835838704\n",
      "alpha: 0.48087872630456024, beta: 1.9792154627370113\n",
      "alpha: 0.48087636285607394, beta: 1.9792369124493694\n",
      "alpha: 0.48087400791496815, beta: 1.979258135160984\n",
      "alpha: 0.4808716615562581, beta: 1.979279133285827\n",
      "alpha: 0.48086932385250647, beta: 1.9792999092120835\n",
      "alpha: 0.4808669948738661, beta: 1.9793204653024272\n",
      "alpha: 0.48086467468812194, beta: 1.9793408038942952\n",
      "alpha: 0.4808623633607328, beta: 1.979360927300158\n",
      "alpha: 0.480860060954872, beta: 1.9793808378077875\n",
      "alpha: 0.4808577675314677, beta: 1.9794005376805222\n",
      "alpha: 0.48085548314924254, beta: 1.9794200291575288\n",
      "alpha: 0.4808532078647529, beta: 1.9794393144540618\n",
      "alpha: 0.48085094173242726, beta: 1.9794583957617198\n",
      "alpha: 0.48084868480460424, beta: 1.9794772752486989\n",
      "alpha: 0.48084643713157, beta: 1.9794959550600442\n",
      "alpha: 0.480844198761595, beta: 1.9795144373178977\n",
      "alpha: 0.4808419697409705, beta: 1.979532724121744\n",
      "alpha: 0.48083975011404406, beta: 1.9795508175486531\n",
      "alpha: 0.48083753992325506, beta: 1.9795687196535212\n",
      "alpha: 0.4808353392091693, beta: 1.9795864324693078\n",
      "alpha: 0.4808331480105132, beta: 1.9796039580072713\n",
      "alpha: 0.4808309663642074, beta: 1.9796212982572012\n",
      "alpha: 0.48082879430540026, beta: 1.979638455187649\n",
      "alpha: 0.4808266318675002, beta: 1.9796554307461547\n",
      "alpha: 0.48082447908220804, beta: 1.9796722268594735\n",
      "alpha: 0.480822335979549, beta: 1.9796888454337975\n",
      "alpha: 0.48082020258790353, beta: 1.9797052883549764\n",
      "alpha: 0.4808180789340384, beta: 1.9797215574887355\n",
      "alpha: 0.48081596504313684, beta: 1.9797376546808914\n",
      "alpha: 0.48081386093882866, beta: 1.9797535817575653\n",
      "alpha: 0.48081176664321945, beta: 1.9797693405253944\n",
      "alpha: 0.4808096821769196, beta: 1.9797849327717403\n",
      "alpha: 0.4808076075590729, beta: 1.9798003602648955\n",
      "alpha: 0.48080554280738474, beta: 1.9798156247542877\n",
      "alpha: 0.4808034879381496, beta: 1.9798307279706828\n",
      "alpha: 0.48080144296627836, beta: 1.9798456716263837\n",
      "alpha: 0.4807994079053254, beta: 1.9798604574154284\n",
      "alpha: 0.48079738276751466, beta: 1.979875087013786\n",
      "alpha: 0.480795367563766, beta: 1.9798895620795502\n",
      "alpha: 0.4807933623037207, beta: 1.9799038842531305\n",
      "alpha: 0.48079136699576686, beta: 1.9799180551574418\n",
      "alpha: 0.4807893816470639, beta: 1.9799320763980919\n",
      "alpha: 0.48078740626356764, beta: 1.9799459495635667\n",
      "alpha: 0.4807854408500538, beta: 1.9799596762254137\n",
      "alpha: 0.4807834854101422, beta: 1.979973257938423\n",
      "alpha: 0.48078153994631995, beta: 1.9799866962408077\n",
      "alpha: 0.4807796044599644, beta: 1.97999999265438\n",
      "alpha: 0.480777678951366, beta: 1.9800131486847288\n",
      "alpha: 0.4807757634197505, beta: 1.980026165821391\n",
      "alpha: 0.4807738578633007, beta: 1.9800390455380255\n",
      "alpha: 0.48077196227917857, beta: 1.980051789292582\n",
      "alpha: 0.48077007666354615, beta: 1.9800643985274695\n",
      "alpha: 0.4807682010115866, beta: 1.980076874669723\n",
      "alpha: 0.48076633531752494, beta: 1.9800892191311676\n",
      "alpha: 0.48076447957464813, beta: 1.9801014333085816\n",
      "alpha: 0.4807626337753253, beta: 1.9801135185838572\n",
      "alpha: 0.4807607979110273, beta: 1.9801254763241605\n",
      "alpha: 0.4807589719723462, beta: 1.9801373078820885\n",
      "alpha: 0.480757155949014, beta: 1.9801490145958256\n",
      "alpha: 0.48075534982992196, beta: 1.9801605977892973\n",
      "alpha: 0.4807535536031385, beta: 1.980172058772323\n",
      "alpha: 0.48075176725592766, beta: 1.9801833988407673\n",
      "alpha: 0.4807499907747671, beta: 1.9801946192766888\n",
      "alpha: 0.4807482241453654, beta: 1.980205721348488\n",
      "alpha: 0.48074646735267956, beta: 1.9802167063110538\n",
      "alpha: 0.48074472038093213, beta: 1.9802275754059078\n",
      "alpha: 0.48074298321362774, beta: 1.980238329861347\n",
      "alpha: 0.48074125583356975, beta: 1.980248970892586\n",
      "alpha: 0.4807395382228765, beta: 1.9802594997018959\n",
      "alpha: 0.4807378303629973, beta: 1.9802699174787444\n",
      "alpha: 0.480736132234728, beta: 1.980280225399931\n",
      "alpha: 0.4807344438182267, beta: 1.9802904246297237\n",
      "alpha: 0.4807327650930288, beta: 1.9803005163199927\n",
      "alpha: 0.48073109603806197, beta: 1.9803105016103428\n",
      "alpha: 0.48072943663166084, beta: 1.9803203816282449\n",
      "alpha: 0.48072778685158174, beta: 1.9803301574891652\n",
      "alpha: 0.4807261466750167, beta: 1.9803398302966946\n",
      "alpha: 0.4807245160786074, beta: 1.9803494011426745\n",
      "alpha: 0.48072289503845933, beta: 1.980358871107324\n",
      "alpha: 0.480721283530155, beta: 1.980368241259362\n",
      "alpha: 0.48071968152876726, beta: 1.9803775126561323\n",
      "alpha: 0.4807180890088728, beta: 1.980386686343724\n",
      "alpha: 0.4807165059445647, beta: 1.9803957633570923\n",
      "alpha: 0.4807149323094652, beta: 1.9804047447201771\n",
      "alpha: 0.4807133680767382, beta: 1.9804136314460214\n",
      "alpha: 0.48071181321910156, beta: 1.9804224245368873\n",
      "alpha: 0.4807102677088391, beta: 1.9804311249843713\n",
      "alpha: 0.4807087315178125, beta: 1.9804397337695185\n",
      "alpha: 0.480707204617473, beta: 1.9804482518629352\n",
      "alpha: 0.4807056869788728, beta: 1.9804566802249006\n",
      "alpha: 0.4807041785726762, beta: 1.9804650198054774\n",
      "alpha: 0.4807026793691711, beta: 1.9804732715446203\n",
      "alpha: 0.48070118933827943, beta: 1.9804814363722845\n",
      "alpha: 0.4806997084495682, beta: 1.9804895152085327\n",
      "alpha: 0.4806982366722598, beta: 1.9804975089636405\n",
      "alpha: 0.48069677397524246, beta: 1.9805054185382012\n",
      "alpha: 0.4806953203270804, beta: 1.980513244823229\n",
      "alpha: 0.48069387569602384, beta: 1.9805209887002615\n",
      "alpha: 0.4806924400500188, beta: 1.980528651041461\n",
      "alpha: 0.48069101335671677, beta: 1.9805362327097147\n",
      "alpha: 0.4806895955834841, beta: 1.9805437345587333\n",
      "alpha: 0.48068818669741153, beta: 1.98055115743315\n",
      "alpha: 0.48068678666532316, beta: 1.9805585021686165\n",
      "alpha: 0.4806853954537855, beta: 1.9805657695918992\n",
      "alpha: 0.4806840130291164, beta: 1.9805729605209748\n",
      "alpha: 0.4806826393573936, beta: 1.9805800757651228\n",
      "alpha: 0.48068127440446334, beta: 1.98058711612502\n",
      "alpha: 0.4806799181359488, beta: 1.980594082392831\n",
      "alpha: 0.48067857051725815, beta: 1.9806009753522997\n",
      "alpha: 0.4806772315135929, beta: 1.9806077957788397\n",
      "alpha: 0.4806759010899555, beta: 1.9806145444396221\n",
      "alpha: 0.4806745792111575, beta: 1.9806212220936645\n",
      "alpha: 0.48067326584182707, beta: 1.9806278294919177\n",
      "alpha: 0.48067196094641634, beta: 1.9806343673773517\n",
      "alpha: 0.4806706644892091, beta: 1.9806408364850412\n",
      "alpha: 0.4806693764343278, beta: 1.9806472375422495\n",
      "alpha: 0.4806680967457408, beta: 1.9806535712685123\n",
      "alpha: 0.48066682538726935, beta: 1.98065983837572\n",
      "alpha: 0.4806655623225945, beta: 1.9806660395681992\n",
      "alpha: 0.4806643075152636, beta: 1.9806721755427936\n",
      "alpha: 0.4806630609286973, beta: 1.980678246988944\n",
      "alpha: 0.4806618225261957, beta: 1.9806842545887666\n",
      "alpha: 0.4806605922709449, beta: 1.9806901990171324\n",
      "alpha: 0.480659370126023, beta: 1.9806960809417433\n",
      "alpha: 0.48065815605440676, beta: 1.9807019010232092\n",
      "alpha: 0.48065695001897707, beta: 1.9807076599151239\n",
      "alpha: 0.4806557519825251, beta: 1.980713358264139\n",
      "alpha: 0.4806545619077581, beta: 1.9807189967100394\n",
      "alpha: 0.4806533797573051, beta: 1.9807245758858152\n",
      "alpha: 0.4806522054937223, beta: 1.980730096417735\n",
      "alpha: 0.48065103907949874, beta: 1.9807355589254174\n",
      "alpha: 0.48064988047706164, beta: 1.980740964021902\n",
      "alpha: 0.4806487296487815, beta: 1.9807463123137192\n",
      "alpha: 0.48064758655697754, beta: 1.9807516044009603\n",
      "alpha: 0.4806464511639224, beta: 1.9807568408773457\n",
      "alpha: 0.48064532343184735, beta: 1.9807620223302929\n",
      "alpha: 0.4806442033229471, beta: 1.9807671493409837\n",
      "alpha: 0.48064309079938455, beta: 1.9807722224844313\n",
      "alpha: 0.4806419858232954, beta: 1.9807772423295449\n",
      "alpha: 0.48064088835679286, beta: 1.9807822094391958\n",
      "alpha: 0.48063979836197196, beta: 1.9807871243702817\n",
      "alpha: 0.48063871580091405, beta: 1.9807919876737896\n",
      "alpha: 0.48063764063569114, beta: 1.9807967998948597\n",
      "alpha: 0.4806365728283701, beta: 1.9808015615728474\n",
      "alpha: 0.4806355123410166, beta: 1.980806273241385\n",
      "alpha: 0.48063445913569947, beta: 1.9808109354284424\n",
      "alpha: 0.48063341317449443, beta: 1.9808155486563885\n",
      "alpha: 0.4806323744194881, beta: 1.9808201134420493\n",
      "alpha: 0.4806313428327817, beta: 1.9808246302967683\n",
      "alpha: 0.4806303183764949, beta: 1.9808290997264641\n",
      "alpha: 0.48062930101276924, beta: 1.9808335222316893\n",
      "alpha: 0.480628290703772, beta: 1.9808378983076864\n",
      "alpha: 0.4806272874116994, beta: 1.9808422284444451\n",
      "alpha: 0.48062629109878025, beta: 1.9808465131267583\n",
      "alpha: 0.480625301727279, beta: 1.9808507528342771\n",
      "alpha: 0.48062431925949944, beta: 1.9808549480415663\n",
      "alpha: 0.4806233436577874, beta: 1.9808590992181574\n",
      "alpha: 0.4806223748845343, beta: 1.9808632068286036\n",
      "alpha: 0.4806214129021799, beta: 1.9808672713325322\n",
      "alpha: 0.48062045767321554, beta: 1.9808712931846963\n",
      "alpha: 0.4806195091601868, beta: 1.9808752728350283\n",
      "alpha: 0.48061856732569663, beta: 1.98087921072869\n",
      "alpha: 0.4806176321324079, beta: 1.9808831073061244\n",
      "alpha: 0.48061670354304625, beta: 1.9808869630031047\n",
      "alpha: 0.4806157815204028, beta: 1.9808907782507854\n",
      "alpha: 0.48061486602733666, beta: 1.9808945534757505\n",
      "alpha: 0.4806139570267775, beta: 1.9808982891000624\n",
      "alpha: 0.4806130544817281, beta: 1.9809019855413106\n",
      "alpha: 0.48061215835526655, beta: 1.9809056432126588\n",
      "alpha: 0.480611268610549, beta: 1.9809092625228921\n",
      "alpha: 0.48061038521081156, beta: 1.9809128438764636\n",
      "alpha: 0.4806095081193729, beta: 1.9809163876735407\n",
      "alpha: 0.4806086372996362, beta: 1.9809198943100506\n",
      "alpha: 0.48060777271509136, beta: 1.9809233641777253\n",
      "alpha: 0.4806069143293172, beta: 1.980926797664147\n",
      "alpha: 0.4806060621059833, beta: 1.980930195152791\n",
      "alpha: 0.4806052160088522, beta: 1.9809335570230704\n",
      "alpha: 0.4806043760017813, beta: 1.9809368836503796\n",
      "alpha: 0.4806035420487245, beta: 1.9809401754061355\n",
      "alpha: 0.48060271411373445, beta: 1.980943432657822\n",
      "alpha: 0.4806018921609639, beta: 1.9809466557690296\n",
      "alpha: 0.4806010761546677, beta: 1.9809498450994991\n",
      "alpha: 0.48060026605920453, beta: 1.9809530010051608\n",
      "alpha: 0.48059946183903846, beta: 1.9809561238381757\n",
      "alpha: 0.4805986634587405, beta: 1.9809592139469756\n",
      "alpha: 0.48059787088299033, beta: 1.9809622716763031\n",
      "alpha: 0.48059708407657753, beta: 1.9809652973672502\n",
      "alpha: 0.48059630300440337, beta: 1.9809682913572977\n",
      "alpha: 0.4805955276314821, beta: 1.9809712539803532\n",
      "alpha: 0.4805947579229423, beta: 1.9809741855667895\n",
      "alpha: 0.4805939938440283, beta: 1.980977086443482\n",
      "alpha: 0.4805932353601014, beta: 1.9809799569338455\n",
      "alpha: 0.4805924824366413, beta: 1.980982797357872\n",
      "alpha: 0.48059173503924724, beta: 1.9809856080321662\n",
      "alpha: 0.48059099313363907, beta: 1.980988389269982\n",
      "alpha: 0.4805902566856586, beta: 1.9809911413812578\n",
      "alpha: 0.4805895256612706, beta: 1.9809938646726521\n",
      "alpha: 0.480588800026564, beta: 1.9809965594475785\n",
      "alpha: 0.4805880797477528, beta: 1.9809992260062392\n",
      "alpha: 0.4805873647911772, beta: 1.9810018646456604\n",
      "alpha: 0.48058665512330445, beta: 1.9810044756597256\n",
      "alpha: 0.4805859507107299, beta: 1.9810070593392086\n",
      "alpha: 0.4805852515201779, beta: 1.9810096159718074\n",
      "alpha: 0.48058455751850265, beta: 1.981012145842176\n",
      "alpha: 0.4805838686726891, beta: 1.9810146492319576\n",
      "alpha: 0.4805831849498537, beta: 1.9810171264198158\n",
      "alpha: 0.48058250631724514, beta: 1.9810195776814667\n",
      "alpha: 0.4805818327422453, beta: 1.9810220032897103\n",
      "alpha: 0.4805811641923698, beta: 1.9810244035144615\n",
      "alpha: 0.4805805006352687, beta: 1.9810267786227806\n",
      "alpha: 0.4805798420387272, beta: 1.9810291288789035\n",
      "alpha: 0.4805791883706665, beta: 1.981031454544272\n",
      "alpha: 0.480578539599144, beta: 1.9810337558775633\n",
      "alpha: 0.48057789569235415, beta: 1.9810360331347197\n",
      "alpha: 0.48057725661862905, beta: 1.981038286568977\n",
      "alpha: 0.4805766223464387, beta: 1.9810405164308944\n",
      "alpha: 0.48057599284439195, beta: 1.9810427229683816\n",
      "alpha: 0.48057536808123646, beta: 1.9810449064267277\n",
      "alpha: 0.4805747480258596, beta: 1.9810470670486289\n",
      "alpha: 0.48057413264728865, beta: 1.9810492050742161\n",
      "alpha: 0.4805735219146913, beta: 1.981051320741082\n",
      "alpha: 0.48057291579737604, beta: 1.9810534142843084\n",
      "alpha: 0.48057231426479236, beta: 1.9810554859364924\n",
      "alpha: 0.48057171728653125, beta: 1.9810575359277731\n",
      "alpha: 0.4805711248323256, beta: 1.9810595644858575\n",
      "alpha: 0.4805705368720503, beta: 1.9810615718360467\n",
      "alpha: 0.4805699533757226, beta: 1.9810635582012608\n",
      "alpha: 0.4805693743135024, beta: 1.981065523802065\n",
      "alpha: 0.4805687996556924, beta: 1.9810674688566938\n",
      "alpha: 0.48056822937273835, beta: 1.9810693935810766\n",
      "alpha: 0.4805676634352294, beta: 1.981071298188861\n",
      "alpha: 0.480567101813898, beta: 1.9810731828914383\n",
      "alpha: 0.4805665444796203, beta: 1.9810750478979668\n",
      "alpha: 0.480565991403416, beta: 1.9810768934153955\n",
      "alpha: 0.48056544255644884, beta: 1.9810787196484876\n",
      "alpha: 0.4805648979100265, beta: 1.9810805267998437\n",
      "alpha: 0.4805643574356006, beta: 1.9810823150699248\n",
      "alpha: 0.4805638211047669, beta: 1.981084084657075\n",
      "alpha: 0.4805632888892654, beta: 1.981085835757544\n",
      "alpha: 0.48056276076098026, beta: 1.9810875685655094\n",
      "alpha: 0.4805622366919398, beta: 1.9810892832730982\n",
      "alpha: 0.48056171665431663, beta: 1.9810909800704093\n",
      "alpha: 0.4805612006204275, beta: 1.9810926591455345\n",
      "alpha: 0.4805606885627335, beta: 1.98109432068458\n",
      "alpha: 0.48056018045383964, beta: 1.9810959648716873\n",
      "alpha: 0.48055967626649515, beta: 1.981097591889054\n",
      "alpha: 0.48055917597359327, beta: 1.9810992019169553\n",
      "alpha: 0.48055867954817116, beta: 1.981100795133763\n",
      "alpha: 0.4805581869634098, beta: 1.9811023717159664\n",
      "alpha: 0.4805576981926339, beta: 1.9811039318381924\n",
      "alpha: 0.4805572132093117, beta: 1.9811054756732251\n",
      "alpha: 0.4805567319870551, beta: 1.9811070033920253\n",
      "alpha: 0.48055625449961914, beta: 1.9811085151637495\n",
      "alpha: 0.48055578072090205, beta: 1.98111001115577\n",
      "alpha: 0.480555310624945, beta: 1.981111491533693\n",
      "alpha: 0.4805548441859321, beta: 1.9811129564613772\n",
      "alpha: 0.4805543813781899, beta: 1.9811144061009531\n",
      "alpha: 0.4805539221761875, beta: 1.9811158406128406\n",
      "alpha: 0.48055346655453596, beta: 1.9811172601557676\n",
      "alpha: 0.48055301448798854, beta: 1.9811186648867878\n",
      "alpha: 0.48055256595144, beta: 1.9811200549612982\n",
      "alpha: 0.4805521209199267, beta: 1.9811214305330571\n",
      "alpha: 0.48055167936862614, beta: 1.9811227917542011\n",
      "alpha: 0.48055124127285676, beta: 1.9811241387752627\n",
      "alpha: 0.4805508066080776, beta: 1.9811254717451865\n",
      "alpha: 0.48055037534988815, beta: 1.981126790811347\n",
      "alpha: 0.48054994747402796, beta: 1.9811280961195648\n",
      "alpha: 0.4805495229563764, beta: 1.9811293878141227\n",
      "alpha: 0.48054910177295207, beta: 1.981130666037783\n",
      "alpha: 0.48054868389991295, beta: 1.9811319309318023\n",
      "alpha: 0.48054826931355576, beta: 1.9811331826359484\n",
      "alpha: 0.48054785799031563, beta: 1.9811344212885158\n",
      "alpha: 0.4805474499067658, beta: 1.9811356470263415\n",
      "alpha: 0.48054704503961726, beta: 1.98113685998482\n",
      "alpha: 0.4805466433657185, beta: 1.981138060297919\n",
      "alpha: 0.48054624486205505, beta: 1.9811392480981942\n",
      "alpha: 0.48054584950574897, beta: 1.9811404235168044\n",
      "alpha: 0.48054545727405873, beta: 1.9811415866835265\n",
      "alpha: 0.4805450681443786, beta: 1.9811427377267699\n",
      "alpha: 0.4805446820942384, beta: 1.9811438767735907\n",
      "alpha: 0.48054429910130303, beta: 1.9811450039497067\n",
      "alpha: 0.48054391914337213, beta: 1.981146119379511\n",
      "alpha: 0.4805435421983795, beta: 1.9811472231860863\n",
      "alpha: 0.48054316824439297, beta: 1.9811483154912188\n",
      "alpha: 0.48054279725961363, beta: 1.9811493964154119\n",
      "alpha: 0.48054242922237567, beta: 1.9811504660778996\n",
      "alpha: 0.48054206411114586, beta: 1.98115152459666\n",
      "alpha: 0.48054170190452306, beta: 1.9811525720884289\n",
      "alpha: 0.4805413425812378, beta: 1.9811536086687125\n",
      "alpha: 0.48054098612015195, beta: 1.9811546344518007\n",
      "alpha: 0.480540632500258, beta: 1.98115564955078\n",
      "alpha: 0.4805402817006789, beta: 1.9811566540775463\n",
      "alpha: 0.4805399337006674, beta: 1.9811576481428166\n",
      "alpha: 0.4805395884796056, beta: 1.9811586318561432\n",
      "alpha: 0.4805392460170046, beta: 1.9811596053259242\n",
      "alpha: 0.4805389062925039, beta: 1.9811605686594171\n",
      "alpha: 0.48053856928587096, beta: 1.98116152196275\n",
      "alpha: 0.48053823497700077, beta: 1.9811624653409343\n",
      "alpha: 0.48053790334591523, beta: 1.9811633988978756\n",
      "alpha: 0.4805375743727628, beta: 1.9811643227363862\n",
      "alpha: 0.48053724803781794, beta: 1.9811652369581962\n",
      "alpha: 0.4805369243214806, beta: 1.9811661416639652\n",
      "alpha: 0.48053660320427577, beta: 1.9811670369532934\n",
      "alpha: 0.48053628466685294, beta: 1.9811679229247332\n",
      "alpha: 0.4805359686899856, beta: 1.9811687996757998\n",
      "alpha: 0.4805356552545707, beta: 1.9811696673029824\n",
      "alpha: 0.48053534434162826, beta: 1.9811705259017551\n",
      "alpha: 0.4805350359323007, beta: 1.9811713755665874\n",
      "alpha: 0.48053473000785235, beta: 1.9811722163909553\n",
      "alpha: 0.48053442654966905, beta: 1.9811730484673513\n",
      "alpha: 0.48053412553925756, beta: 1.9811738718872949\n",
      "alpha: 0.480533826958245, beta: 1.9811746867413431\n",
      "alpha: 0.48053353078837835, beta: 1.9811754931191008\n",
      "alpha: 0.4805332370115239, beta: 1.9811762911092299\n",
      "alpha: 0.4805329456096668, beta: 1.9811770807994602\n",
      "alpha: 0.4805326565649105, beta: 1.981177862276599\n",
      "alpha: 0.48053236985947617, beta: 1.9811786356265408\n",
      "alpha: 0.48053208547570214, beta: 1.981179400934277\n",
      "alpha: 0.48053180339604346, beta: 1.981180158283905\n",
      "alpha: 0.4805315236030713, beta: 1.9811809077586386\n",
      "alpha: 0.48053124607947245, beta: 1.9811816494408163\n",
      "alpha: 0.4805309708080488, beta: 1.9811823834119116\n",
      "alpha: 0.48053069777171675, beta: 1.9811831097525407\n",
      "alpha: 0.4805304269535066, beta: 1.9811838285424734\n",
      "alpha: 0.4805301583365622, beta: 1.9811845398606405\n",
      "alpha: 0.48052989190414025, beta: 1.9811852437851432\n",
      "alpha: 0.48052962763960977, beta: 1.981185940393262\n",
      "alpha: 0.48052936552645165, beta: 1.9811866297614653\n",
      "alpha: 0.48052910554825795, beta: 1.9811873119654182\n",
      "alpha: 0.48052884768873155, beta: 1.9811879870799904\n",
      "alpha: 0.4805285919316855, beta: 1.981188655179265\n",
      "alpha: 0.4805283382610424, beta: 1.9811893163365468\n",
      "alpha: 0.48052808666083396, beta: 1.9811899706243705\n",
      "alpha: 0.4805278371152005, beta: 1.9811906181145087\n",
      "alpha: 0.4805275896083901, beta: 1.98119125887798\n",
      "alpha: 0.4805273441247585, beta: 1.9811918929850572\n",
      "alpha: 0.48052710064876825, beta: 1.9811925205052747\n",
      "alpha: 0.48052685916498816, beta: 1.9811931415074369\n",
      "alpha: 0.48052661965809296, beta: 1.9811937560596253\n",
      "alpha: 0.4805263821128625, beta: 1.9811943642292067\n",
      "alpha: 0.48052614651418146, beta: 1.98119496608284\n",
      "alpha: 0.4805259128470385, beta: 1.981195561686485\n",
      "alpha: 0.48052568109652594, beta: 1.9811961511054077\n",
      "alpha: 0.48052545124783924, beta: 1.9811967344041899\n",
      "alpha: 0.4805252232862762, beta: 1.9811973116467347\n",
      "alpha: 0.48052499719723674, beta: 1.9811978828962749\n",
      "alpha: 0.48052477296622204, beta: 1.9811984482153786\n",
      "alpha: 0.48052455057883425, beta: 1.9811990076659582\n",
      "alpha: 0.48052433002077577, beta: 1.9811995613092754\n",
      "alpha: 0.48052411127784883, beta: 1.9812001092059492\n",
      "alpha: 0.4805238943359548, beta: 1.9812006514159626\n",
      "alpha: 0.48052367918109384, beta: 1.9812011879986686\n",
      "alpha: 0.48052346579936417, beta: 1.9812017190127977\n",
      "alpha: 0.4805232541769616, beta: 1.981202244516464\n",
      "alpha: 0.4805230443001791, beta: 1.981202764567172\n",
      "alpha: 0.4805228361554061, beta: 1.9812032792218222\n",
      "alpha: 0.4805226297291279, beta: 1.9812037885367186\n",
      "alpha: 0.4805224250079254, beta: 1.9812042925675746\n",
      "alpha: 0.4805222219784744, beta: 1.9812047913695188\n",
      "alpha: 0.4805220206275449, beta: 1.9812052849971016\n",
      "alpha: 0.48052182094200085, beta: 1.9812057735043014\n",
      "\n",
      "Coefficients from gradient descent algorithm: \n",
      " 1.9812057735043014\n",
      "\n",
      "Intercept from gradient descent algorithm: \n",
      " 0.48052182094200085\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run the algorithm.\n",
    "for iter in range(stop):\n",
    "    \n",
    "    # Take a step, assigning the results of our step function to feed into\n",
    "    # the next step.\n",
    "    alpha, beta = step(alpha, beta, learning_rate, x, y)\n",
    "    \n",
    "    # Calculate the error.\n",
    "    error = LR_cost_function(alpha, beta, x, y)\n",
    "    \n",
    "    # Store the error to instpect later.\n",
    "    all_error.append(error)\n",
    "\n",
    "    \n",
    "print('\\nCoefficients from gradient descent algorithm: \\n', beta)\n",
    "print('\\nIntercept from gradient descent algorithm: \\n', alpha)\n",
    "\n",
    "plt.plot(all_error, 'o', ms=.4)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Error scores for each iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Things Get Messy\n",
    "\n",
    "Linear regression is a good way to introduce the gradient descent algorithm because there is only one minimum â€“ one absolute best solution.  In other algorithms, however, there may be both a global minimum (the lowest possible value over the entire surface) and many local minima, areas on the surface that are lower than the surface around them.\n",
    "\n",
    "![local and global minima and maxima](assets/maxima_and_minima.svg)\n",
    "\n",
    "When using the gradient descent algorithm with models that have local minima the algorithm can get 'caught' in one and converge on a less-than-optimal solution.  One way to avoid this is to run the algorithm multiple times with different starting values.\n",
    "\n",
    "Still a bit confused? [This](http://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html) is a useful resource for another explanation.\n",
    "\n",
    "## Stopping rules\n",
    "\n",
    "In the implementation programmed above, the only stopping rule involves the number of iterations.  As you can see from the plot above, this might be a bit inefficient in this case.  Modify the code above by adding a stopping threshold so that the algorithm stops when the difference in error between two successive iterations is less than .001.  With that rule, how many iterations do you need before you stop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Your gradient descent algorithm with stopping threshold here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "105px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
