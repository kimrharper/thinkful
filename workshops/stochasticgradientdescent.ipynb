{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function we'll call each iteration (or step) of the gradient algorithm.\n",
    "def step (alpha_cur, beta_cur, learning_rate, x, y):\n",
    "    '''Move downhill from a current cost function to a new, more optimal one.'''\n",
    "    alpha = 0\n",
    "    beta = 0\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        # Partial derivative of the intercept.\n",
    "        point_alpha = -(2/n) * (y[i] - ((alpha_cur + beta_cur * x[i])))\n",
    "        alpha += point_alpha\n",
    "        \n",
    "        # Partial derivative of the slope.\n",
    "        point_beta = -(2 / n) * x[i] * (y[i] - ((alpha_cur + beta_cur * x[i])))\n",
    "        beta += point_beta\n",
    "        \n",
    "    new_alpha = alpha_cur - learning_rate * alpha \n",
    "    new_beta = beta_cur - learning_rate * beta\n",
    "    return [new_alpha, new_beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial\\alpha} =\\frac2n \\sum_{i=1}^n - (y^i-(\\alpha + \\beta x_i) )$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\beta} =\\frac2n \\sum_{i=1}^n - x_i(y^i-(\\alpha + \\beta x_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, m_current=0, b_current=0, epochs=1000, learning_rate=0.0001):\n",
    "    N = float(len(y))\n",
    "    for i in range(epochs):\n",
    "        y_current = (m_current * X) + b_current\n",
    "        cost = sum([data**2 for data in (y-y_current)]) / N\n",
    "        m_gradient = -(2/N) * sum(X * (y - y_current))\n",
    "        b_gradient = -(2/N) * sum(y - y_current)\n",
    "        m_current = m_current - (learning_rate * m_gradient)\n",
    "        b_current = b_current - (learning_rate * b_gradient)\n",
    "    return m_current, b_current, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(range(0,11))\n",
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "der_alpha_sum = lambda X,y,a,b: -(2/len(X))*np.sum((y-(a+b*X)))\n",
    "der_beta_sum = lambda X,y,a,b: -(2/len(X))*np.sum(X*(y-(a+b*X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha,beta,learning_rate=0,0,.1\n",
    "\n",
    "alpha = der_alpha_sum(X,y,alpha,beta)\n",
    "beta = der_beta_sum(X,y,alpha,beta)\n",
    "    \n",
    "new_a = alpha-learning_rate*alpha\n",
    "new_b = beta-learning_rate*beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-63.0, -1125.0)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a,new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "der_alpha = lambda X,y,a,b,n: -(2/n)*(y-(a+b*X))\n",
    "der_beta = lambda X,y,a,b,n: -(2/n)*X*(y-(a+b*X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha,beta,learning_rate=0,0,.1\n",
    "n = len(x)\n",
    "for i in range(n):\n",
    "    alpha+=der_alpha(X[i],y[i],alpha,beta,n)\n",
    "    beta+=der_beta(X[i],y[i],alpha,beta,n)\n",
    "    \n",
    "new_a = alpha-learning_rate*alpha\n",
    "new_b = beta-learning_rate*beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-26788656.668574974, -314937385.8379723)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a,new_b"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import the necessary packages\n",
    "\n",
    " \n",
    "def sigmoid_activation(x):\n",
    "    # compute and return the sigmoid activation value for a\n",
    "    # given input value\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    " \n",
    "def next_batch(X, y, batchSize):\n",
    "    # loop over our dataset `X` in mini-batches of size `batchSize`\n",
    "    for i in np.arange(0, X.shape[0], batchSize):\n",
    "        # yield a tuple of the current batched data and labels\n",
    "        yield (X[i:i + batchSize], y[i:i + batchSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(-20,20))\n",
    "y = [df(v) for v in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] X: -6.6, Grad Descent: -1.9199999999999995, step_size: 9.6\n",
      "[2] X: -4.68, Grad Descent: 0.38400000000000034, step_size: 1.92\n",
      "[3] X: -5.064, Grad Descent: -0.07680000000000006, step_size: 0.38400000000000034\n",
      "[4] X: -4.9872, Grad Descent: 0.01536000000000044, step_size: 0.07680000000000042\n",
      "[5] X: -5.00256, Grad Descent: -0.0030719999999998747, step_size: 0.015360000000000262\n",
      "[6] X: -4.999488, Grad Descent: 0.0006143999999995486, step_size: 0.0030719999999995196\n",
      "[7] X: -5.0001024, Grad Descent: -0.00012288000000033604, step_size: 0.0006143999999999039\n",
      "[8] X: -4.99997952, Grad Descent: 2.4575999999854046e-05, step_size: 0.00012288000000015842\n",
      "[9] X: -5.000004096, Grad Descent: -4.9151999995444834e-06, step_size: 2.4575999999498777e-05\n",
      "[10] X: -4.9999991808, Grad Descent: 9.830400003352224e-07, step_size: 4.915199999899755e-06\n",
      "[11] X: -5.00000016384, Grad Descent: -1.966080002802073e-07, step_size: 9.830400005128581e-07\n",
      "The local minimum occurs at -5.00000016384\n"
     ]
    }
   ],
   "source": [
    "cur_x = 3 # The algorithm starts at x=3\n",
    "rate = 0.6 # Learning rate\n",
    "precision = 0.000001 #This tells us when to stop the algorithm\n",
    "previous_step_size = 1 #\n",
    "max_iters = 10000 # maximum number of iterations\n",
    "iters = 0 #iteration counter\n",
    "df = lambda x: 2*(x+5) #Gradient of our function \n",
    "\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    prev_x = cur_x #Store current x value in prev_x\n",
    "    cur_x = cur_x - rate * df(prev_x) #Grad descent\n",
    "    previous_step_size = abs(cur_x - prev_x) #Change in x\n",
    "    iters = iters+1 #iteration count\n",
    "    print(\"[{0:}] X: {1}, Grad Descent: {2}, step_size: {3}\".format(iters,cur_x,rate *df(cur_x),previous_step_size)) #Print iterations\n",
    "    \n",
    "print(\"The local minimum occurs at\", cur_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
